{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Fardeen210/Text-Summarization/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ca9ccf-af25-49d9-b1ad-5de0f1fd77a4",
   "metadata": {
    "id": "15ca9ccf-af25-49d9-b1ad-5de0f1fd77a4"
   },
   "outputs": [],
   "source": [
    "# installed all relavant library to acess unstructured open source library,\n",
    "\n",
    "# Before installing there are bunch of other supporting libraries need to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9331db",
   "metadata": {
    "id": "3b9331db"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"enter gpt api key\")\n",
    "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"enter MISTRAL api key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a73f8dd-6b62-4d58-ac41-ab223a385a14",
   "metadata": {
    "id": "3a73f8dd-6b62-4d58-ac41-ab223a385a14"
   },
   "outputs": [],
   "source": [
    "import unstructured\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df82ab2",
   "metadata": {
    "id": "5df82ab2"
   },
   "outputs": [],
   "source": [
    "pdf_path = r\"Article_contents\\Article_1\\Article 1.pdf\"\n",
    "images_folder=r\"Article_contents\\Article_1\\Images\"\n",
    "\n",
    "elements = partition_pdf(\n",
    "    filename=pdf_path,                 \n",
    "    strategy=\"hi_res\",                  \n",
    "    extract_images_in_pdf=True,         \n",
    "    extract_image_block_types=[\"Image\", \"Table\"], \n",
    "    extract_image_block_to_payload=False,  \n",
    "    extract_image_block_output_dir=images_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3aa3f45-96e1-4ef9-bbb3-f9d51cb81639",
   "metadata": {
    "id": "c3aa3f45-96e1-4ef9-bbb3-f9d51cb81639",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize lists to categorize elements\n",
    "Header = []\n",
    "Title = []\n",
    "Footer = []\n",
    "Text = []\n",
    "List_item = []\n",
    "Narrative_text = []\n",
    "Document_order = []\n",
    "abstract = []\n",
    "\n",
    "for i, element in enumerate(elements):\n",
    "    # Get the next elements, if they exist\n",
    "    next_ele = elements[i + 1] if i + 1 < len(elements) else None\n",
    "    next_ele1 = elements[i + 2] if i + 2 < len(elements) else None\n",
    "\n",
    "    if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
    "        Header.append(str(element))\n",
    "\n",
    "    elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
    "        Footer.append(str(element))\n",
    "\n",
    "    elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
    "        title = str(element)\n",
    "\n",
    "        # Check for \"Abstract\" and extract one or two NarrativeText paragraphs\n",
    "        if title == \"Abstract\":\n",
    "            if next_ele and \"unstructured.documents.elements.NarrativeText\" in str(type(next_ele)):\n",
    "                abstract.append(str(next_ele))\n",
    "            if next_ele1 and \"unstructured.documents.elements.NarrativeText\" in str(type(next_ele1)):\n",
    "                abstract.append(str(next_ele1))\n",
    "\n",
    "        if title == \"Conclusion\":\n",
    "                Title.append(title)\n",
    "                Document_order.append(title)\n",
    "                Document_order.append(next_ele)\n",
    "                print(\"Conclusion found, exiting loop.\")\n",
    "                break\n",
    "\n",
    "        # Process numeric titles\n",
    "        if title and title[0].isdigit():\n",
    "            Title.append(title)\n",
    "            Document_order.append(title)\n",
    "\n",
    "    elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "        Narrative_text.append(str(element))\n",
    "        Document_order.append(str(element))\n",
    "    elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
    "        Text.append(str(element))\n",
    "        Document_order.append(str(element))\n",
    "    elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
    "        List_item.append(str(element))\n",
    "        Document_order.append(str(element))\n",
    "# Combine the document order into a single string\n",
    "Document_order = ' '.join(Document_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7bd3029-2295-43a6-af6b-d23de9e435b1",
   "metadata": {
    "id": "d7bd3029-2295-43a6-af6b-d23de9e435b1",
    "outputId": "0d007241-db80-4362-ea39-032d58f5c8d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#Here we are using sematic Chunking from langchain\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings(),number_of_chunks=10, min_chunk_size=1500)\n",
    "text_docs = text_splitter.create_documents([Document_order])\n",
    "print(len(text_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe34cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = ''.join(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22db01c-af3c-4427-82f6-55707392d873",
   "metadata": {
    "id": "d22db01c-af3c-4427-82f6-55707392d873",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''CASE-1: All Chunks(RAW TEXT ONLY) will be given as a context to LLM and asked to summarize'''\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        (\"system\", \"You are an expert summarizer. Read the given context and generate a concise, clear, and accurate summary:\\n\\n{context}\")\n",
    ")\n",
    "\n",
    "'''gpt'''\n",
    "\n",
    "llm1 = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "chain1 = create_stuff_documents_chain(llm1, prompt)\n",
    "gpt_chain_result = chain1.invoke({\"context\":text_docs})\n",
    "\n",
    "\n",
    "'''Mistral AI '''\n",
    "\n",
    "llm2 = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "chain2 = create_stuff_documents_chain(llm2, prompt)\n",
    "MistralAI_chain_result = chain2.invoke({\"context\":text_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0062b19-f41b-4dce-b797-90ff23340318",
   "metadata": {
    "id": "a0062b19-f41b-4dce-b797-90ff23340318"
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge1_score(reference_text, generated_text):\n",
    "    \"\"\"\n",
    "    Calculate and return the ROUGE-1 F1 score.\n",
    "\n",
    "    Args:\n",
    "        reference_text (str): The reference text (e.g., human-written summary).\n",
    "        generated_text (str): The generated text (e.g., model summary).\n",
    "\n",
    "    Returns:\n",
    "        float: The ROUGE-1 F1 score.\n",
    "    \"\"\"\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    scores = scorer.score(reference_text, generated_text)\n",
    "\n",
    "    # Extract and return the ROUGE-1 F1 score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa15059b-44c1-4c97-9b82-432a082ebd00",
   "metadata": {
    "id": "fa15059b-44c1-4c97-9b82-432a082ebd00",
    "outputId": "6df2f9e2-988e-4f02-ce67-90d85e3f1193",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model  Metric  Precision  Recall  F-Measure\n",
      "0  GPT-4o-mini  rouge1     0.6250  0.3105     0.4148\n",
      "1  GPT-4o-mini  rouge2     0.1722  0.0852     0.1140\n",
      "2  GPT-4o-mini  rougeL     0.2632  0.1307     0.1747\n",
      "3   Mistral AI  rouge1     0.7500  0.2255     0.3467\n",
      "4   Mistral AI  rouge2     0.2967  0.0885     0.1364\n",
      "5   Mistral AI  rougeL     0.3804  0.1144     0.1759\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Score calculation\n",
    "score_gpt_mini = calculate_rouge1_score(abstract, gpt_chain_result)\n",
    "score_mistral = calculate_rouge1_score(abstract, MistralAI_chain_result)\n",
    "\n",
    "# Function to convert scores into a structured format\n",
    "def extract_scores(model_name, scores):\n",
    "    results = []\n",
    "    for metric, score in scores.items():\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Metric\": metric,\n",
    "            \"Precision\": round(score.precision, 4),\n",
    "            \"Recall\": round(score.recall, 4),\n",
    "            \"F-Measure\": round(score.fmeasure, 4),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Extract scores for both models\n",
    "gpt_scores = extract_scores(\"GPT-4o-mini\", score_gpt_mini)\n",
    "mistral_scores = extract_scores(\"Mistral AI\", score_mistral)\n",
    "\n",
    "# Combine results into a DataFrame\n",
    "scores_df= pd.DataFrame(gpt_scores + mistral_scores)\n",
    "\n",
    "scores_df_text_only  = scores_df[[\"Model\", \"Metric\", \"Precision\", \"Recall\", \"F-Measure\"]]\n",
    "\n",
    "# Print the DataFrame\n",
    "print(scores_df_text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62b9bea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.*Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "length of abstract 2045\n",
      "--------------------------------------------------\n",
      "GPT-40-mini model summary response\n",
      "The document presents the Transformer, a novel neural network architecture developed by researchers from Google Brain and Google Research, which relies entirely on attention mechanisms, eliminating the use of recurrent and convolutional layers. The Transformer model demonstrates significant improvements in machine translation tasks, achieving state-of-the-art BLEU scores of 28.4 for English-to-German and 41.8 for English-to-French translations, while also being more efficient and faster to train compared to existing models. The architecture consists of an encoder-decoder structure utilizing multi-head self-attention and feed-forward networks, allowing for greater parallelization and better learning of long-range dependencies. The research also highlights the model's versatility, successfully applying it to tasks like English constituency parsing. The authors express enthusiasm for future applications of attention-based models beyond text, including areas like images and audio. The code used for training and evaluation is available online.\n",
      "Length of gpt response 1054\n",
      "--------------------------------------------------\n",
      "MistralAI model summmary response\n",
      "The paper introduces the Transformer, a new neural network architecture based solely on attention mechanisms, eliminating the need for recurrent or convolutional layers. This architecture significantly improves parallelization and reduces training time. The Transformer model achieves state-of-the-art results on machine translation tasks, including WMT 2014 English-to-German and English-to-French, outperforming previous models with less computational cost. The model also shows promise in English constituency parsing, demonstrating its generalizability to other tasks. The Transformer's success highlights the potential of attention-based models in sequence transduction tasks.\n",
      "Length of mistralAI response 681\n"
     ]
    }
   ],
   "source": [
    "print(\"Orginal abstract\")\n",
    "print(abstract)\n",
    "\n",
    "print(\"length of abstract\",len(abstract))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"GPT-40-mini model summary response\")\n",
    "print(gpt_chain_result)\n",
    "\n",
    "print(\"Length of gpt response\",len(gpt_chain_result))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"MistralAI model summmary response\")\n",
    "\n",
    "print(MistralAI_chain_result)\n",
    "print(\"Length of mistralAI response\",len(MistralAI_chain_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c85e6d77",
   "metadata": {
    "id": "c85e6d77",
    "outputId": "2da40b4d-51ee-4502-cbea-790951441990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Article    Model BLEU Score\n",
      "0  Article_1                    \n",
      "1                 GPT     0.3034\n",
      "2             Mistral     0.1175\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "bleu_score_gpt1 = sentence_bleu([abstract], gpt_chain_result)\n",
    "bleu_score_mistal1 = sentence_bleu([abstract], MistralAI_chain_result)\n",
    "\n",
    "bleu_score_data = {\n",
    "    \"Article\": [\"Article_1\", \"\", \"\"],\n",
    "    \"Model\": [\"\", \"GPT\", \"Mistral\"],\n",
    "    \"BLEU Score\": [\"\", round(bleu_score_gpt1, 4), round(bleu_score_mistal1, 4)]\n",
    "}\n",
    "bleu_scores_df = pd.DataFrame(bleu_score_data)\n",
    "print(bleu_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df38af3-6e8b-4508-9fc6-3699a167e893",
   "metadata": {
    "id": "7df38af3-6e8b-4508-9fc6-3699a167e893"
   },
   "source": [
    "\n",
    "\n",
    "CASE:2 Adding Images such as flowchart, related_images to the context to test will there any difference in summarization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e96440-8595-4279-b176-fbe4abc52f2c",
   "metadata": {
    "id": "65e96440-8595-4279-b176-fbe4abc52f2c"
   },
   "outputs": [],
   "source": [
    "# Installing requiered libraries to obtain gpt via langchain\n",
    "# langchain-core defines the base abstractions for the LangChain ecosystem.\n",
    "# The interfaces for core components like chat models, LLMs, vector stores, retrievers, and more are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528cf39-910f-4862-abb2-2f414e818bcc",
   "metadata": {
    "id": "9528cf39-910f-4862-abb2-2f414e818bcc"
   },
   "outputs": [],
   "source": [
    "# I am passing direct images to GPT4V to get summary of it.\n",
    "# for ease i am storing all my images in my  local dirve and text in word document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d3963d2-5b41-4a2a-b32d-10d2a6eff51d",
   "metadata": {
    "id": "6d3963d2-5b41-4a2a-b32d-10d2a6eff51d"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import base64\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b1cbc-672e-4fe0-af7c-a807f59f621f",
   "metadata": {
    "id": "8c4b1cbc-672e-4fe0-af7c-a807f59f621f",
    "outputId": "d021ffdc-4b91-403d-f822-af382b7aec71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below code is copied from langchain cookbook'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Below code is copied from langchain cookbook'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1824607-71c0-4de4-8cc0-3b6dc8ef3d4b",
   "metadata": {
    "id": "d1824607-71c0-4de4-8cc0-3b6dc8ef3d4b"
   },
   "outputs": [],
   "source": [
    "prompt_text=\"\"\"Analyze the image provided.\n",
    "Summarize key insights, highlighting any significant patterns, trends, or relationships between the data in the table and visual content in the image.\n",
    "Focus on critical details that reveal important information, and provide a clear, concise summary \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e76a0e4-8c97-401d-a652-ce89defbea0e",
   "metadata": {
    "id": "0e76a0e4-8c97-401d-a652-ce89defbea0e"
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9694be8-5828-4c7c-af43-2a8cfca362dd",
   "metadata": {
    "id": "c9694be8-5828-4c7c-af43-2a8cfca362dd"
   },
   "outputs": [],
   "source": [
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "718735ed-b884-45ca-bf4e-502b5de227be",
   "metadata": {
    "id": "718735ed-b884-45ca-bf4e-502b5de227be"
   },
   "outputs": [],
   "source": [
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images and tables.\n",
    "    Give a concise summary of the image or table which is readlale. \"\"\"\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image) # for reference we are storing the base64_iamge\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1e149b9-a83f-49a1-8d71-9ef06df16137",
   "metadata": {
    "id": "f1e149b9-a83f-49a1-8d71-9ef06df16137"
   },
   "outputs": [],
   "source": [
    "fpath= \"Article_contents\\Article_1\\Images\"\n",
    "img_base64_list, image_summaries = generate_img_summaries(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87619c-75c7-4233-b3b3-6a9e9ea15db6",
   "metadata": {
    "id": "cd87619c-75c7-4233-b3b3-6a9e9ea15db6"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Storing raw text only in vecotre store\n",
    "# We will use GPT4ALL embeddings and chroma vectore storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12982c5-d640-4bb5-a315-ef8c057bc33d",
   "metadata": {
    "id": "a12982c5-d640-4bb5-a315-ef8c057bc33d"
   },
   "outputs": [],
   "source": [
    "# If processing multiple documents and need to reset the vector store for each document\n",
    "# Uncomment the following line to delete previous embeddings:\n",
    "\n",
    "# vector_store.delete_collection() \n",
    "\n",
    "# For a single document, thereâ€™s no need to clear embeddings as they only pertain to that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a67fdd1-da89-4c31-8f55-ff8dd3eeeff4",
   "metadata": {
    "id": "3a67fdd1-da89-4c31-8f55-ff8dd3eeeff4"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6c13d4d-fdfa-49a2-9543-3d9c7bd2857a",
   "metadata": {
    "id": "f6c13d4d-fdfa-49a2-9543-3d9c7bd2857a"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"summary_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"C:\\chroma\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "899df48f-c477-4e56-bd59-71fc8f0d22cd",
   "metadata": {
    "id": "899df48f-c477-4e56-bd59-71fc8f0d22cd"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "doc_id = [str(uuid.uuid4()) for i in text_docs ]\n",
    "\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "Text_docs_document = [\n",
    "    Document(page_content=s.page_content, metadata={\"id_key\": doc_id[i],\"type\": \"text\"})\n",
    "    for i,s in enumerate(text_docs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58ff3c8f",
   "metadata": {
    "id": "58ff3c8f",
    "outputId": "23e5d226-e96f-4d9f-a6f1-b1b2d502b1bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bb090bcc-2847-49e0-8b87-c2cc3dab9c4e',\n",
       " 'b8d8244a-7e96-4185-bd86-dedd611c6b01',\n",
       " 'f97e84c7-b041-486b-928d-651beec3949d',\n",
       " 'acd97866-22b6-4baf-a206-c8a7a1260ae0',\n",
       " '5867bf48-4967-4367-acba-4d509b8c36ac',\n",
       " 'b26f8496-b8db-49a9-8524-4bb848383643']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=Text_docs_document, ids=doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9e823",
   "metadata": {},
   "source": [
    "**# Here, I am doing similarity search between images_summaries and Text_vectore_embeddings\n",
    "Then, finding image summaries and text which has high similarity and appending them together.\n",
    "Then, Passing those appended_content to the llm context to get detail summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aeddbd34-ba47-40f2-adf5-a670b33810d0",
   "metadata": {
    "id": "aeddbd34-ba47-40f2-adf5-a670b33810d0"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.schema import Document\n",
    "\n",
    "for summary in image_summaries:\n",
    "    # Perform similarity search to get top 1 similar documents\n",
    "    results = vector_store.similarity_search(summary, k=1)\n",
    "\n",
    "    for doc in results:\n",
    "        updated_page_content = f\"{doc.page_content} [RELATED_IMAGE_SUMMARY: {summary}]\"\n",
    "        updated_doc = Document( metadata=doc.metadata,page_content=updated_page_content)\n",
    "        vector_store.update_document(document_id=doc.metadata[\"id_key\"], document=updated_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2637d0d-40dd-48a4-be6e-aae537a05d34",
   "metadata": {
    "id": "a2637d0d-40dd-48a4-be6e-aae537a05d34"
   },
   "outputs": [],
   "source": [
    "'''Here, i am extracting entire content stored in vectore database'''\n",
    "\n",
    "result = vector_store.get()\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "updated_image_text_doc3 = []\n",
    "\n",
    "for doc, meta in zip(result['documents'], result['metadatas']):\n",
    "    # Create a Document object with proper arguments\n",
    "    document = Document(metadata=meta, page_content=doc)\n",
    "    updated_image_text_doc3.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841dbe99",
   "metadata": {},
   "source": [
    "Case:2 Image_summaries and pdf raw text is given as a context to LLM and asked to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48fa2e9f-bb1d-4fed-9195-0503b72d0873",
   "metadata": {
    "id": "48fa2e9f-bb1d-4fed-9195-0503b72d0873"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define prompt\n",
    "prompt1 = ChatPromptTemplate.from_messages(\n",
    "    [(\n",
    "            \"system\",\n",
    "            \"You are a summarization assistant specialized in scientific content. Given the context of a scientific paper, which includes raw text and related image summaries, generate a comprehensive and concise summary. Integrate key points from both the text and image summaries:\\n\"\n",
    "            \"Context:\\n\\n{context} [RELATED_IMAGE_SUMMARY]\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Instantiate chains\n",
    "chain1_1 = create_stuff_documents_chain(llm1, prompt1)\n",
    "chain2_1 = create_stuff_documents_chain(llm2, prompt1)\n",
    "\n",
    "gpt_doc_img_chain_result = chain1_1.invoke({\"context\": updated_image_text_doc3})\n",
    "mistralai_doc_img_chain_result = chain2_1.invoke({\"context\": updated_image_text_doc3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70a789cf",
   "metadata": {
    "id": "70a789cf",
    "outputId": "fbbb0247-ba69-479c-d3b3-433313376554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPt img response result\n",
      "rouge1 - Precision: 0.3147, Recall: 0.3497, Fmeasure: 0.3313\n",
      "rouge2 - Precision: 0.0531, Recall: 0.0590, Fmeasure: 0.0559\n",
      "rougeL - Precision: 0.1353, Recall: 0.1503, Fmeasure: 0.1424\n",
      "\n",
      "\n",
      "mistral img response result\n",
      "rouge1 - Precision: 0.2627, Recall: 0.4902, Fmeasure: 0.3421\n",
      "rouge2 - Precision: 0.0474, Recall: 0.0885, Fmeasure: 0.0617\n",
      "rougeL - Precision: 0.0963, Recall: 0.1797, Fmeasure: 0.1254\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_Score(scores):\n",
    "\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"{metric} - Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, Fmeasure: {score.fmeasure:.4f}\")\n",
    "\n",
    "\n",
    "# ROUGE Score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "\n",
    "score_gpt_mini_img_doc = scorer.score(abstract,gpt_doc_img_chain_result)\n",
    "\n",
    "score_mistral_img_doc = scorer.score(abstract,mistralai_doc_img_chain_result)\n",
    "\n",
    "\n",
    "print(\"GPt img response result\")\n",
    "get_Score(score_gpt_mini_img_doc)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"mistral img response result\")\n",
    "get_Score(score_mistral_img_doc)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5c1d690",
   "metadata": {
    "id": "f5c1d690",
    "outputId": "1c7485f5-ab55-4029-bedc-4a8fd59bf9e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GPT summary BERT (text only)  F1 Score: 0.8631501197814941\n",
      "Mistral AI summary (BERT text) only  F1 Score: 0.8247866034507751\n",
      "\n",
      "\n",
      "GPT summary BERT (text and Images)  F1 Score: 0.8654877543449402\n",
      "Mistral AI summary BERT (text  and Images)  F1 Score:  0.8123260140419006\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "scorer = BERTScorer(lang=\"en\")\n",
    "\n",
    "P1, R1, F1_1 = scorer.score([gpt_chain_result], [abstract])\n",
    "\n",
    "P1_1, R1_1, F1_1_1 = scorer.score([gpt_doc_img_chain_result], [abstract])\n",
    "\n",
    "P2, R2, F1_2 = scorer.score([MistralAI_chain_result], [abstract])\n",
    "\n",
    "P2_1, R2_1, F2_2_1 = scorer.score([mistralai_doc_img_chain_result], [abstract])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"GPT summary BERT (text only)  F1 Score:\", F1_1.tolist()[0])\n",
    "print(\"Mistral AI summary (BERT text) only  F1 Score:\", F1_1_1.tolist()[0])\n",
    "print(\"\\n\")\n",
    "print(\"GPT summary BERT (text and Images)  F1 Score:\", F1_2.tolist()[0])\n",
    "print(\"Mistral AI summary BERT (text  and Images)  F1 Score: \", F2_2_1.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0604f17f",
   "metadata": {
    "id": "0604f17f",
    "outputId": "447f2fdc-0fd9-4e78-9591-4ce14da17761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Article    Model BLEU Score\n",
      "0  Article_1                    \n",
      "1                 GPT     0.5121\n",
      "2             Mistral     0.3533\n"
     ]
    }
   ],
   "source": [
    "bleu_score_gpt2 = sentence_bleu([abstract], gpt_doc_img_chain_result)\n",
    "bleu_score_mistal2 = sentence_bleu([abstract], mistralai_doc_img_chain_result)\n",
    "\n",
    "bleu_score_data1 = {\n",
    "    \"Article\": [\"Article_1\", \"\", \"\"],\n",
    "    \"Model\": [\"\", \"GPT\", \"Mistral\"],\n",
    "    \"BLEU Score\": [\"\", round(bleu_score_gpt2, 4), round(bleu_score_mistal2, 4)]\n",
    "}\n",
    "\n",
    "bleu_scores_df1 = pd.DataFrame(bleu_score_data1)\n",
    "\n",
    "print(bleu_scores_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28dc582",
   "metadata": {},
   "source": [
    "G-EVAl method evalution \n",
    "This part of code is imported from openai cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcdad72a",
   "metadata": {
    "id": "bcdad72a"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Evaluation prompt template based on G-Eval\n",
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions very carefully.\n",
    "Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "{steps}\n",
    "\n",
    "Example:\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{document}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "- {metric_name}\n",
    "\"\"\"\n",
    "\n",
    "# Metric 1: Relevance\n",
    "\n",
    "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
    "Relevance(1-5) - selection of important content from the source. \\\n",
    "The summary should include only important information from the source document. \\\n",
    "Annotators were instructed to penalize summaries which contained redundancies and excess information.\n",
    "\"\"\"\n",
    "\n",
    "RELEVANCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the summary and the source document carefully.\n",
    "2. Compare the summary to the source document and identify the main points of the article.\n",
    "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 2: Coherence\n",
    "\n",
    "COHERENCE_SCORE_CRITERIA = \"\"\"\n",
    "Coherence(1-5) - the collective quality of all sentences. \\\n",
    "We align this dimension with the DUC quality question of structure and coherence \\\n",
    "whereby \"the summary should be well-structured and well-organized. \\\n",
    "The summary should not just be a heap of related information, but should build from sentence to a\\\n",
    "coherent body of information about a topic.\"\n",
    "\"\"\"\n",
    "\n",
    "COHERENCE_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main topic and key points.\n",
    "2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,\n",
    "and if it presents them in a clear and logical order.\n",
    "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 3: Consistency\n",
    "\n",
    "CONSISTENCY_SCORE_CRITERIA = \"\"\"\n",
    "Consistency(1-5) - the factual alignment between the summary and the summarized source. \\\n",
    "A factually consistent summary contains only statements that are entailed by the source document. \\\n",
    "Annotators were also asked to penalize summaries that contained hallucinated facts.\n",
    "\"\"\"\n",
    "\n",
    "CONSISTENCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main facts and details it presents.\n",
    "2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.\n",
    "3. Assign a score for consistency based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 4: Fluency\n",
    "\n",
    "FLUENCY_SCORE_CRITERIA = \"\"\"\n",
    "Fluency(1-5): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
    "1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n",
    "2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "3: Good. The summary has few or no errors and is easy to read and follow.\n",
    "4: Very Good. The summary is almost perfect human readibale but also can be improved.\n",
    "5: Excellent. The summary is pefect and completlely human readibale no improvements needed.\n",
    "\"\"\"\n",
    "\n",
    "FLUENCY_SCORE_STEPS = \"\"\"\n",
    "Read the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_geval_score(\n",
    "    criteria: str, steps: str, document: str, summary: str, metric_name: str\n",
    "):\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        criteria=criteria,\n",
    "        steps=steps,\n",
    "        metric_name=metric_name,\n",
    "        document=document,\n",
    "        summary=summary,\n",
    "    )\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=5,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
    "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
    "    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n",
    "    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n",
    "}\n",
    "\n",
    "summaries = {\"abstract\": abstract,\"gpt_chain_result 1 \" : gpt_chain_result , \"gpt_doc_img_chain_result 1\": gpt_doc_img_chain_result , \"mistralAI_chain_result 1\" : MistralAI_chain_result, \"mistralai_doc_img_chain_result 1\" : mistralai_doc_img_chain_result}\n",
    "\n",
    "data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n",
    "\n",
    "for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
    "    for summ_type, summary in summaries.items():\n",
    "        data[\"Evaluation Type\"].append(eval_type)\n",
    "        data[\"Summary Type\"].append(summ_type)\n",
    "        result = get_geval_score(criteria, steps, abstract, summary, eval_type)\n",
    "        score_num = int(result.strip())\n",
    "        data[\"Score\"].append(score_num)\n",
    "\n",
    "pivot_df = pd.DataFrame(data, index=None).pivot(\n",
    "    index=\"Evaluation Type\", columns=\"Summary Type\", values=\"Score\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f68b3727",
   "metadata": {
    "id": "f68b3727",
    "outputId": "39d697df-bee5-4ae3-8fdf-49a90b60714d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Type     abstract  gpt_chain_result 1   gpt_doc_img_chain_result 1  \\\n",
      "Evaluation Type                                                              \n",
      "Coherence               1                    5                           1   \n",
      "Consistency             5                    5                           1   \n",
      "Fluency                 5                    5                           5   \n",
      "Relevance               5                    5                           1   \n",
      "\n",
      "Summary Type     mistralAI_chain_result 1  mistralai_doc_img_chain_result 1  \n",
      "Evaluation Type                                                              \n",
      "Coherence                               5                                 3  \n",
      "Consistency                             5                                 1  \n",
      "Fluency                                 5                                 5  \n",
      "Relevance                               5                                 1  \n"
     ]
    }
   ],
   "source": [
    "print(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9281940-5385-465e-ab99-6269465965c6",
   "metadata": {
    "id": "c9281940-5385-465e-ab99-6269465965c6",
    "outputId": "7b206cbc-4b49-4d95-91e3-b17031cc58c3"
   },
   "outputs": [],
   "source": [
    "query = \"what is GNMT + RL Ensemble bleu score and training cost \"\n",
    "\n",
    "def relavent_info(query):\n",
    "    result = vector_store.similarity_search(query)\n",
    "    aggregated_text = \" \".join([doc.page_content for doc in result])\n",
    "    return aggregated_text\n",
    "\n",
    "result = relavent_info(query)\n",
    "\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Option 1: LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0.5)\n",
    "\n",
    "# RAG pipeline\n",
    "chain2 = (\n",
    "    {\n",
    "        \"context\": RunnablePassthrough(),  # Wrap context in RunnablePassthrough\n",
    "        \"question\": RunnablePassthrough(),  # Same for the question\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "output = chain2.invoke({\"context\": result, \"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44b4f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GNMT + RL Ensemble has a BLEU score of 26.30 for English-to-German (EN-DE) and 41.16 for English-to-French (EN-FR). The training costs for these models range from \\(9.6 \\cdot 10^{18}\\) to \\(8.0 \\cdot 10^{20}\\) FLOPs.\n"
     ]
    }
   ],
   "source": [
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
