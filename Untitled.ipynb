{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Fardeen210/Text-Summarization/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ca9ccf-af25-49d9-b1ad-5de0f1fd77a4",
   "metadata": {
    "id": "15ca9ccf-af25-49d9-b1ad-5de0f1fd77a4"
   },
   "outputs": [],
   "source": [
    "# installed all relavant library to acess unstructured open source library,\n",
    "\n",
    "# Before installing there are bunch of other supporting libraries need to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9331db",
   "metadata": {
    "id": "3b9331db"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"enter gpt api key\")\n",
    "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"enter MISTRAL api key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a73f8dd-6b62-4d58-ac41-ab223a385a14",
   "metadata": {
    "id": "3a73f8dd-6b62-4d58-ac41-ab223a385a14"
   },
   "outputs": [],
   "source": [
    "import unstructured\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df82ab2",
   "metadata": {
    "id": "5df82ab2"
   },
   "outputs": [],
   "source": [
    "pdf_path = r\"Article_contents\\Article_1\\Article 1.pdf\"\n",
    "images_folder=r\"Article_contents\\Article_1\\Images\"\n",
    "\n",
    "elements = partition_pdf(\n",
    "    filename=pdf_path,                 \n",
    "    strategy=\"hi_res\",                  \n",
    "    extract_images_in_pdf=True,         \n",
    "    extract_image_block_types=[\"Image\", \"Table\"], \n",
    "    extract_image_block_to_payload=False,  \n",
    "    extract_image_block_output_dir=images_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3aa3f45-96e1-4ef9-bbb3-f9d51cb81639",
   "metadata": {
    "id": "c3aa3f45-96e1-4ef9-bbb3-f9d51cb81639",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize lists to categorize elements\n",
    "Header = []\n",
    "Title = []\n",
    "Footer = []\n",
    "Text = []\n",
    "List_item = []\n",
    "Narrative_text = []\n",
    "Document_order = []\n",
    "abstract = []\n",
    "\n",
    "for i, element in enumerate(elements):\n",
    "    # Get the next elements, if they exist\n",
    "    next_ele = elements[i + 1] if i + 1 < len(elements) else None\n",
    "    next_ele1 = elements[i + 2] if i + 2 < len(elements) else None\n",
    "\n",
    "    if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
    "        Header.append(str(element))\n",
    "\n",
    "    elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
    "        Footer.append(str(element))\n",
    "\n",
    "    elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
    "        title = str(element)\n",
    "\n",
    "        # Check for \"Abstract\" and extract one or two NarrativeText paragraphs\n",
    "        if title == \"Abstract\":\n",
    "            if next_ele and \"unstructured.documents.elements.NarrativeText\" in str(type(next_ele)):\n",
    "                abstract.append(str(next_ele))\n",
    "            if next_ele1 and \"unstructured.documents.elements.NarrativeText\" in str(type(next_ele1)):\n",
    "                abstract.append(str(next_ele1))\n",
    "\n",
    "        if title == \"Conclusion\":\n",
    "                Title.append(title)\n",
    "                Document_order.append(title)\n",
    "                Document_order.append(next_ele)\n",
    "                print(\"Conclusion found, exiting loop.\")\n",
    "                break\n",
    "\n",
    "        # Process numeric titles\n",
    "        if title and title[0].isdigit():\n",
    "            Title.append(title)\n",
    "            Document_order.append(title)\n",
    "\n",
    "    elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "        Narrative_text.append(str(element))\n",
    "        Document_order.append(str(element))\n",
    "    elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
    "        Text.append(str(element))\n",
    "        Document_order.append(str(element))\n",
    "    elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
    "        List_item.append(str(element))\n",
    "        Document_order.append(str(element))\n",
    "# Combine the document order into a single string\n",
    "Document_order = ' '.join(Document_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7bd3029-2295-43a6-af6b-d23de9e435b1",
   "metadata": {
    "id": "d7bd3029-2295-43a6-af6b-d23de9e435b1",
    "outputId": "0d007241-db80-4362-ea39-032d58f5c8d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#Here we are using sematic Chunking from langchain\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings(),number_of_chunks=10, min_chunk_size=1500)\n",
    "text_docs = text_splitter.create_documents([Document_order])\n",
    "print(len(text_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe34cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = ''.join(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22db01c-af3c-4427-82f6-55707392d873",
   "metadata": {
    "id": "d22db01c-af3c-4427-82f6-55707392d873",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''CASE-1: All Chunks(RAW TEXT ONLY) will be given as a context to LLM and asked to summarize'''\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        (\"system\", \"You are an expert summarizer. Read the given context and generate a concise, clear, and accurate summary:\\n\\n{context}\")\n",
    ")\n",
    "\n",
    "'''gpt'''\n",
    "\n",
    "llm1 = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "chain1 = create_stuff_documents_chain(llm1, prompt)\n",
    "gpt_chain_result = chain1.invoke({\"context\":text_docs})\n",
    "\n",
    "\n",
    "'''Mistral AI '''\n",
    "\n",
    "llm2 = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "chain2 = create_stuff_documents_chain(llm2, prompt)\n",
    "MistralAI_chain_result = chain2.invoke({\"context\":text_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0062b19-f41b-4dce-b797-90ff23340318",
   "metadata": {
    "id": "a0062b19-f41b-4dce-b797-90ff23340318"
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge1_score(reference_text, generated_text):\n",
    "    \"\"\"\n",
    "    Calculate and return the ROUGE-1 F1 score.\n",
    "\n",
    "    Args:\n",
    "        reference_text (str): The reference text (e.g., human-written summary).\n",
    "        generated_text (str): The generated text (e.g., model summary).\n",
    "\n",
    "    Returns:\n",
    "        float: The ROUGE-1 F1 score.\n",
    "    \"\"\"\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    scores = scorer.score(reference_text, generated_text)\n",
    "\n",
    "    # Extract and return the ROUGE-1 F1 score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa15059b-44c1-4c97-9b82-432a082ebd00",
   "metadata": {
    "id": "fa15059b-44c1-4c97-9b82-432a082ebd00",
    "outputId": "6df2f9e2-988e-4f02-ce67-90d85e3f1193",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model  Metric  Precision  Recall  F-Measure\n",
      "0  GPT-4o-mini  rouge1     0.6250  0.3105     0.4148\n",
      "1  GPT-4o-mini  rouge2     0.1722  0.0852     0.1140\n",
      "2  GPT-4o-mini  rougeL     0.2632  0.1307     0.1747\n",
      "3   Mistral AI  rouge1     0.7500  0.2255     0.3467\n",
      "4   Mistral AI  rouge2     0.2967  0.0885     0.1364\n",
      "5   Mistral AI  rougeL     0.3804  0.1144     0.1759\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Score calculation\n",
    "score_gpt_mini = calculate_rouge1_score(abstract, gpt_chain_result)\n",
    "score_mistral = calculate_rouge1_score(abstract, MistralAI_chain_result)\n",
    "\n",
    "# Function to convert scores into a structured format\n",
    "def extract_scores(model_name, scores):\n",
    "    results = []\n",
    "    for metric, score in scores.items():\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Metric\": metric,\n",
    "            \"Precision\": round(score.precision, 4),\n",
    "            \"Recall\": round(score.recall, 4),\n",
    "            \"F-Measure\": round(score.fmeasure, 4),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Extract scores for both models\n",
    "gpt_scores = extract_scores(\"GPT-4o-mini\", score_gpt_mini)\n",
    "mistral_scores = extract_scores(\"Mistral AI\", score_mistral)\n",
    "\n",
    "# Combine results into a DataFrame\n",
    "scores_df= pd.DataFrame(gpt_scores + mistral_scores)\n",
    "\n",
    "scores_df_text_only  = scores_df[[\"Model\", \"Metric\", \"Precision\", \"Recall\", \"F-Measure\"]]\n",
    "\n",
    "# Print the DataFrame\n",
    "print(scores_df_text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62b9bea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.*Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "length of abstract 2045\n",
      "--------------------------------------------------\n",
      "GPT-40-mini model summary response\n",
      "The document presents the Transformer, a novel neural network architecture developed by researchers from Google Brain and Google Research, which relies entirely on attention mechanisms, eliminating the use of recurrent and convolutional layers. The Transformer model demonstrates significant improvements in machine translation tasks, achieving state-of-the-art BLEU scores of 28.4 for English-to-German and 41.8 for English-to-French translations, while also being more efficient and faster to train compared to existing models. The architecture consists of an encoder-decoder structure utilizing multi-head self-attention and feed-forward networks, allowing for greater parallelization and better learning of long-range dependencies. The research also highlights the model's versatility, successfully applying it to tasks like English constituency parsing. The authors express enthusiasm for future applications of attention-based models beyond text, including areas like images and audio. The code used for training and evaluation is available online.\n",
      "Length of gpt response 1054\n",
      "--------------------------------------------------\n",
      "MistralAI model summmary response\n",
      "The paper introduces the Transformer, a new neural network architecture based solely on attention mechanisms, eliminating the need for recurrent or convolutional layers. This architecture significantly improves parallelization and reduces training time. The Transformer model achieves state-of-the-art results on machine translation tasks, including WMT 2014 English-to-German and English-to-French, outperforming previous models with less computational cost. The model also shows promise in English constituency parsing, demonstrating its generalizability to other tasks. The Transformer's success highlights the potential of attention-based models in sequence transduction tasks.\n",
      "Length of mistralAI response 681\n"
     ]
    }
   ],
   "source": [
    "print(\"Orginal abstract\")\n",
    "print(abstract)\n",
    "\n",
    "print(\"length of abstract\",len(abstract))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"GPT-40-mini model summary response\")\n",
    "print(gpt_chain_result)\n",
    "\n",
    "print(\"Length of gpt response\",len(gpt_chain_result))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"MistralAI model summmary response\")\n",
    "\n",
    "print(MistralAI_chain_result)\n",
    "print(\"Length of mistralAI response\",len(MistralAI_chain_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c85e6d77",
   "metadata": {
    "id": "c85e6d77",
    "outputId": "2da40b4d-51ee-4502-cbea-790951441990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Article    Model BLEU Score\n",
      "0  Article_1                    \n",
      "1                 GPT     0.3034\n",
      "2             Mistral     0.1175\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "bleu_score_gpt1 = sentence_bleu([abstract], gpt_chain_result)\n",
    "bleu_score_mistal1 = sentence_bleu([abstract], MistralAI_chain_result)\n",
    "\n",
    "bleu_score_data = {\n",
    "    \"Article\": [\"Article_1\", \"\", \"\"],\n",
    "    \"Model\": [\"\", \"GPT\", \"Mistral\"],\n",
    "    \"BLEU Score\": [\"\", round(bleu_score_gpt1, 4), round(bleu_score_mistal1, 4)]\n",
    "}\n",
    "bleu_scores_df = pd.DataFrame(bleu_score_data)\n",
    "print(bleu_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df38af3-6e8b-4508-9fc6-3699a167e893",
   "metadata": {
    "id": "7df38af3-6e8b-4508-9fc6-3699a167e893"
   },
   "source": [
    "\n",
    "\n",
    "CASE:2 Adding Images such as flowchart, related_images to the context to test will there any difference in summarization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e96440-8595-4279-b176-fbe4abc52f2c",
   "metadata": {
    "id": "65e96440-8595-4279-b176-fbe4abc52f2c"
   },
   "outputs": [],
   "source": [
    "# Installing requiered libraries to obtain gpt via langchain\n",
    "# langchain-core defines the base abstractions for the LangChain ecosystem.\n",
    "# The interfaces for core components like chat models, LLMs, vector stores, retrievers, and more are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528cf39-910f-4862-abb2-2f414e818bcc",
   "metadata": {
    "id": "9528cf39-910f-4862-abb2-2f414e818bcc"
   },
   "outputs": [],
   "source": [
    "# I am passing direct images to GPT4V to get summary of it.\n",
    "# for ease i am storing all my images in my  local dirve and text in word document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d3963d2-5b41-4a2a-b32d-10d2a6eff51d",
   "metadata": {
    "id": "6d3963d2-5b41-4a2a-b32d-10d2a6eff51d"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import base64\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b1cbc-672e-4fe0-af7c-a807f59f621f",
   "metadata": {
    "id": "8c4b1cbc-672e-4fe0-af7c-a807f59f621f",
    "outputId": "d021ffdc-4b91-403d-f822-af382b7aec71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below code is copied from langchain cookbook'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Below code is copied from langchain cookbook'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1824607-71c0-4de4-8cc0-3b6dc8ef3d4b",
   "metadata": {
    "id": "d1824607-71c0-4de4-8cc0-3b6dc8ef3d4b"
   },
   "outputs": [],
   "source": [
    "prompt_text=\"\"\"Analyze the image provided.\n",
    "Summarize key insights, highlighting any significant patterns, trends, or relationships between the data in the table and visual content in the image.\n",
    "Focus on critical details that reveal important information, and provide a clear, concise summary \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e76a0e4-8c97-401d-a652-ce89defbea0e",
   "metadata": {
    "id": "0e76a0e4-8c97-401d-a652-ce89defbea0e"
   },
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9694be8-5828-4c7c-af43-2a8cfca362dd",
   "metadata": {
    "id": "c9694be8-5828-4c7c-af43-2a8cfca362dd"
   },
   "outputs": [],
   "source": [
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "718735ed-b884-45ca-bf4e-502b5de227be",
   "metadata": {
    "id": "718735ed-b884-45ca-bf4e-502b5de227be"
   },
   "outputs": [],
   "source": [
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images and tables.\n",
    "    Give a concise summary of the image or table which is readlale. \"\"\"\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image) # for reference we are storing the base64_iamge\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1e149b9-a83f-49a1-8d71-9ef06df16137",
   "metadata": {
    "id": "f1e149b9-a83f-49a1-8d71-9ef06df16137"
   },
   "outputs": [],
   "source": [
    "fpath= \"Article_contents\\Article_1\\Images\"\n",
    "img_base64_list, image_summaries = generate_img_summaries(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87619c-75c7-4233-b3b3-6a9e9ea15db6",
   "metadata": {
    "id": "cd87619c-75c7-4233-b3b3-6a9e9ea15db6"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Storing raw text only in vecotre store\n",
    "# We will use GPT4ALL embeddings and chroma vectore storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12982c5-d640-4bb5-a315-ef8c057bc33d",
   "metadata": {
    "id": "a12982c5-d640-4bb5-a315-ef8c057bc33d"
   },
   "outputs": [],
   "source": [
    "# If processing multiple documents and need to reset the vector store for each document\n",
    "# Uncomment the following line to delete previous embeddings:\n",
    "\n",
    "# vector_store.delete_collection() \n",
    "\n",
    "# For a single document, there’s no need to clear embeddings as they only pertain to that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a67fdd1-da89-4c31-8f55-ff8dd3eeeff4",
   "metadata": {
    "id": "3a67fdd1-da89-4c31-8f55-ff8dd3eeeff4"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6c13d4d-fdfa-49a2-9543-3d9c7bd2857a",
   "metadata": {
    "id": "f6c13d4d-fdfa-49a2-9543-3d9c7bd2857a"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"summary_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"C:\\chroma\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "899df48f-c477-4e56-bd59-71fc8f0d22cd",
   "metadata": {
    "id": "899df48f-c477-4e56-bd59-71fc8f0d22cd"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "doc_id = [str(uuid.uuid4()) for i in text_docs ]\n",
    "\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "Text_docs_document = [\n",
    "    Document(page_content=s.page_content, metadata={\"id_key\": doc_id[i],\"type\": \"text\"})\n",
    "    for i,s in enumerate(text_docs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58ff3c8f",
   "metadata": {
    "id": "58ff3c8f",
    "outputId": "23e5d226-e96f-4d9f-a6f1-b1b2d502b1bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bb090bcc-2847-49e0-8b87-c2cc3dab9c4e',\n",
       " 'b8d8244a-7e96-4185-bd86-dedd611c6b01',\n",
       " 'f97e84c7-b041-486b-928d-651beec3949d',\n",
       " 'acd97866-22b6-4baf-a206-c8a7a1260ae0',\n",
       " '5867bf48-4967-4367-acba-4d509b8c36ac',\n",
       " 'b26f8496-b8db-49a9-8524-4bb848383643']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=Text_docs_document, ids=doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9e823",
   "metadata": {},
   "source": [
    "**# Here, I am doing similarity search between images_summaries and Text_vectore_embeddings\n",
    "Then, finding image summaries and text which has high similarity and appending them together.\n",
    "Then, Passing those appended_content to the llm context to get detail summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aeddbd34-ba47-40f2-adf5-a670b33810d0",
   "metadata": {
    "id": "aeddbd34-ba47-40f2-adf5-a670b33810d0"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.schema import Document\n",
    "\n",
    "for summary in image_summaries:\n",
    "    # Perform similarity search to get top 1 similar documents\n",
    "    results = vector_store.similarity_search(summary, k=1)\n",
    "\n",
    "    for doc in results:\n",
    "        updated_page_content = f\"{doc.page_content} [RELATED_IMAGE_SUMMARY: {summary}]\"\n",
    "        updated_doc = Document( metadata=doc.metadata,page_content=updated_page_content)\n",
    "        vector_store.update_document(document_id=doc.metadata[\"id_key\"], document=updated_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2637d0d-40dd-48a4-be6e-aae537a05d34",
   "metadata": {
    "id": "a2637d0d-40dd-48a4-be6e-aae537a05d34"
   },
   "outputs": [],
   "source": [
    "'''Here, i am extracting entire content stored in vectore database'''\n",
    "\n",
    "result = vector_store.get()\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "updated_image_text_doc3 = []\n",
    "\n",
    "for doc, meta in zip(result['documents'], result['metadatas']):\n",
    "    # Create a Document object with proper arguments\n",
    "    document = Document(metadata=meta, page_content=doc)\n",
    "    updated_image_text_doc3.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841dbe99",
   "metadata": {},
   "source": [
    "Case:2 Image_summaries and pdf raw text is given as a context to LLM and asked to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48fa2e9f-bb1d-4fed-9195-0503b72d0873",
   "metadata": {
    "id": "48fa2e9f-bb1d-4fed-9195-0503b72d0873"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define prompt\n",
    "prompt1 = ChatPromptTemplate.from_messages(\n",
    "    [(\n",
    "            \"system\",\n",
    "            \"You are a summarization assistant specialized in scientific content. Given the context of a scientific paper, which includes raw text and related image summaries, generate a comprehensive and concise summary. Integrate key points from both the text and image summaries:\\n\"\n",
    "            \"Context:\\n\\n{context} [RELATED_IMAGE_SUMMARY]\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Instantiate chains\n",
    "chain1_1 = create_stuff_documents_chain(llm1, prompt1)\n",
    "chain2_1 = create_stuff_documents_chain(llm2, prompt1)\n",
    "\n",
    "gpt_doc_img_chain_result = chain1_1.invoke({\"context\": updated_image_text_doc3})\n",
    "mistralai_doc_img_chain_result = chain2_1.invoke({\"context\": updated_image_text_doc3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70a789cf",
   "metadata": {
    "id": "70a789cf",
    "outputId": "fbbb0247-ba69-479c-d3b3-433313376554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPt img response result\n",
      "rouge1 - Precision: 0.3147, Recall: 0.3497, Fmeasure: 0.3313\n",
      "rouge2 - Precision: 0.0531, Recall: 0.0590, Fmeasure: 0.0559\n",
      "rougeL - Precision: 0.1353, Recall: 0.1503, Fmeasure: 0.1424\n",
      "\n",
      "\n",
      "mistral img response result\n",
      "rouge1 - Precision: 0.2627, Recall: 0.4902, Fmeasure: 0.3421\n",
      "rouge2 - Precision: 0.0474, Recall: 0.0885, Fmeasure: 0.0617\n",
      "rougeL - Precision: 0.0963, Recall: 0.1797, Fmeasure: 0.1254\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_Score(scores):\n",
    "\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"{metric} - Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, Fmeasure: {score.fmeasure:.4f}\")\n",
    "\n",
    "\n",
    "# ROUGE Score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "\n",
    "score_gpt_mini_img_doc = scorer.score(abstract,gpt_doc_img_chain_result)\n",
    "\n",
    "score_mistral_img_doc = scorer.score(abstract,mistralai_doc_img_chain_result)\n",
    "\n",
    "\n",
    "print(\"GPt img response result\")\n",
    "get_Score(score_gpt_mini_img_doc)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"mistral img response result\")\n",
    "get_Score(score_mistral_img_doc)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5c1d690",
   "metadata": {
    "id": "f5c1d690",
    "outputId": "1c7485f5-ab55-4029-bedc-4a8fd59bf9e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GPT summary BERT (text only)  F1 Score: 0.8631501197814941\n",
      "Mistral AI summary (BERT text) only  F1 Score: 0.8247866034507751\n",
      "\n",
      "\n",
      "GPT summary BERT (text and Images)  F1 Score: 0.8654877543449402\n",
      "Mistral AI summary BERT (text  and Images)  F1 Score:  0.8123260140419006\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "scorer = BERTScorer(lang=\"en\")\n",
    "\n",
    "P1, R1, F1_1 = scorer.score([gpt_chain_result], [abstract])\n",
    "\n",
    "P1_1, R1_1, F1_1_1 = scorer.score([gpt_doc_img_chain_result], [abstract])\n",
    "\n",
    "P2, R2, F1_2 = scorer.score([MistralAI_chain_result], [abstract])\n",
    "\n",
    "P2_1, R2_1, F2_2_1 = scorer.score([mistralai_doc_img_chain_result], [abstract])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"GPT summary BERT (text only)  F1 Score:\", F1_1.tolist()[0])\n",
    "print(\"Mistral AI summary (BERT text) only  F1 Score:\", F1_1_1.tolist()[0])\n",
    "print(\"\\n\")\n",
    "print(\"GPT summary BERT (text and Images)  F1 Score:\", F1_2.tolist()[0])\n",
    "print(\"Mistral AI summary BERT (text  and Images)  F1 Score: \", F2_2_1.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0604f17f",
   "metadata": {
    "id": "0604f17f",
    "outputId": "447f2fdc-0fd9-4e78-9591-4ce14da17761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Article    Model BLEU Score\n",
      "0  Article_1                    \n",
      "1                 GPT     0.5121\n",
      "2             Mistral     0.3533\n"
     ]
    }
   ],
   "source": [
    "bleu_score_gpt2 = sentence_bleu([abstract], gpt_doc_img_chain_result)\n",
    "bleu_score_mistal2 = sentence_bleu([abstract], mistralai_doc_img_chain_result)\n",
    "\n",
    "bleu_score_data1 = {\n",
    "    \"Article\": [\"Article_1\", \"\", \"\"],\n",
    "    \"Model\": [\"\", \"GPT\", \"Mistral\"],\n",
    "    \"BLEU Score\": [\"\", round(bleu_score_gpt2, 4), round(bleu_score_mistal2, 4)]\n",
    "}\n",
    "\n",
    "bleu_scores_df1 = pd.DataFrame(bleu_score_data1)\n",
    "\n",
    "print(bleu_scores_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28dc582",
   "metadata": {},
   "source": [
    "G-EVAl method evalution \n",
    "This part of code is imported from openai cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcdad72a",
   "metadata": {
    "id": "bcdad72a"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Evaluation prompt template based on G-Eval\n",
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions very carefully.\n",
    "Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "{steps}\n",
    "\n",
    "Example:\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{document}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "- {metric_name}\n",
    "\"\"\"\n",
    "\n",
    "# Metric 1: Relevance\n",
    "\n",
    "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
    "Relevance(1-5) - selection of important content from the source. \\\n",
    "The summary should include only important information from the source document. \\\n",
    "Annotators were instructed to penalize summaries which contained redundancies and excess information.\n",
    "\"\"\"\n",
    "\n",
    "RELEVANCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the summary and the source document carefully.\n",
    "2. Compare the summary to the source document and identify the main points of the article.\n",
    "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 2: Coherence\n",
    "\n",
    "COHERENCE_SCORE_CRITERIA = \"\"\"\n",
    "Coherence(1-5) - the collective quality of all sentences. \\\n",
    "We align this dimension with the DUC quality question of structure and coherence \\\n",
    "whereby \"the summary should be well-structured and well-organized. \\\n",
    "The summary should not just be a heap of related information, but should build from sentence to a\\\n",
    "coherent body of information about a topic.\"\n",
    "\"\"\"\n",
    "\n",
    "COHERENCE_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main topic and key points.\n",
    "2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,\n",
    "and if it presents them in a clear and logical order.\n",
    "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 3: Consistency\n",
    "\n",
    "CONSISTENCY_SCORE_CRITERIA = \"\"\"\n",
    "Consistency(1-5) - the factual alignment between the summary and the summarized source. \\\n",
    "A factually consistent summary contains only statements that are entailed by the source document. \\\n",
    "Annotators were also asked to penalize summaries that contained hallucinated facts.\n",
    "\"\"\"\n",
    "\n",
    "CONSISTENCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main facts and details it presents.\n",
    "2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.\n",
    "3. Assign a score for consistency based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 4: Fluency\n",
    "\n",
    "FLUENCY_SCORE_CRITERIA = \"\"\"\n",
    "Fluency(1-5): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
    "1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n",
    "2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "3: Good. The summary has few or no errors and is easy to read and follow.\n",
    "4: Very Good. The summary is almost perfect human readibale but also can be improved.\n",
    "5: Excellent. The summary is pefect and completlely human readibale no improvements needed.\n",
    "\"\"\"\n",
    "\n",
    "FLUENCY_SCORE_STEPS = \"\"\"\n",
    "Read the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_geval_score(\n",
    "    criteria: str, steps: str, document: str, summary: str, metric_name: str\n",
    "):\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        criteria=criteria,\n",
    "        steps=steps,\n",
    "        metric_name=metric_name,\n",
    "        document=document,\n",
    "        summary=summary,\n",
    "    )\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=5,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
    "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
    "    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n",
    "    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n",
    "}\n",
    "\n",
    "summaries = {\"abstract\": abstract,\"gpt_chain_result 1 \" : gpt_chain_result , \"gpt_doc_img_chain_result 1\": gpt_doc_img_chain_result , \"mistralAI_chain_result 1\" : MistralAI_chain_result, \"mistralai_doc_img_chain_result 1\" : mistralai_doc_img_chain_result}\n",
    "\n",
    "data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n",
    "\n",
    "for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
    "    for summ_type, summary in summaries.items():\n",
    "        data[\"Evaluation Type\"].append(eval_type)\n",
    "        data[\"Summary Type\"].append(summ_type)\n",
    "        result = get_geval_score(criteria, steps, abstract, summary, eval_type)\n",
    "        score_num = int(result.strip())\n",
    "        data[\"Score\"].append(score_num)\n",
    "\n",
    "pivot_df = pd.DataFrame(data, index=None).pivot(\n",
    "    index=\"Evaluation Type\", columns=\"Summary Type\", values=\"Score\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f68b3727",
   "metadata": {
    "id": "f68b3727",
    "outputId": "39d697df-bee5-4ae3-8fdf-49a90b60714d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Type     abstract  gpt_chain_result 1   gpt_doc_img_chain_result 1  \\\n",
      "Evaluation Type                                                              \n",
      "Coherence               1                    5                           1   \n",
      "Consistency             5                    5                           1   \n",
      "Fluency                 5                    5                           5   \n",
      "Relevance               5                    5                           1   \n",
      "\n",
      "Summary Type     mistralAI_chain_result 1  mistralai_doc_img_chain_result 1  \n",
      "Evaluation Type                                                              \n",
      "Coherence                               5                                 3  \n",
      "Consistency                             5                                 1  \n",
      "Fluency                                 5                                 5  \n",
      "Relevance                               5                                 1  \n"
     ]
    }
   ],
   "source": [
    "print(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9281940-5385-465e-ab99-6269465965c6",
   "metadata": {
    "id": "c9281940-5385-465e-ab99-6269465965c6",
    "outputId": "7b206cbc-4b49-4d95-91e3-b17031cc58c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'id_key': 'bb090bcc-2847-49e0-8b87-c2cc3dab9c4e', 'type': 'text'}, page_content='2023 Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Noam Shazeer* Google Brain noam@google.com Niki Parmar* Google Research nikip@google.com Jakob Uszkoreit* Google Research usz@google.com Google Research llion@google.com Aidan N. Gomez* ¢ University of Toronto aidan@cs.toronto.edu Lukasz Kaiser* Google Brain lukaszkaiser@google.com illia.polosukhin@gmail.com The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution. [RELATED_IMAGE_SUMMARY: The table presents performance metrics for various parsers evaluated using the WSJ 23 F1 score. It lists different parsers and their respective training methods along with their F1 scores:\\n\\n1. **Vinyals & Kaiser (2014)** - F1 score: 88.3 (discriminative)\\n2. **Petrov et al. (2006)** - F1 score: 90.4 (discriminative)\\n3. **Zhu et al. (2013)** - F1 scores: 90.4 (discriminative) and 91.3 (semi-supervised)\\n4. **Dyer et al. (2016)** - F1 scores: 91.7 (discriminative) and 93.3 (generative)\\n5. **Transformer (4 layers)** - F1 scores: 91.3 (discriminative) and 92.7 (semi-supervised)\\n6. **Huang & Harper (2009)** - F1 score: 91.3 (semi-supervised)\\n7. **McClosky et al. (2006)** - F1 scores: 92.1 (semi-supervised) and 92.2 (semi-supervised)\\n8. **Luong et al. (2015)** - F1 score: 93.0 (multi-task)\\n\\nOverall, the highest F1 score is noted for Dyer et al. (2016) with a generative approach at 93.3.] [RELATED_IMAGE_SUMMARY: The table compares various translation models based on their BLEU scores and training costs (measured in FLOPs) for English to German (EN-DE) and English to French (EN-FR) translations.\\n\\n### BLEU Scores:\\n- **Top Models:**\\n  - MoE: 26.03 (EN-DE), 40.56 (EN-FR)\\n  - ConvS2S: 25.16 (EN-DE), 40.46 (EN-FR)\\n  - GNMT + RL: 24.6 (EN-DE), 39.92 (EN-FR)\\n  - Deep-Att + PosUnk: 23.75 (EN-DE), 39.2 (EN-FR)\\n  - ByteNet: 23.75 (EN-DE)\\n\\n- **Ensembles:**\\n  - ConvS2S Ensemble: 26.36 (EN-DE), 41.29 (EN-FR)\\n  - GNMT + RL Ensemble: 26.30 (EN-DE), 41.16 (EN-FR)\\n  - Deep-Att + PosUnk Ensemble: 40.4 (EN-FR)\\n\\n- **Transformers:**\\n  - Base Model: 27.3 (EN-DE), 38.1 (EN-FR)\\n  - Big Model: 28.4 (EN-DE), 41.8 (EN-FR)\\n\\n### Training Costs:\\n- Ranges from \\\\(9.6 \\\\cdot 10^{18}\\\\) to \\\\(8.0 \\\\cdot 10^{20}\\\\) FLOPs across the models for both EN-DE and EN-FR.\\n\\nOverall, ensemble models tend to perform better in BLEU scores, particularly for EN-FR translations, while transformer models exhibit competitive performance with relatively lower training costs.] [RELATED_IMAGE_SUMMARY: The table compares various configurations of a model, detailing parameters such as:\\n\\n- **N**: Number of layers\\n- **d_model**: Dimensionality of the model\\n- **d_ff**: Dimensionality of the feed-forward layer\\n- **h**: Number of attention heads\\n- **d_k** and **d_v**: Dimensions for key and value vectors\\n- **P_drop**: Dropout probability\\n- **ε_ls**: Label smoothing parameter\\n- **train steps**: Number of training steps\\n- **PPL (dev)**: Perplexity on the development set\\n- **BLEU (dev)**: BLEU score on the development set\\n- **params**: Total parameters in millions\\n\\nKey findings include:\\n- The **base** model has 6 layers with a PPL of 4.92 and a BLEU score of 25.8.\\n- Variations in configurations (A to E) show changes in PPL and BLEU scores, with configuration **(E)** achieving the best performance (PPL 4.33, BLEU 26.4) using positional embeddings instead of sinusoidal ones. \\n\\nThe results indicate that adjustments in model architecture impact performance metrics significantly.]'), Document(metadata={'id_key': '60bdeacd-4f04-46ee-8bcc-83f4548b8d38', 'type': 'text'}, page_content='v1 Initial version. Presents the experiments carried out before the ILSVRC submission. v2 Adds post-submission ILSVRC experiments with training set augmentation using scale jittering, which improves the performance. v3 Adds generalisation experiments (Appendix[B) on PASCAL VOC and Caltech image classifica- tion datasets. The models used for these experiments are publicly available. v4 The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple crops for classification. v6 Camera-ready ICLR-2015 conference paper. Adds a comparison of the net B with a shallow net and the results on PASCAL VOC action classification benchmark. 14 [RELATED_IMAGE_SUMMARY: The table presents mean Average Precision (mAP) results for different configurations of the Faster R-CNN model on the COCO dataset. \\n\\nKey points include:\\n\\n- **Training Data**: COCO train and COCO trainval.\\n- **Test Data**: COCO val and COCO test-dev.\\n- **mAP Scores**:\\n  - Baseline models show mAP scores of 41.5 (VGG-16) and 48.4 (ResNet-101) at IoU threshold 0.5.\\n  - Improvements with additional techniques:\\n    - Box refinement: 49.9\\n    - Context: 51.1\\n    - Multi-scale testing: 53.8\\n- **Ensemble Model**: Achieves the highest scores of 59.0 for COCO trainval and 37.4 for COCO test-dev.\\n\\nOverall, the addition of techniques consistently improves performance across both validation and test datasets.] [RELATED_IMAGE_SUMMARY: The table presents performance metrics for various models evaluated in the ILSVRC challenges. It includes:\\n\\n- **GoogLeNet (ILSVRC\\'14)**: No validation score provided, with a test score of 43.9.\\n- **Our single model (ILSVRC\\'15)**: Validation score of 60.5 and a test score of 58.8.\\n- **Our ensemble (ILSVRC\\'15)**: Validation score of 63.6 and a test score of 62.1.\\n\\nOverall, the ensemble model shows the highest performance in both validation and testing compared to the single model and GoogLeNet.] [RELATED_IMAGE_SUMMARY: The table summarizes the performance of various localization (LOC) methods and networks in terms of their classification error rates. Key points include:\\n\\n- **LOC Methods**: VGG, RPN (Region Proposal Network).\\n- **Networks Used**: VGG-16 and ResNet-101.\\n- **Testing Types**: \"1-crop\" and \"dense.\"\\n- **LOC Error on GT CLS**: \\n  - VGG-16: 33.1%\\n  - ResNet-101 (1-crop): 13.3%\\n  - ResNet-101 (dense): 11.7%\\n- **Classification Network**: Primarily ResNet-101 and an ensemble approach.\\n- **Top-5 LOC Error on Predicted CLS**:\\n  - ResNet-101: 14.4%\\n  - RPN+ResNet-101: 10.6%\\n  - RPN+RCNN ensemble: 8.9%\\n\\nOverall, the results indicate that using ensemble methods and dense testing improves classification performance.] [RELATED_IMAGE_SUMMARY: The table presents the top-5 localization error rates (in percentage) for various methods evaluated on the ILSVRC dataset across validation and test sets. \\n\\n- **OverFeat (ILSVRC\\'13)**: 30.0 (val), 29.9 (test)\\n- **GoogLeNet (ILSVRC\\'14)**: Not applicable (val), 26.7 (test)\\n- **VGG (ILSVRC\\'14)**: 26.9 (val), 25.3 (test)\\n- **Ours (ILSVRC\\'15)**: 8.9 (val), 9.0 (test)\\n\\nThe method \"Ours\" shows the lowest localization error, significantly outperforming the other methods listed.] [RELATED_IMAGE_SUMMARY: The table compares the performance of two metrics, VGG-16 and ResNet-101, based on mean Average Precision (mAP) at different thresholds. \\n\\n- For mAP at 0.5, VGG-16 scores 41.5, while ResNet-101 scores higher at 48.4.\\n- For mAP at the range [0.5, 0.95], VGG-16 achieves 21.2, and ResNet-101 shows an improved score of 27.2. \\n\\nOverall, ResNet-101 outperforms VGG-16 in both metrics.] [RELATED_IMAGE_SUMMARY: The table presents the performance of two models, VGG-16 and ResNet-101, on different training and test datasets. \\n\\n- For the training data \"07+12\":\\n  - On the VOC 07 test, VGG-16 achieved 73.2, while ResNet-101 scored 76.4.\\n  \\n- For the training data \"07++12\":\\n  - On the VOC 12 test, VGG-16 scored 70.4, and ResNet-101 scored 73.8.\\n\\nOverall, ResNet-101 consistently outperformed VGG-16 across both test datasets.]'), Document(metadata={'id_key': 'f97e84c7-b041-486b-928d-651beec3949d', 'type': 'text'}, page_content='We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2. Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3p, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3. Optimizer We used the Adam optimizer with 6; = 0.9, 82 = 0.98 and « = 10~°. We varied the learning rate over the course of training, according to the formula: This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000. 5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Parop = 0.1. Label Smoothing During training, we employed label smoothing of value €,, = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table[2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table|3| Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pop = 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty a = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table[2]summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPup| 6.2. Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the 5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table[3] In Table[3|rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section [3.2.2] While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table B]rows (B), we observe that reducing the attention key size dj, hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with d;ode1 = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences (B7]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section[5.4p, learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table [4] show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com, tensorflow/tensor2tensor Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv: 1607.06450, 2016. [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.'), Document(metadata={'id_key': 'acd97866-22b6-4baf-a206-c8a7a1260ae0', 'type': 'text'}, page_content='Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017. [4 Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv: 1601.06733, 2016. 5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rmn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. 6 Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv: 1610.02357, 2016. Junyoung Chung, Caglar Giilgehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. 7 8 Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016. 9 Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint\\\\arXiv: 1705.03 1222, 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint 2013. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016. 1 2] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jiirgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001. 3] Sepp Hochreiter and Jiirgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. 4] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832-841. ACL, August 2009. 5] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint\\\\arXiv:1602.02410, 2016.')]\n"
     ]
    }
   ],
   "source": [
    "query = \"what is GNMT + RL Ensemble bleu score and training cost \"\n",
    "\n",
    "def relavent_info(query):\n",
    "    result = vector_store.similarity_search(query)\n",
    "    print(result)\n",
    "    aggregated_text = \" \".join([doc.page_content for doc in result])\n",
    "    return aggregated_text\n",
    "\n",
    "result = relavent_info(query)\n",
    "\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Option 1: LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0.5)\n",
    "\n",
    "# RAG pipeline\n",
    "chain2 = (\n",
    "    {\n",
    "        \"context\": RunnablePassthrough(),  # Wrap context in RunnablePassthrough\n",
    "        \"question\": RunnablePassthrough(),  # Same for the question\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "output = chain2.invoke({\"context\": result, \"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44b4f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GNMT + RL Ensemble has a BLEU score of 26.30 for English-to-German (EN-DE) and 41.16 for English-to-French (EN-FR). The training costs for these models range from \\(9.6 \\cdot 10^{18}\\) to \\(8.0 \\cdot 10^{20}\\) FLOPs.\n"
     ]
    }
   ],
   "source": [
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
