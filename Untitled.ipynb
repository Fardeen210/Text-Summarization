{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ca9ccf-af25-49d9-b1ad-5de0f1fd77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installed all relavant library to acess unstructured open source library,\n",
    "\n",
    "# Before installing there are bunch of other supporting libraries need to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "3b9331db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"enter gpt api key\")\n",
    "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"enter MISTRAL api key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "3a73f8dd-6b62-4d58-ac41-ab223a385a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unstructured\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "5df82ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = partition_pdf(r\"C:\\Text_Summarization\\Article 5\\article 5.pdf\",\n",
    "                        strategy=\"auto\",\n",
    "                        extract_images_in_pdf=False,  # Don't extract images\n",
    "                        infer_table_structure=False,  # Don't infer table structure\n",
    "                        skip_infer_table_types=[\"pdf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "c3aa3f45-96e1-4ef9-bbb3-f9d51cb81639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize lists to categorize elements\n",
    "Header = []\n",
    "Title = []\n",
    "Footer = []\n",
    "Text = []\n",
    "List_item = []\n",
    "Narrative_text = []\n",
    "Document_order = []\n",
    "abstract = []\n",
    "\n",
    "for i, element in enumerate(elements):\n",
    "    # Get the next elements, if they exist\n",
    "    next_ele = elements[i + 1] if i + 1 < len(elements) else None\n",
    "    next_ele1 = elements[i + 2] if i + 2 < len(elements) else None\n",
    "\n",
    "    if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
    "        Header.append(str(element))\n",
    "    \n",
    "    elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
    "        Footer.append(str(element))\n",
    "    \n",
    "    elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
    "        title = str(element)\n",
    "        \n",
    "        # Check for \"Abstract\" and extract one or two NarrativeText paragraphs\n",
    "        if title == \"Abstract\":\n",
    "            if next_ele and \"unstructured.documents.elements.NarrativeText\" in str(type(next_ele)):\n",
    "                abstract.append(str(next_ele))\n",
    "            if next_ele1 and \"unstructured.documents.elements.NarrativeText\" in str(type(next_ele1)):\n",
    "                abstract.append(str(next_ele1))\n",
    "\n",
    "        if title == \"Conclusion\":\n",
    "                Title.append(title)\n",
    "                Document_order.append(title)\n",
    "                Document_order.append(next_ele)\n",
    "                print(\"Conclusion found, exiting loop.\")\n",
    "                break\n",
    "        \n",
    "        # Process numeric titles\n",
    "        if title and title[0].isdigit():            \n",
    "            Title.append(title)\n",
    "            Document_order.append(title)\n",
    "            \n",
    "    elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "        Narrative_text.append(str(element))\n",
    "        Document_order.append(str(element))\n",
    "    elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
    "        Text.append(str(element))\n",
    "        Document_order.append(str(element))\n",
    "    elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
    "        List_item.append(str(element))\n",
    "        Document_order.append(str(element))\n",
    "# Combine the document order into a single string\n",
    "Document_order = ' '.join(Document_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "48c13753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"arXiv:1409.1556v6 [cs.CV] 10 Apr 2015 Published as a conference paper at ICLR 2015 Karen Simonyan* & Andrew Zisserman* Visual Geometry Group, Department of Engineering Science, University of Oxford {karen,az}@robots.ox.ac.uk In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 x 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa- tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili- tate further research on the use of deep visual representations in computer vision. 1 INTRODUCTION Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale _im- age and video recognition (Krizhevsky et al. |2012; |Zeiler & Fergus, 20135 Sermanet et all 2014; [Simonyan & Zisserman|,|2014) which has become possible due to the large public image reposito- ries, such as ImageNet (Deng et al |2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al.)|2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recog- nition Challenge (ILSVRC) , which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature en- codings (the winner of ILSVRC-2011) to deep ConvNets (the winner of ILSVRC-2012). With ConvNets becoming more of a commodity in the computer vision field, a number of at- tempts have been made to improve the original architecture of [Krizhevsky et al] (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC- 2013 (Zeiler & Fergus, (2013; [Sermanet et al |2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt_with training and testing the networks densely over the whole image and over multiple scales 014 [Howard D014), In this paper, we address another important aspect of ConvNet architecture design — its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 x 3) convolution filters in all layers. As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning). We have released our two best-performing model] to facilitate further research. The rest of the paper is organised as follows. In Sect. [2] we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect.[3] and the Published as a conference paper at ICLR 2015 configurations are compared on the ILSVRC classification task in Sect. [4] Sect. [5] concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix[A] and discuss the generalisation of very deep features to other datasets in Appendix] Finally, Appendix[C|contains the list of major paper revisions. 2 CONVNET CONFIGURATIONS To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by (Citesan et ail (2011): |Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect.[2.1) and then detail the specific configurations used in the evaluation (Sect.[2.2). Our design choices are then discussed and compared to the prior art in Sect. [2.3] 2.1 ARCHITECTURE During training, the input to our ConvNets is a fixed-size 224 x 224 RGB image. The only pre- processing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 x 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 x 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 x 3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 x 2 pixel window, with stride 2. A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000- way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks. All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al} |2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al 12012): as will be shown in Sect. such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory con- sumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al.||2012). 2.2 CONFIGURATIONS The ConvNet configurations, evaluated in this paper, are outlined in Table[I] one per column. In the following we will refer to the nets by their names (A—E). All configurations follow the generic design presented in Sect. [2.1] and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512. In Table[2]we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in [2014)). 2.3. DISCUSSION Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al.) |2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; [Sermanet et al 2014). Rather than using relatively large receptive fields in the first conv. lay- ers (e.g. 11 x 11 with stride 4 in (Krizhevsky et al|,|2012), or 7 x 7 with stride 2 in (Zeiler & Fergus, 2013; 2014)), we use very small 3 x 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 x 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 x 5; three Published as a conference paper at ICLR 2015 Table 1: ConvNet configurations (shown in columns). The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as “conv (receptive field size)-(number of channels)”. The ReLU activation function is not shown for brevity. 16 weight layers layers input (224 x 224 RGB image) conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 19 weight layers conv3-128 | conv3-128 | conv3-128 | conv3-128 | conv3-128 | conv3-128 conv3-128 | conv3-128 | conv3-128 | conv3-128 maxpool conv3-256 conv3-256 conv3-256 | conv3-256 conv3-256 | conv3-256 conv3-256 | conv3-256 | conv3-256 conv3-256 | conv3-256 | conv3-256 convl-256 | conv3-256 | conv3-256 conv3-256 maxpool conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv1-512 | conv3-512 | conv3-512 conv3-512 maxpool conv3-512 | conv3-512 | conv3-512 |] conv3-512 | conv3-512 | conv3-512 conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv1-512 | conv3-512 | conv3-512 conv3-512 Table 2: Number of parameters (in millions). Network A,A-LRN B C D E such layers have a 7 x 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3 x 3 conv. layers instead of a single 7 x 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 x 3 convolution stack has C channels, the stack is parametrised by 3 (3°C?) = 270? weights; at the same time, a single 7 x 7 conv. layer would require 77C? = 49C? parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7 x 7 conv. filters, forcing them to have a decomposition through the 3 x 3 filters (with non-linearity injected in between). The incorporation of 1 x 1 conv. layers (configuration C, Table[I) is a way to increase the non- linearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 x 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1 x 1 conv. layers have recently been utilised in the “Network in Network” architecture of|Lin et al] (2014). Small-size convolution filters have been previously used by (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. [Goodfellow et al] (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al.,|2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets Published as a conference paper at ICLR 2015 (22 weight layers) and small convolution filters (apart from 3 x 3, they also use 1 x 1 and 5 x 5 convolutions). Their network topology is, however, more complex than ours, and the spatial reso- lution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. our model is outperforming that of |Szegedy et al] in terms of the single-network classification accuracy. 3. CLASSIFICATION FRAMEWORK In the previous section we presented the details of our network configurations. In this section, we describe the details of classification ConvNet training and evaluation. 3.1 TRAINING The ConvNet training procedure generally follows (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et all (1929) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the Lz penalty multiplier set to 5-10~4) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to 10~?, and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al||2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers. The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table[I), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully- connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and 10~? variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of|Glorot & Bengio (2010). To obtain the fixed-size 224 x 224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al||2012). Training image rescaling is explained below. Training image size. Let S be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to S' as the training scale). While the crop size is fixed to 224 x 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for S' > 224 the crop will correspond to a small part of the image, containing a small object or an object part. We consider two approaches for setting the training scale S. The first is to fix S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi- scale image statistics). In our experiments, we evaluated models trained at two fixed scales:_S = 256 (which has been widely used in the prior art Somnnetetail Dora) and S = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of 107°. The second approach to setting S' is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [Smin, Smax] (we used Sirin = 256 and Smax = 512). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single Published as a conference paper at ICLR 2015 model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same configuration, pre-trained with fixed S = 384. 3.2 TESTING At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect.4] using several values of Q for each S' leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sma TO Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 x 7 conv. layer, the last two FC layers to 1 x 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image. Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time 2012), which is less efficient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szewedy etal] (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 x 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by oeeed otal (2014). 3.3. IMPLEMENTATION DETAILS Our implementation is derived from the publicly available C++ Caffe toolbox ( (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU. While more sophisticated methods of speeding up ConvNet training have been recently pro- posed (Krizhevsky, (2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2-3 weeks depending on the architecture. 4 CLASSIFICATION EXPERIMENTS Dataset. In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012-2014 chal- lenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The clas- sification performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the Published as a conference paper at ICLR 2015 main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories. For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al. 2014). 4.1 SINGLE SCALE EVALUATION We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in Sect.[2.2] The test image size was set as follows: Q = S for fixed S,and Q =0.5(Smin + Smax) for jittered S € [Simin, Smax]. The results of are shown in TableB] First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B—E). Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 x 1 conv. layers), performs worse than the configuration D, which uses 3 x 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 x 5 conv. layers, which was derived from B by replacing each pair of 3 x 3 conv. layers with a single 5 x 5 conv. layer (which has the same receptive field as explained in Sect.[2.3). The top-! error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters. Finally, scale jittering at training time (S € [256;512]) leads to significantly better results than training on images with fixed smallest side (S = 256 or S = 384), even though a single scale is used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics. ConvNet config. (TableLI) [smallest image side | top-I val. error (%) | top-5 val. error (%) train(S) | test (Q) A A-LRN B Cc [2563512] 256 D 384 E 4.2 MULTI-SCALE EVALUATION Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: Q = {S — 32,5,5 + 32}. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable S € [Sinin3 Smaz] was evaluated over a larger range of sizes Q = {Smin,0.5(Simin + Smazx); Smaz }- Published as a conference paper at ICLR 2015 The results, presented in Table[4] indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table B). As before, the deepest configurations (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table[4). On the test set, the configuration E achieves 7.3% top-5 error. ConvNet config. (Table[I) smallest image si top-1 val. error (%) | top-5 val. error (%) train (S) B 256 256 256,288 : 5 Cc 384 352,384,416 27.8 9.2 [256; 512] | 256,384,512 256 224,256,288 384,416 256; 512] | 256,384,512 ; 256 224,256,288 26.9 &7 RA 84.4 i 3 0 256; 512] | 256,384,512 4.3. MULTI-CROP EVALUATION In Table [5] we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2] for de- tails). We also assess the complementarity of the two evaluation techniques by averaging their soft- max outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions. Table 5: ConvNet evaluation techniques comparison. In all experiments the training scale S was sampled from [256; 512], and three test scales Q were considered: {256, 384,512}. ConvNet config. (Tablel) [ Evaluation method | top-I val. error (%) | top-5 val. error (%) lense 24.8 75 D ti-crop multi-crop & 4.4 CONVNET FUSION Up until now, we evaluated the performance of individual ConvNet models. In this part of the exper- iments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et all, and 2013 (Zeiler & Fergus, 2013: [Sermanet et al} 2014). The results are shown in Table [6] By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table[5). 4.5 COMPARISON WITH THE STATE OF THE ART Finally, we compare our results with the state of the art in Table [7] In the classification task of ILSVRC-2014 challenge (Russakovsky et al (2014), our “VGG” team secured the 2nd place with Published as a conference paper at ICLR 2015 Combined ConvNet models top-I val] top-5 val ILSVRC submission (D/256/224,256,288), (D/384/352,384,416), (D/[256;512]/256,384,512) (C/256/224,256,288), (C/384/352,384,416) (E/256/224,256,288), (E/384/352,384,416) 15 73 (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), dense eval. D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop & dense eval. 7A 7.0 7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models. As can be seen from Table[J] our very deep ConvNets significantly outperform the previous gener- ation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competi- tions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models — significantly less than used in most ILSVRC submissions. In terms of the single- -net performance, our architecture achieves the best result (7.0% test error), outperforming a single Goog et by 0.9%. Notably, we did not depart from the classical ConvNet architecture of [LeCun et al] a Goed , but improved it by substantially increasing the depth. Table 7: Comparison with the state of the art in ILSVRC classification. Our method is denoted as “VGG”. Only the results obtained without outside training data are reported. Method top-1 val. error (%)|top-5 val. error (%)|top-5 test error (%) VGG (2 nets, multi-crop & dense eval.) 23.7 6.8 6.8 VGG (I net, multi-crop & dense eval.) 24.4 7A 7.0 VGG (ILSVRC submission, 7 nets, dense eval.) 24.7 75 7.3 GoogLeNet (Szegedy et al., 2014) (I net) - 79 MSRA (He et al. sor tt nets) MSRA (He et al., 2014) (1 net) Clarifai Russakousiy et a 2014) (multiple nets) Zeiler & Forus Zeiler & Feraus st a sat OverFeat (Sermanet et al., 2014) (7 nets) OverFeat (Sermanet ct al., 2014) (1 net) Krizhevsky et al. Rrchesky et al., 2012) (1 net) 5 CONCLUSION In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large- scale image classification. It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al.,{1989%|Krizhevsky et al.,|2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again confirm the importance of depth in visual representations. This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. Published as a conference paper at ICLR 2015 Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context database. CoRR, abs/1412.0623, 2014. Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. In Proc. BMVC., 2014. Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation. CoRR, abs/1411.6836, 2014. Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance convolutional neural networks for image classification. In LJCAI, pp. 1237-1242, 2011. Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In N/PS, pp. 1232-1240, 2012. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proc. CVPR, 2009. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013. Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual object classes challenge: A retrospective. LJCV, 111(1):98-136, 2015. Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In EEE CVPR Workshop of Generative Model Based Vision, 2004. Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014. Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604, 2014. Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. AISTATS, volume 9, pp. 249-256, 2010. Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street view imagery using deep convolutional neural networks. In Proc. ICLR, 2014. Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR, abs/1406.4729v2, 2014. Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014. Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc. ICLR, 2014. Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR, abs/1412.2306, 2014. Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539, 2014. Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014. Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural net- works. In N/PS, pp. 1106-1114, 2012. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropa- gation applied to handwritten zip code recognition. Neural Computation, 1(4):541-551, 1989. Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. JCLR, 2014. Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038, 2014. Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks. In Proc. CVPR, 2014. Perronnin, F., Sanchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In Proc. ECCV, 2010. Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline for Recognition. CoRR, abs/1403.6382, 2014. Published as a conference paper at ICLR 2015 Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014. Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR, abs/1406.2199, 2014. Published in Proc. NIPS, 2014. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR, abs/1406.5726, 2014. Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. Published in Proc. ECCV, 2014. In the main body of the paper we have considered the classification task of the ILSVRC challenge, and performed a thorough evaluation of ConvNet architectures of different depth. In this section, we turn to the localisation task of the challenge, which we have won in 2014 with 25.3% error. It can be seen as a special case of object detection, where a single object bounding box should be predicted for each of the top-5 classes, irrespective of the actual number of objects of the class. For this we adopt the approach of|Sermanet et al! (2014), the winners of the ILSVRC-2013 localisation challenge, with a few modifications. Our method is described in Sect.[A_Iand evaluated in Sect.[A.2] To perform object localisation, we use a very deep ConvNet, where the last fully connected layer predicts the bounding box location instead of the class scores. A bounding box is represented by a 4-D vector storing its center coordinates, width, and height. There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR [2014)) or is class-specific (per-class regression, PCR). In the former case, the last layer is 4-D, while in the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table[I), which contains 16 weight layers and was found to be the best-performing in the classification task (Sect.[4). Training. Training of localisation ConvNets is similar to that of the classification ConvNets (Sect.B.I). The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth. We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 submission). Training was initialised with the corresponding classification models (trained on the same scales), and the initial learning rate was set to 10-3. We explored both fine-tuning all layers and fine-tuning only the first two fully-connected layers, as done in (Sermanet et all 2014), The last fully-connected layer was initialised randomly and trained from scratch. Testing. We consider two testing protocols. The first is used for comparing different network modifications on the validation set, and considers only the bounding box prediction for the ground truth class (to factor out the classification errors). The bounding box is obtained by applying the network only to the central crop of the image. The second, fully-fledged, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classification task (Sect. [3.2). The difference is that instead of the class score map, the output of the last fully-connected layer is a set of bounding box predictions. To come up with the final prediction, we utilise the greedy merging procedure of (2014), which first merges spatially close predictions (by averaging their coor- dinates), and then rates them based on the class scores, obtained from the classification ConvNet. When several localisation ConvNets are used, we first take the union of their sets of bounding box predictions, and then run the merging procedure on the union. We did not use the multiple pooling 10 Published as a conference paper at ICLR 2015 offsets technique of|Sermanet et al] (2014), which increases the spatial resolution of the bounding box predictions and can further improve the results. A.2 LOCALISATION EXPERIMENTS In this section we first determine the best-performing localisation setting (using the first test proto- col), and then evaluate it in a fully-fledged scenario (the second protocol). The localisation error is measured according to the ILSVRC criterion iRuasakoweky ol DOTS, ie. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5. Settings comparison. As can be seen from Table[8] per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of Bemmanet et all (2014), where PCR was outperformed by SCR. We also note that fine-tuning all layers for the lo- calisation task leads to noticeably better results than fine-tuning only the fully-connected layers (as done in (Sermanet et al.,|2014)). In these experiments, the smallest images side was set to S = 384; the results with S' = 256 exhibit the same behaviour and are not shown for brevity. Table 8: Localisation error for different modifications with the simplified testing protocol: the bounding box is predicted from a single central image crop, and the ground-truth class is used. All ConvNet layers (except for the last one) have the configuration D (Table [I), while the last layer performs either single-class regression (SCR) or per-class regression (PCR). Fine-tuned layers]regression type]GT class localisation error 1st and 2nd FC Fully-fledged evaluation. Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted us- ing our best-performing classification system (Sect. 45) ), and multiple densely-computed bounding box predictions are merged using the method of [Sermanet et al] (014). As can be seen from Ta- ble[9] application of the localisation ConvNet to the whole image substantially improves the results compared to using a center crop (Table[8), despite using the top-5 predicted class labels instead of the ground truth. Similarly to the classification task (Sect.[4), testing at several scales and combining the predictions of multiple networks further improves the performance. Table 9: Localisation error smallest image side top-5 localisation error (%) test (Q) val. test. 256 29.5 26.7 384 352,384 [ fusion: 256/256 and 384/352,384 | 26.9 25.3 Comparison with the state of the art. We compare our best localisation result with the state of the art in Table ma With 25.3% test error, our “VGG” team won the localisation challenge of ILSVRC-2014 (Russakovsky et al |2014). Notably, our results are considerably better than those of the ILSVRC-2013 winner Overfeat (Sermanet et al] (2014), even though we used less scales and did not employ their resolution enhancement technique. We envisage that better localisation per- formance can be achieved if this technique is incorporated into our method. This indicates the performance advancement brought by our very deep ConvNets — we got better results with a simpler localisation method, but a more powerful representation. In the previous sections we have discussed training and evaluation of very deep ConvNets on the ILSVRC dataset. In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature 11 Published as a conference paper at ICLR 2015 Table 10: Comparison with the state of the art in ILSVRC localisation. Our method is denoted Krizhevsky et al. (Krizhevsky et al., 2012 extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting. Recently, there has been a lot of interest in such a use case Donahue et alJ, 2013} [Razavian et al) [20144 Chatfield et al 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin. Following that line of work, we investigate if our models lead to better performance than more shallow models utilised in the state-of-the-art methods. In this evaluation, we consider two models with the best classification performance on ILSVRC (Sect.[4) — configurations “Net-D” and “Net-E” (which we made publicly available). To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales. The resulting image descriptor is L2-normalised and combined with a linear SVM classifier, trained on the target dataset. For simplicity, pre-trained ConvNet weights are kept fixed (no fine-tuning is performed). Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure (Sect. [3.2). Namely, an image is first rescaled so that its smallest side equals Q, and then the net- work is densely applied over the image plane (which is possible when all weight layers are treated as convolutional). We then perform global average pooling on the resulting feature map, which produces a 4096-D image descriptor. The descriptor is then averaged with the descriptor of a hori- zontally flipped image. As was shown in Sect. [4.2] evaluation over multiple scales is beneficial, so we extract features over several scales Q. The resulting multi-scale features can be either stacked or pooled across scales. Stacking allows a subsequent classifier to learn how to optimally combine image statistics over a range of scales; this, however, comes at the cost of the increased descriptor dimensionality. We return to the discussion of this design choice in the experiments below. We also assess late fusion of features, computed using two networks, which is performed by stacking their respective image descriptors. Table 11: Comparison with the state of the art in image classification on VOC-2007, VOC-2012, Caltech-101, and Caltech-256. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (2000 classes). Method VOC-2007 Caltech-101 Caltech-256 (mean AP) | (mean AP) |(mean class recall) | (mean class recall) Zeiler & Fergus (Zeiler & Fergus, 2013) - . 86.5+£0.5 74.2 £0.3 Chatfield et al. (Chatfield et al., 2014) 82.4 . 88.4 + 0.6 776+0.1 He et al. (He et al., 2014) 82.4 93.4 + 0.5 - Wei et al. (Wei et al., 2014) 81.5 (85.2*) - - VGG Net-D (16 layers) ; . 918+ 71.0 85.0 £ 0.2 VGG Net-E (19 layers) . . 92.3 + 0.5 85.1 40.3 VGG Net-D & Net-E . . 92.7 + 0.5 86.2 + 0.3 Image Classification on VOC-2007 and VOC-2012. We begin with the evaluation on the imag classification task of PASCAL VOC-2007 and VOC-2012 benchmarks Bois), These datasets contain 10K and 22.5K images respectively, and each image is annotated with one or several labels, corresponding to 20 object categories. The VOC organisers provide a pre-defined split into training, validation, and test data (the test data for VOC-2012 is not publicly available; instead, an official evaluation server is provided). Recognition performance is measured using mean average precision (mAP) across classes. Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs sim- 12 Published as a conference paper at ICLR 2015 ilarly to the aggregation by stacking. We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-specific seman- tics which a classifier could exploit. Since averaging has a benefit of not inflating the descrip- tor dimensionality, we were able to aggregated image descriptors over a wide range of scales: Q © {256, 384,512,640, 768}. It is worth noting though that the improvement over a smaller range of {256, 384, 512} was rather marginal (0.3%). The test set performance is reported and compared with other approaches in Table[I1] Our networks “Net-D” and “Net-E” exhibit identical performance on VOC datasets, and their combination slightly improves the results. Our methods set the new state of the art across image representations, pre- trained on the ILSVRC dataset, outperforming the previous best result oftChatfield et all (2014) by more than 6%. It should be noted that the method of (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, semantically close to those in VOC datasets. It also benefits from the fusion with an object detection-assisted classification pipeline. Image Classification on Caltech-101 and Caltech-256. In this section we evaluate very deep fea- tures on Caltech-101 and Caltech-256 image classification benchmarks. Caltech-101 contains 9K images labelled into 102 classes (101 object categories and a background class), while Caltech-256 is larger with 31K images and 257 classes. A standard eval- uation protocol on these datasets is to generate several random splits into training and test data and report the average recognition performance across the splits, which is measured by the mean class recall (which compensates for a different number of test images per class). Following|Chatfield et al (2014); (2013); [He et al] (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training images per class (and the rest is used for testing). In each split, 20% of training images were used as a validation set for hyper-parameter selection. We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multi- ple scales, performs better than averaging or max-pooling. This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are se- mantically different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such scale-specific representations. We used three scales Q € {256, 384, 512}. Our models are compared to each other and the state of the art in Table[11] As can be seen, the deeper 19-layer Net-E performs better than the 16-layer Net-D, and their combination further improves the performance. On Caltech-101, our representations are competitive with the approach of (014, which, however, performs significantly worse than our nets on VOC-2007. On Caltech-256, our features outperform the state of the art (Chatfield et al] by a large margin (8.6%). Action Classification on VOC-2012. We also evaluated our best-performing image representa- tion (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task Eecinshae call BOTs) which consists in predicting an action class from a single image, given a bounding box of the person performing the action. The dataset contains 4.6K training im- ages, labelled into 11 classes. Similarly to the VOC-2012 object classification task, the performance is measured using the mAP. We considered two training settings: (i) computing the ConvNet fea- tures on the whole image and ignoring the provided bounding box; (ii) computing the features on the whole image and on the provided bounding box, and stacking them to obtain the final representation. The results are compared to other approaches in Table[I2] Our representation achieves the state of art on the VOC action classification task even without using the provided bounding boxes, and the results are further improved when using both images and bounding boxes. Unlike other approaches, we did not incorporate any task-specific heuristics, but relied on the representation power of very deep convolutional features. Other Recognition Tasks. Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, consistently outperform- ing more shallow representations. For instance, (2014) achieve the state of the object detection results by replacing the ConvNet of |Krizhevsky et al| (2012) with our 16-layer 2012) have been ob- 13 Published as a conference paper at ICLR 2015 Table 12: Comparison with the state of the art in single-image action classification on VOC- 2012. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (1512 classes). Hoai, 2014 VGG Net-D & Net-E, image and bounding box 84.0 | served in semantic segmentation (Long et al, [2014), image caption generation (Kiros etal} 2014; Karpathy & Fei-Feil|2014), texture and material recognition 2014: |Bell et al.,|2014). Here we present the list of major paper revisions, outlining the substantial changes for the conve- nience of the reader. v1 Initial version. Presents the experiments carried out before the ILSVRC submission. v2 Adds post-submission ILSVRC experiments with training set augmentation using scale jittering, which improves the performance. v3 Adds generalisation experiments (Appendix[B) on PASCAL VOC and Caltech image classifica- tion datasets. The models used for these experiments are publicly available. v4 The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple crops for classification. v6 Camera-ready ICLR-2015 conference paper. Adds a comparison of the net B with a shallow net and the results on PASCAL VOC action classification benchmark. 14\""
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "b91a4ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884\n",
      "52405\n"
     ]
    }
   ],
   "source": [
    "abstract = ''.join(abstract)\n",
    "print(len(abstract))\n",
    "print(len(Document_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "ed75c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract= '  In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3×3) convolutionfilters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–1 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "d7bd3029-2295-43a6-af6b-d23de9e435b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#Here we are using sematic Chunking from langchain\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings(),number_of_chunks=10, min_chunk_size=1500)\n",
    "text_docs = text_splitter.create_documents([Document_order])\n",
    "print(len(text_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "73c3b514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"arXiv:1409.1556v6 [cs.CV] 10 Apr 2015 Published as a conference paper at ICLR 2015 Karen Simonyan* & Andrew Zisserman* Visual Geometry Group, Department of Engineering Science, University of Oxford {karen,az}@robots.ox.ac.uk In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 x 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa- tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili- tate further research on the use of deep visual representations in computer vision. 1 INTRODUCTION Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale _im- age and video recognition (Krizhevsky et al. |2012; |Zeiler & Fergus, 20135 Sermanet et all 2014; [Simonyan & Zisserman|,|2014) which has become possible due to the large public image reposito- ries, such as ImageNet (Deng et al |2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al.)|2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recog- nition Challenge (ILSVRC) , which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature en- codings (the winner of ILSVRC-2011) to deep ConvNets (the winner of ILSVRC-2012). With ConvNets becoming more of a commodity in the computer vision field, a number of at- tempts have been made to improve the original architecture of [Krizhevsky et al] (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC- 2013 (Zeiler & Fergus, (2013; [Sermanet et al |2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt_with training and testing the networks densely over the whole image and over multiple scales 014 [Howard D014), In this paper, we address another important aspect of ConvNet architecture design — its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 x 3) convolution filters in all layers. As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning). We have released our two best-performing model] to facilitate further research. The rest of the paper is organised as follows. In Sect. [2] we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect.[3] and the Published as a conference paper at ICLR 2015 configurations are compared on the ILSVRC classification task in Sect. [4] Sect. [5] concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix[A] and discuss the generalisation of very deep features to other datasets in Appendix] Finally, Appendix[C|contains the list of major paper revisions. 2 CONVNET CONFIGURATIONS To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by (Citesan et ail (2011): |Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect.[2.1) and then detail the specific configurations used in the evaluation (Sect.[2.2). Our design choices are then discussed and compared to the prior art in Sect. [2.3] 2.1 ARCHITECTURE During training, the input to our ConvNets is a fixed-size 224 x 224 RGB image. The only pre- processing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 x 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 x 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 x 3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 x 2 pixel window, with stride 2. A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000- way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks. All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al} |2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al 12012): as will be shown in Sect. such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory con- sumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al.||2012). 2.2 CONFIGURATIONS The ConvNet configurations, evaluated in this paper, are outlined in Table[I] one per column. In the following we will refer to the nets by their names (A—E). All configurations follow the generic design presented in Sect. [2.1] and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512. In Table[2]we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in [2014)). 2.3. DISCUSSION Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al.) |2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; [Sermanet et al 2014). Rather than using relatively large receptive fields in the first conv. lay- ers (e.g. 11 x 11 with stride 4 in (Krizhevsky et al|,|2012), or 7 x 7 with stride 2 in (Zeiler & Fergus, 2013; 2014)), we use very small 3 x 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 x 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 x 5; three Published as a conference paper at ICLR 2015 Table 1: ConvNet configurations (shown in columns). The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as “conv (receptive field size)-(number of channels)”. The ReLU activation function is not shown for brevity. 16 weight layers layers input (224 x 224 RGB image) conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 19 weight layers conv3-128 | conv3-128 | conv3-128 | conv3-128 | conv3-128 | conv3-128 conv3-128 | conv3-128 | conv3-128 | conv3-128 maxpool conv3-256 conv3-256 conv3-256 | conv3-256 conv3-256 | conv3-256 conv3-256 | conv3-256 | conv3-256 conv3-256 | conv3-256 | conv3-256 convl-256 | conv3-256 | conv3-256 conv3-256 maxpool conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv1-512 | conv3-512 | conv3-512 conv3-512 maxpool conv3-512 | conv3-512 | conv3-512 |] conv3-512 | conv3-512 | conv3-512 conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv1-512 | conv3-512 | conv3-512 conv3-512 Table 2: Number of parameters (in millions). Network A,A-LRN B C D E such layers have a 7 x 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3 x 3 conv. layers instead of a single 7 x 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 x 3 convolution stack has C channels, the stack is parametrised by 3 (3°C?) = 270? weights; at the same time, a single 7 x 7 conv. layer would require 77C? = 49C? parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7 x 7 conv. filters, forcing them to have a decomposition through the 3 x 3 filters (with non-linearity injected in between). The incorporation of 1 x 1 conv. layers (configuration C, Table[I) is a way to increase the non- linearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 x 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1 x 1 conv. layers have recently been utilised in the “Network in Network” architecture of|Lin et al] (2014). Small-size convolution filters have been previously used by (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. [Goodfellow et al] (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al.,|2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets Published as a conference paper at ICLR 2015 (22 weight layers) and small convolution filters (apart from 3 x 3, they also use 1 x 1 and 5 x 5 convolutions). Their network topology is, however, more complex than ours, and the spatial reso- lution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. our model is outperforming that of |Szegedy et al] in terms of the single-network classification accuracy. 3. CLASSIFICATION FRAMEWORK In the previous section we presented the details of our network configurations. In this section, we describe the details of classification ConvNet training and evaluation. 3.1 TRAINING The ConvNet training procedure generally follows (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et all (1929) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the Lz penalty multiplier set to 5-10~4) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to 10~?, and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al||2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers. The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table[I), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully- connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and 10~? variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of|Glorot & Bengio (2010). To obtain the fixed-size 224 x 224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al||2012). Training image rescaling is explained below. Training image size. Let S be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to S' as the training scale). While the crop size is fixed to 224 x 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for S' > 224 the crop will correspond to a small part of the image, containing a small object or an object part. We consider two approaches for setting the training scale S. The first is to fix S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi- scale image statistics). In our experiments, we evaluated models trained at two fixed scales:_S = 256 (which has been widely used in the prior art Somnnetetail Dora) and S = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of 107°. The second approach to setting S' is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [Smin, Smax] (we used Sirin = 256 and Smax = 512). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single Published as a conference paper at ICLR 2015 model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same configuration, pre-trained with fixed S = 384. 3.2 TESTING At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect.4] using several values of Q for each S' leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sma TO Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 x 7 conv. layer, the last two FC layers to 1 x 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image. Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time 2012), which is less efficient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szewedy etal] (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 x 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by oeeed otal (2014). 3.3. IMPLEMENTATION DETAILS Our implementation is derived from the publicly available C++ Caffe toolbox ( (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU. While more sophisticated methods of speeding up ConvNet training have been recently pro- posed (Krizhevsky, (2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2-3 weeks depending on the architecture. 4 CLASSIFICATION EXPERIMENTS Dataset. In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012-2014 chal- lenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The clas- sification performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the Published as a conference paper at ICLR 2015 main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories. For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al. 2014). 4.1 SINGLE SCALE EVALUATION We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in Sect.[2.2] The test image size was set as follows: Q = S for fixed S,and Q =0.5(Smin + Smax) for jittered S € [Simin, Smax]. The results of are shown in TableB] First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B—E). Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 x 1 conv. layers), performs worse than the configuration D, which uses 3 x 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 x 5 conv. layers, which was derived from B by replacing each pair of 3 x 3 conv. layers with a single 5 x 5 conv. layer (which has the same receptive field as explained in Sect.[2.3). The top-! error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters. Finally, scale jittering at training time (S € [256;512]) leads to significantly better results than training on images with fixed smallest side (S = 256 or S = 384), even though a single scale is used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics. ConvNet config. (TableLI) [smallest image side | top-I val. error (%) | top-5 val. error (%) train(S) | test (Q) A A-LRN B Cc [2563512] 256 D 384 E 4.2 MULTI-SCALE EVALUATION Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: Q = {S — 32,5,5 + 32}. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable S € [Sinin3 Smaz] was evaluated over a larger range of sizes Q = {Smin,0.5(Simin + Smazx); Smaz }- Published as a conference paper at ICLR 2015 The results, presented in Table[4] indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table B). As before, the deepest configurations (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table[4). On the test set, the configuration E achieves 7.3% top-5 error. ConvNet config. (Table[I) smallest image si top-1 val. error (%) | top-5 val. error (%) train (S) B 256 256 256,288 : 5 Cc 384 352,384,416 27.8 9.2 [256; 512] | 256,384,512 256 224,256,288 384,416 256; 512] | 256,384,512 ; 256 224,256,288 26.9 &7 RA 84.4 i 3 0 256; 512] | 256,384,512 4.3. MULTI-CROP EVALUATION In Table [5] we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2] for de- tails). We also assess the complementarity of the two evaluation techniques by averaging their soft- max outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions. Table 5: ConvNet evaluation techniques comparison. In all experiments the training scale S was sampled from [256; 512], and three test scales Q were considered: {256, 384,512}. ConvNet config. (Tablel) [ Evaluation method | top-I val. error (%) | top-5 val. error (%) lense 24.8 75 D ti-crop multi-crop & 4.4 CONVNET FUSION Up until now, we evaluated the performance of individual ConvNet models. In this part of the exper- iments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et all, and 2013 (Zeiler & Fergus, 2013: [Sermanet et al} 2014). The results are shown in Table [6] By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table[5). 4.5 COMPARISON WITH THE STATE OF THE ART Finally, we compare our results with the state of the art in Table [7] In the classification task of ILSVRC-2014 challenge (Russakovsky et al (2014), our “VGG” team secured the 2nd place with Published as a conference paper at ICLR 2015 Combined ConvNet models top-I val] top-5 val ILSVRC submission (D/256/224,256,288), (D/384/352,384,416), (D/[256;512]/256,384,512) (C/256/224,256,288), (C/384/352,384,416) (E/256/224,256,288), (E/384/352,384,416) 15 73 (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), dense eval. D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop & dense eval. 7A 7.0 7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models. As can be seen from Table[J] our very deep ConvNets significantly outperform the previous gener- ation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competi- tions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models — significantly less than used in most ILSVRC submissions. In terms of the single- -net performance, our architecture achieves the best result (7.0% test error), outperforming a single Goog et by 0.9%. Notably, we did not depart from the classical ConvNet architecture of [LeCun et al] a Goed , but improved it by substantially increasing the depth. Table 7: Comparison with the state of the art in ILSVRC classification. Our method is denoted as “VGG”. Only the results obtained without outside training data are reported. Method top-1 val.\"),\n",
       " Document(metadata={}, page_content='error (%)|top-5 val. error (%)|top-5 test error (%) VGG (2 nets, multi-crop & dense eval.) 23.7 6.8 6.8 VGG (I net, multi-crop & dense eval.) 24.4 7A 7.0 VGG (ILSVRC submission, 7 nets, dense eval.) 24.7 75 7.3 GoogLeNet (Szegedy et al., 2014) (I net) - 79 MSRA (He et al. sor tt nets) MSRA (He et al., 2014) (1 net) Clarifai Russakousiy et a 2014) (multiple nets) Zeiler & Forus Zeiler & Feraus st a sat OverFeat (Sermanet et al., 2014) (7 nets) OverFeat (Sermanet ct al., 2014) (1 net) Krizhevsky et al. Rrchesky et al., 2012) (1 net) 5 CONCLUSION In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large- scale image classification. It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al.,{1989%|Krizhevsky et al.,|2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again confirm the importance of depth in visual representations. This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. Published as a conference paper at ICLR 2015 Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context database. CoRR, abs/1412.0623, 2014. Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. In Proc.'),\n",
       " Document(metadata={}, page_content='BMVC., 2014. Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation. CoRR, abs/1411.6836, 2014. Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance convolutional neural networks for image classification. In LJCAI, pp. 1237-1242, 2011. Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In N/PS, pp. 1232-1240, 2012. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proc. CVPR, 2009. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013. Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual object classes challenge: A retrospective. LJCV, 111(1):98-136, 2015. Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In EEE CVPR Workshop of Generative Model Based Vision, 2004. Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014. Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604, 2014. Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. AISTATS, volume 9, pp. 249-256, 2010. Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street view imagery using deep convolutional neural networks. In Proc.'),\n",
       " Document(metadata={}, page_content='ICLR, 2014. Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR, abs/1406.4729v2, 2014. Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014. Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc. ICLR, 2014. Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR, abs/1412.2306, 2014. Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539, 2014. Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014. Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural net- works. In N/PS, pp. 1106-1114, 2012. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropa- gation applied to handwritten zip code recognition. Neural Computation, 1(4):541-551, 1989. Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. JCLR, 2014. Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038, 2014. Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks. In Proc. CVPR, 2014. Perronnin, F., Sanchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In Proc. ECCV, 2010. Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline for Recognition. CoRR, abs/1403.6382, 2014. Published as a conference paper at ICLR 2015 Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014. Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR, abs/1406.2199, 2014. Published in Proc. NIPS, 2014. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR, abs/1406.5726, 2014. Zeiler, M. D.'),\n",
       " Document(metadata={}, page_content=\"and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. Published in Proc. ECCV, 2014. In the main body of the paper we have considered the classification task of the ILSVRC challenge, and performed a thorough evaluation of ConvNet architectures of different depth. In this section, we turn to the localisation task of the challenge, which we have won in 2014 with 25.3% error. It can be seen as a special case of object detection, where a single object bounding box should be predicted for each of the top-5 classes, irrespective of the actual number of objects of the class. For this we adopt the approach of|Sermanet et al! (2014), the winners of the ILSVRC-2013 localisation challenge, with a few modifications. Our method is described in Sect.[A_Iand evaluated in Sect.[A.2] To perform object localisation, we use a very deep ConvNet, where the last fully connected layer predicts the bounding box location instead of the class scores. A bounding box is represented by a 4-D vector storing its center coordinates, width, and height. There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR [2014)) or is class-specific (per-class regression, PCR). In the former case, the last layer is 4-D, while in the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table[I), which contains 16 weight layers and was found to be the best-performing in the classification task (Sect.[4). Training. Training of localisation ConvNets is similar to that of the classification ConvNets (Sect.B.I). The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth. We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 submission). Training was initialised with the corresponding classification models (trained on the same scales), and the initial learning rate was set to 10-3. We explored both fine-tuning all layers and fine-tuning only the first two fully-connected layers, as done in (Sermanet et all 2014), The last fully-connected layer was initialised randomly and trained from scratch. Testing. We consider two testing protocols. The first is used for comparing different network modifications on the validation set, and considers only the bounding box prediction for the ground truth class (to factor out the classification errors). The bounding box is obtained by applying the network only to the central crop of the image. The second, fully-fledged, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classification task (Sect. [3.2). The difference is that instead of the class score map, the output of the last fully-connected layer is a set of bounding box predictions. To come up with the final prediction, we utilise the greedy merging procedure of (2014), which first merges spatially close predictions (by averaging their coor- dinates), and then rates them based on the class scores, obtained from the classification ConvNet. When several localisation ConvNets are used, we first take the union of their sets of bounding box predictions, and then run the merging procedure on the union. We did not use the multiple pooling 10 Published as a conference paper at ICLR 2015 offsets technique of|Sermanet et al] (2014), which increases the spatial resolution of the bounding box predictions and can further improve the results. A.2 LOCALISATION EXPERIMENTS In this section we first determine the best-performing localisation setting (using the first test proto- col), and then evaluate it in a fully-fledged scenario (the second protocol). The localisation error is measured according to the ILSVRC criterion iRuasakoweky ol DOTS, ie. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5. Settings comparison. As can be seen from Table[8] per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of Bemmanet et all (2014), where PCR was outperformed by SCR. We also note that fine-tuning all layers for the lo- calisation task leads to noticeably better results than fine-tuning only the fully-connected layers (as done in (Sermanet et al.,|2014)). In these experiments, the smallest images side was set to S = 384; the results with S' = 256 exhibit the same behaviour and are not shown for brevity. Table 8: Localisation error for different modifications with the simplified testing protocol: the bounding box is predicted from a single central image crop, and the ground-truth class is used. All ConvNet layers (except for the last one) have the configuration D (Table [I), while the last layer performs either single-class regression (SCR) or per-class regression (PCR). Fine-tuned layers]regression type]GT class localisation error 1st and 2nd FC Fully-fledged evaluation. Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted us- ing our best-performing classification system (Sect. 45) ), and multiple densely-computed bounding box predictions are merged using the method of [Sermanet et al] (014). As can be seen from Ta- ble[9] application of the localisation ConvNet to the whole image substantially improves the results compared to using a center crop (Table[8), despite using the top-5 predicted class labels instead of the ground truth. Similarly to the classification task (Sect.[4), testing at several scales and combining the predictions of multiple networks further improves the performance. Table 9: Localisation error smallest image side top-5 localisation error (%) test (Q) val. test. 256 29.5 26.7 384 352,384 [ fusion: 256/256 and 384/352,384 | 26.9 25.3 Comparison with the state of the art. We compare our best localisation result with the state of the art in Table ma With 25.3% test error, our “VGG” team won the localisation challenge of ILSVRC-2014 (Russakovsky et al |2014). Notably, our results are considerably better than those of the ILSVRC-2013 winner Overfeat (Sermanet et al] (2014), even though we used less scales and did not employ their resolution enhancement technique. We envisage that better localisation per- formance can be achieved if this technique is incorporated into our method. This indicates the performance advancement brought by our very deep ConvNets — we got better results with a simpler localisation method, but a more powerful representation. In the previous sections we have discussed training and evaluation of very deep ConvNets on the ILSVRC dataset. In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature 11 Published as a conference paper at ICLR 2015 Table 10: Comparison with the state of the art in ILSVRC localisation. Our method is denoted Krizhevsky et al. (Krizhevsky et al., 2012 extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting. Recently, there has been a lot of interest in such a use case Donahue et alJ, 2013} [Razavian et al) [20144 Chatfield et al 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin. Following that line of work, we investigate if our models lead to better performance than more shallow models utilised in the state-of-the-art methods. In this evaluation, we consider two models with the best classification performance on ILSVRC (Sect.[4) — configurations “Net-D” and “Net-E” (which we made publicly available). To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales. The resulting image descriptor is L2-normalised and combined with a linear SVM classifier, trained on the target dataset. For simplicity, pre-trained ConvNet weights are kept fixed (no fine-tuning is performed). Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure (Sect. [3.2). Namely, an image is first rescaled so that its smallest side equals Q, and then the net- work is densely applied over the image plane (which is possible when all weight layers are treated as convolutional). We then perform global average pooling on the resulting feature map, which produces a 4096-D image descriptor. The descriptor is then averaged with the descriptor of a hori- zontally flipped image. As was shown in Sect. [4.2] evaluation over multiple scales is beneficial, so we extract features over several scales Q. The resulting multi-scale features can be either stacked or pooled across scales. Stacking allows a subsequent classifier to learn how to optimally combine image statistics over a range of scales; this, however, comes at the cost of the increased descriptor dimensionality. We return to the discussion of this design choice in the experiments below. We also assess late fusion of features, computed using two networks, which is performed by stacking their respective image descriptors. Table 11: Comparison with the state of the art in image classification on VOC-2007, VOC-2012, Caltech-101, and Caltech-256. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (2000 classes). Method VOC-2007 Caltech-101 Caltech-256 (mean AP) | (mean AP) |(mean class recall) | (mean class recall) Zeiler & Fergus (Zeiler & Fergus, 2013) - . 86.5+£0.5 74.2 £0.3 Chatfield et al. (Chatfield et al., 2014) 82.4 . 88.4 + 0.6 776+0.1 He et al. (He et al., 2014) 82.4 93.4 + 0.5 - Wei et al. (Wei et al., 2014) 81.5 (85.2*) - - VGG Net-D (16 layers) ; . 918+ 71.0 85.0 £ 0.2 VGG Net-E (19 layers) . . 92.3 + 0.5 85.1 40.3 VGG Net-D & Net-E . . 92.7 + 0.5 86.2 + 0.3 Image Classification on VOC-2007 and VOC-2012. We begin with the evaluation on the imag classification task of PASCAL VOC-2007 and VOC-2012 benchmarks Bois), These datasets contain 10K and 22.5K images respectively, and each image is annotated with one or several labels, corresponding to 20 object categories. The VOC organisers provide a pre-defined split into training, validation, and test data (the test data for VOC-2012 is not publicly available; instead, an official evaluation server is provided). Recognition performance is measured using mean average precision (mAP) across classes. Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs sim- 12 Published as a conference paper at ICLR 2015 ilarly to the aggregation by stacking. We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-specific seman- tics which a classifier could exploit. Since averaging has a benefit of not inflating the descrip- tor dimensionality, we were able to aggregated image descriptors over a wide range of scales: Q © {256, 384,512,640, 768}. It is worth noting though that the improvement over a smaller range of {256, 384, 512} was rather marginal (0.3%). The test set performance is reported and compared with other approaches in Table[I1] Our networks “Net-D” and “Net-E” exhibit identical performance on VOC datasets, and their combination slightly improves the results. Our methods set the new state of the art across image representations, pre- trained on the ILSVRC dataset, outperforming the previous best result oftChatfield et all (2014) by more than 6%. It should be noted that the method of (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, semantically close to those in VOC datasets. It also benefits from the fusion with an object detection-assisted classification pipeline. Image Classification on Caltech-101 and Caltech-256. In this section we evaluate very deep fea- tures on Caltech-101 and Caltech-256 image classification benchmarks. Caltech-101 contains 9K images labelled into 102 classes (101 object categories and a background class), while Caltech-256 is larger with 31K images and 257 classes. A standard eval- uation protocol on these datasets is to generate several random splits into training and test data and report the average recognition performance across the splits, which is measured by the mean class recall (which compensates for a different number of test images per class). Following|Chatfield et al (2014); (2013); [He et al] (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training images per class (and the rest is used for testing). In each split, 20% of training images were used as a validation set for hyper-parameter selection. We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multi- ple scales, performs better than averaging or max-pooling. This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are se- mantically different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such scale-specific representations. We used three scales Q € {256, 384, 512}. Our models are compared to each other and the state of the art in Table[11] As can be seen, the deeper 19-layer Net-E performs better than the 16-layer Net-D, and their combination further improves the performance. On Caltech-101, our representations are competitive with the approach of (014, which, however, performs significantly worse than our nets on VOC-2007. On Caltech-256, our features outperform the state of the art (Chatfield et al] by a large margin (8.6%). Action Classification on VOC-2012. We also evaluated our best-performing image representa- tion (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task Eecinshae call BOTs) which consists in predicting an action class from a single image, given a bounding box of the person performing the action. The dataset contains 4.6K training im- ages, labelled into 11 classes. Similarly to the VOC-2012 object classification task, the performance is measured using the mAP. We considered two training settings: (i) computing the ConvNet fea- tures on the whole image and ignoring the provided bounding box; (ii) computing the features on the whole image and on the provided bounding box, and stacking them to obtain the final representation. The results are compared to other approaches in Table[I2] Our representation achieves the state of art on the VOC action classification task even without using the provided bounding boxes, and the results are further improved when using both images and bounding boxes. Unlike other approaches, we did not incorporate any task-specific heuristics, but relied on the representation power of very deep convolutional features. Other Recognition Tasks. Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, consistently outperform- ing more shallow representations. For instance, (2014) achieve the state of the object detection results by replacing the ConvNet of |Krizhevsky et al| (2012) with our 16-layer 2012) have been ob- 13 Published as a conference paper at ICLR 2015 Table 12: Comparison with the state of the art in single-image action classification on VOC- 2012. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (1512 classes). Hoai, 2014 VGG Net-D & Net-E, image and bounding box 84.0 | served in semantic segmentation (Long et al, [2014), image caption generation (Kiros etal} 2014; Karpathy & Fei-Feil|2014), texture and material recognition 2014: |Bell et al.,|2014). Here we present the list of major paper revisions, outlining the substantial changes for the conve- nience of the reader.\"),\n",
       " Document(metadata={}, page_content='v1 Initial version. Presents the experiments carried out before the ILSVRC submission. v2 Adds post-submission ILSVRC experiments with training set augmentation using scale jittering, which improves the performance. v3 Adds generalisation experiments (Appendix[B) on PASCAL VOC and Caltech image classifica- tion datasets. The models used for these experiments are publicly available. v4 The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple crops for classification. v6 Camera-ready ICLR-2015 conference paper. Adds a comparison of the net B with a shallow net and the results on PASCAL VOC action classification benchmark. 14')]"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "0a6f0b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28473\n",
      "1778\n",
      "1925\n",
      "2806\n",
      "16754\n",
      "664\n"
     ]
    }
   ],
   "source": [
    "for text in text_docs:\n",
    "    print(len(text.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "d22db01c-af3c-4427-82f6-55707392d873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''CASE-1: All Chunks(RAW TEXT ONLY) will be given as a context to LLM and asked to summarize'''\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        (\"system\", \"You are an expert summarizer. Read the given context and generate a concise, clear, and accurate summary:\\n\\n{context}\")\n",
    ")\n",
    "\n",
    "\n",
    "'''gpt'''\n",
    "\n",
    "llm1 = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "chain1 = create_stuff_documents_chain(llm1, prompt)\n",
    "gpt_chain_result = chain1.invoke({\"context\":text_docs})\n",
    "\n",
    "\n",
    "'''Mistral AI '''\n",
    "\n",
    "llm2 = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "chain2 = create_stuff_documents_chain(llm2, prompt)\n",
    "MistralAI_chain_result = chain2.invoke({\"context\":text_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a0062b19-f41b-4dce-b797-90ff23340318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge1_score(reference_text, generated_text):\n",
    "    \"\"\"\n",
    "    Calculate and return the ROUGE-1 F1 score.\n",
    "\n",
    "    Args:\n",
    "        reference_text (str): The reference text (e.g., human-written summary).\n",
    "        generated_text (str): The generated text (e.g., model summary).\n",
    "\n",
    "    Returns:\n",
    "        float: The ROUGE-1 F1 score.\n",
    "    \"\"\"\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    scores = scorer.score(reference_text, generated_text)\n",
    "    \n",
    "    # Extract and return the ROUGE-1 F1 score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "fa15059b-44c1-4c97-9b82-432a082ebd00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model  Metric  Precision  Recall  F-Measure\n",
      "0  GPT-4o-mini  rouge1     0.5143  0.5217     0.5180\n",
      "1  GPT-4o-mini  rouge2     0.1942  0.1971     0.1957\n",
      "2  GPT-4o-mini  rougeL     0.3714  0.3768     0.3741\n",
      "3   Mistral AI  rouge1     0.4565  0.6087     0.5217\n",
      "4   Mistral AI  rouge2     0.1967  0.2628     0.2250\n",
      "5   Mistral AI  rougeL     0.2826  0.3768     0.3230\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Score calculation\n",
    "score_gpt_mini = calculate_rouge1_score(abstract, gpt_chain_result)\n",
    "score_mistral = calculate_rouge1_score(abstract, MistralAI_chain_result)\n",
    "\n",
    "# Function to convert scores into a structured format\n",
    "def extract_scores(model_name, scores):\n",
    "    results = []\n",
    "    for metric, score in scores.items():\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Metric\": metric,\n",
    "            \"Precision\": round(score.precision, 4),\n",
    "            \"Recall\": round(score.recall, 4),\n",
    "            \"F-Measure\": round(score.fmeasure, 4),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Extract scores for both models\n",
    "gpt_scores = extract_scores(\"GPT-4o-mini\", score_gpt_mini)\n",
    "mistral_scores = extract_scores(\"Mistral AI\", score_mistral)\n",
    "\n",
    "# Combine results into a DataFrame\n",
    "scores_df= pd.DataFrame(gpt_scores+ mistral_scores)\n",
    "\n",
    "scores_df_text_only  = scores_df[[\"Model\", \"Metric\", \"Precision\", \"Recall\", \"F-Measure\"]]\n",
    "\n",
    "# Print the DataFrame\n",
    "print(scores_df_text_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "c85e6d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Article    Model BLEU Score\n",
      "0  Article_1                    \n",
      "1                 GPT     0.6218\n",
      "2             Mistral     0.5256\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "bleu_score_gpt1 = sentence_bleu([abstract], gpt_chain_result)\n",
    "bleu_score_mistal1 = sentence_bleu([abstract], MistralAI_chain_result)\n",
    "\n",
    "bleu_score_data = {\n",
    "    \"Article\": [\"Article_1\", \"\", \"\"],\n",
    "    \"Model\": [\"\", \"GPT\", \"Mistral\"],\n",
    "    \"BLEU Score\": [\"\", round(bleu_score_gpt1, 4), round(bleu_score_mistal1, 4)]\n",
    "}\n",
    "\n",
    "bleu_scores_df = pd.DataFrame(bleu_score_data)\n",
    "\n",
    "print(bleu_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "0fc29efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884\n",
      "1019\n",
      "1297\n"
     ]
    }
   ],
   "source": [
    "print(len(abstract))\n",
    "\n",
    "print(len(gpt_chain_result))\n",
    "\n",
    "print(len(MistralAI_chain_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df38af3-6e8b-4508-9fc6-3699a167e893",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "CASE:2 Adding Images such as flowchart, related_images to the context to test will there any difference in summarization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "65e96440-8595-4279-b176-fbe4abc52f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing requiered libraries to obtain gpt via langchain\n",
    "# langchain-core defines the base abstractions for the LangChain ecosystem. \n",
    "# The interfaces for core components like chat models, LLMs, vector stores, retrievers, and more are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9528cf39-910f-4862-abb2-2f414e818bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am passing direct images to GPT4V to get summary of it.\n",
    "# for ease i am storing all my images in my  local dirve and text in word document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "6d3963d2-5b41-4a2a-b32d-10d2a6eff51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import base64\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8c4b1cbc-672e-4fe0-af7c-a807f59f621f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below code is copied from langchain cookbook'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Below code is copied from langchain cookbook'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "d1824607-71c0-4de4-8cc0-3b6dc8ef3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text=\"\"\"Analyze the image provided. \n",
    "Summarize key insights, highlighting any significant patterns, trends, or relationships between the data in the table and visual content in the image. \n",
    "Focus on critical details that reveal important information, and provide a clear, concise summary \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "0e76a0e4-8c97-401d-a652-ce89defbea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "c9694be8-5828-4c7c-af43-2a8cfca362dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1024)\n",
    "    \n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "718735ed-b884-45ca-bf4e-502b5de227be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images and tables.\n",
    "    Give a concise summary of the image or table which is readlale. \"\"\"\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image) # for reference we are storing the base64_iamge\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "f1e149b9-a83f-49a1-8d71-9ef06df16137",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath= r\"C:\\Text_Summarization\\Article 4\"\n",
    "img_base64_list, image_summaries = generate_img_summaries(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "d29664af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The image consists of two graphs comparing the training and test errors of two neural network architectures with different depths: a 56-layer model and a 20-layer model.\\n\\n### Left Graph (Training Error):\\n- **Y-axis**: Training error (%) \\n- **X-axis**: Iterations (in thousands)\\n- The 56-layer model (red) starts with a higher error but decreases more rapidly, approaching lower error values.\\n- The 20-layer model (yellow) has a consistently lower error but does not decrease as significantly as the 56-layer model.\\n\\n### Right Graph (Test Error):\\n- **Y-axis**: Test error (%) \\n- **X-axis**: Iterations (in thousands)\\n- Similar trends: the 56-layer model shows a higher initial test error that decreases over time, while the 20-layer model maintains a lower but less decreasing trend.\\n\\nOverall, the 56-layer model shows better performance in both training and test error reduction compared to the 20-layer model over the iterations.',\n",
       " 'The image illustrates a neural network architecture featuring a residual block. It starts with an input \\\\( x \\\\), which passes through two weight layers followed by a ReLU activation function. The output of this sequence is added to the original input \\\\( x \\\\), and the sum is then processed by another ReLU activation. This architecture allows for the learning of residual functions, facilitating better training of deep networks.',\n",
       " 'The image shows two line graphs comparing error rates (%) over iterations for different neural network architectures. \\n\\n**Left Graph:**\\n- Compares \"plain\" networks with 18 and 34 layers.\\n- The error for the 34-layer model starts higher but decreases more effectively over iterations compared to the 18-layer model, which shows a less steep decline.\\n\\n**Right Graph:**\\n- Compares ResNet architectures with 18 and 34 layers.\\n- Similar trends are observed; the ResNet-34 model achieves lower error rates compared to ResNet-18, indicating better performance as training progresses.\\n\\nOverall, deeper networks (34 layers) tend to perform better than shallower ones (18 layers) in both plain and ResNet architectures.',\n",
       " 'The image presents two neural network architectures, showcasing different configurations for processing data. \\n\\n1. **64-dimensional (64-d) Architecture**:\\n   - Consists of two sequential layers of 3x3 convolutions, each with 64 filters.\\n   - A ReLU activation function is applied after each convolution.\\n   - Outputs are combined using an addition operation before passing through another ReLU.\\n\\n2. **256-dimensional (256-d) Architecture**:\\n   - Features a 1x1 convolution with 64 filters, followed by a 3x3 convolution (64 filters), and another 1x1 convolution with 256 filters.\\n   - Similar to the first architecture, ReLU activations are applied, and outputs are summed before the final activation.\\n\\nBoth architectures highlight the use of convolutional layers and residual connections.',\n",
       " 'The image consists of three plots showing the error rates (%) over time for various neural network architectures, specifically different configurations of ResNet layers:\\n\\n1. **Left Plot**: Displays the error rates for plain networks with 20, 32, 44, and 56 layers. The 56-layer network (in red) shows a higher error rate initially but improves significantly over time, while the 20-layer network (in blue) achieves a lower error rate earlier on.\\n\\n2. **Middle Plot**: Illustrates the error rates for ResNet architectures with 20, 32, 44, 56, and 110 layers. The 56-layer ResNet (in red) performs well, with a noticeable improvement in error reduction over time. The 110-layer ResNet (in black) maintains a lower error rate compared to the others.\\n\\n3. **Right Plot**: Compares two residual networks: 110 layers (black) and 1202 layers (magenta). Both show low error rates, with the 1202-layer network demonstrating slightly better performance, especially over extended training.\\n\\nOverall, the plots suggest that deeper networks generally achieve lower error rates, albeit with varying initial performances.',\n",
       " \"The image presents two graphs comparing the standard deviation (std) of different neural network architectures across their layer indices. \\n\\n1. **Top Graph**: Displays std values against original layer indices. The lines represent:\\n   - Plain-20 (dashed yellow)\\n   - Plain-56 (dashed red)\\n   - ResNet-20 (solid red)\\n   - ResNet-56 (dashed green)\\n   - ResNet-110 (solid black)\\n   The std values vary significantly, with ResNet-110 showing a relatively stable trend.\\n\\n2. **Bottom Graph**: Shows std values sorted by magnitude across the same layer indices. The trends are more consistent, with ResNet-110 maintaining lower std values compared to the others, indicating better performance or stability across layers.\\n\\nOverall, the graphs illustrate the differences in layer-wise standard deviation for various neural network designs, highlighting ResNet-110's superior stability.\",\n",
       " 'The table presents mean Average Precision (mAP) results for different configurations of the Faster R-CNN model on the COCO dataset. \\n\\nKey points include:\\n\\n- **Training Data**: COCO train and COCO trainval.\\n- **Test Data**: COCO val and COCO test-dev.\\n- **mAP Scores**:\\n  - Baseline models show mAP scores of 41.5 (VGG-16) and 48.4 (ResNet-101) at IoU threshold 0.5.\\n  - Improvements with additional techniques:\\n    - Box refinement: 49.9\\n    - Context: 51.1\\n    - Multi-scale testing: 53.8\\n- **Ensemble Model**: Achieves the highest scores of 59.0 for COCO trainval and 37.4 for COCO test-dev.\\n\\nOverall, the addition of techniques consistently improves performance across both validation and test datasets.',\n",
       " \"The table presents performance metrics for various models evaluated in the ILSVRC challenges. It includes:\\n\\n- **GoogLeNet (ILSVRC'14)**: No validation score provided, with a test score of 43.9.\\n- **Our single model (ILSVRC'15)**: Validation score of 60.5 and a test score of 58.8.\\n- **Our ensemble (ILSVRC'15)**: Validation score of 63.6 and a test score of 62.1.\\n\\nOverall, the ensemble model shows the highest performance in both validation and testing compared to the single model and GoogLeNet.\",\n",
       " 'The table summarizes the performance of various localization (LOC) methods and networks in terms of their classification error rates. Key points include:\\n\\n- **LOC Methods**: VGG, RPN (Region Proposal Network).\\n- **Networks Used**: VGG-16 and ResNet-101.\\n- **Testing Types**: \"1-crop\" and \"dense.\"\\n- **LOC Error on GT CLS**: \\n  - VGG-16: 33.1%\\n  - ResNet-101 (1-crop): 13.3%\\n  - ResNet-101 (dense): 11.7%\\n- **Classification Network**: Primarily ResNet-101 and an ensemble approach.\\n- **Top-5 LOC Error on Predicted CLS**:\\n  - ResNet-101: 14.4%\\n  - RPN+ResNet-101: 10.6%\\n  - RPN+RCNN ensemble: 8.9%\\n\\nOverall, the results indicate that using ensemble methods and dense testing improves classification performance.',\n",
       " 'The table presents the top-5 localization error rates (in percentage) for various methods evaluated on the ILSVRC dataset across validation and test sets. \\n\\n- **OverFeat (ILSVRC\\'13)**: 30.0 (val), 29.9 (test)\\n- **GoogLeNet (ILSVRC\\'14)**: Not applicable (val), 26.7 (test)\\n- **VGG (ILSVRC\\'14)**: 26.9 (val), 25.3 (test)\\n- **Ours (ILSVRC\\'15)**: 8.9 (val), 9.0 (test)\\n\\nThe method \"Ours\" shows the lowest localization error, significantly outperforming the other methods listed.',\n",
       " 'The image presents a flowchart illustrating two convolutional neural network architectures for image processing. Each architecture consists of layers that perform convolution operations followed by pooling layers, which downsample the feature maps. \\n\\n1. **Left Architecture**:\\n   - Starts with a 224x224 input image.\\n   - Contains multiple convolution layers with varying filter sizes (3x3 and 7x7) and increasing filter counts (64, 128, 256, 512).\\n   - Progressive downsampling through pooling layers reduces the output size from 224 to 7.\\n   - Ends with a fully connected layer (fc 4096) and an average pooling layer.\\n\\n2. **Right Architecture**:\\n   - Also processes a 224x224 input image.\\n   - Features similar convolution layers but with a 7x7 convolution at the beginning and a different arrangement of layer sizes.\\n   - The output size is reduced to 1 at the final layer.\\n\\nBoth architectures are designed for deep learning tasks, likely focused on image classification or recognition.',\n",
       " 'The table presents a summary of convolutional layers in a neural network architecture across various depths (18-layer, 34-layer, 50-layer, 101-layer, and 152-layer). \\n\\n- **Layer Names**: The layers are named conv1, conv2_x, conv3_x, conv4_x, and conv5_x.\\n- **Output Size**: The output sizes decrease progressively from 112x112 in conv1 to 1x1 in conv5_x.\\n- **Layer Configuration**: Each layer varies in the number of filter sizes and quantities:\\n  - **conv2_x**: 56x56 output, with multiple configurations of 3x3 filters.\\n  - **conv3_x**: 28x28 output, also with 3x3 filters, but with higher filter counts.\\n  - **conv4_x**: 14x14 output, using 3x3 filters with increased complexity.\\n  - **conv5_x**: 7x7 output, with the most filters and configurations.\\n- **Final Layer**: The last layer is an average pooling followed by a fully connected (fc) layer with 1000 outputs and a softmax activation.\\n- **FLOPs**: The table also lists the number of floating point operations (FLOPs) for each architecture variation, ranging from 1.8 x 10^9 for the 18-layer model to 11.3 x 10^9 for the 152-layer model. \\n\\nThis structure indicates the increasing complexity and computational demand of deeper network architectures.',\n",
       " 'The table compares the performance of two neural network architectures (plain and ResNet) across two different layer configurations (18 layers and 34 layers). \\n\\n- For 18 layers: \\n  - Plain model achieves a score of 27.94.\\n  - ResNet model achieves a score of 27.88.\\n\\n- For 34 layers:\\n  - Plain model achieves a higher score of 28.54.\\n  - ResNet model has a significantly lower score of 25.03.\\n\\nOverall, the plain model consistently outperforms the ResNet model in both configurations.',\n",
       " 'The table compares the performance of various models based on their top-1 and top-5 error rates. \\n\\n- **VGG-16** has a top-1 error rate of 28.07 and a top-5 error rate of 9.33.\\n- **GoogLeNet** shows a top-1 error rate as not applicable (-) and a top-5 error rate of 9.15.\\n- **PReLU-net** achieves a top-1 error of 24.27 and a top-5 error of 7.38.\\n\\nFor the other models:\\n- **plain-34** has a top-1 error of 28.54 and a top-5 error of 10.02.\\n- **ResNet-34 A**: 25.03 (top-1), 7.76 (top-5)\\n- **ResNet-34 B**: 24.52 (top-1), 7.46 (top-5)\\n- **ResNet-34 C**: 24.19 (top-1), 7.40 (top-5)\\n- **ResNet-50**: 22.85 (top-1), 6.71 (top-5)\\n- **ResNet-101**: 21.75 (top-1), 6.05 (top-5)\\n- **ResNet-152**: 21.43 (top-1), 5.71 (top-5)\\n\\nOverall, ResNet-152 shows the best performance with the lowest top-1 and top-5 error rates.',\n",
       " \"The table summarizes the performance of various neural network architectures in terms of their top-1 and top-5 error rates on the ILSVRC'14 dataset. \\n\\nKey points include:\\n\\n- VGG and GoogLeNet do not have top-1 error rates listed, but GoogLeNet has a top-5 error of 7.89.\\n- The VGG model (v5) has a top-1 error of 24.4 and a top-5 error of 7.1.\\n- Among other models, ResNet-152 shows the best performance with a top-1 error of 19.38 and a top-5 error of 4.49, followed closely by ResNet-101 with top-1 error of 19.87 and top-5 error of 4.60. \\n- Overall, newer architectures like ResNet models outperform the earlier VGG and GoogLeNet models.\",\n",
       " \"The table summarizes the top-5 error rates for various image classification methods tested in the ILSVRC (ImageNet Large Scale Visual Recognition Challenge). \\n\\n- **ResNet (ILSVC'15)** shows the lowest error rate at **3.57%**.\\n- The next best is **BN-inception** at **4.82%**, followed closely by **PReLU-net** at **4.94%**.\\n- **VGG (v5)** has an error rate of **6.8%**, while **GoogLeNet** stands at **6.66%**.\\n- The original **VGG** method has the highest error rate in this list at **7.32%**.\",\n",
       " 'The table presents information about three different output map sizes (32x32, 16x16, and 8x8) in relation to the number of layers and filters in a neural network:\\n\\n- For the **32x32** map size:\\n  - **# layers**: \\\\(1 + 2n\\\\)\\n  - **# filters**: 16\\n\\n- For the **16x16** map size:\\n  - **# layers**: \\\\(2n\\\\)\\n  - **# filters**: 32\\n\\n- For the **8x8** map size:\\n  - **# layers**: \\\\(2n\\\\)\\n  - **# filters**: 64\\n\\nThis indicates a relationship between the output size, the number of layers, and the number of filters, with larger maps generally having fewer filters and varying layer counts based on a variable \\\\(n\\\\).',\n",
       " 'The table presents various neural network methods along with their corresponding error rates, number of layers, and parameters. \\n\\n- **Maxout**, **NIN**, and **DSN** are methods with error rates of 9.38%, 8.81%, and 8.22%, respectively.\\n- **FitNet** (19 layers, 2.5M params) has an error of 8.39%.\\n- **Highway Networks**: One with 19 layers and 2.3M params shows 7.54% error; another with 32 layers and 1.25M params has 8.80% error.\\n- **ResNet** variations exhibit decreasing error rates with increasing layers: \\n  - 20 layers: 8.75%\\n  - 32 layers: 7.51%\\n  - 44 layers: 7.17%\\n  - 56 layers: 6.97%\\n  - 110 layers: 6.43% (with a reported variance of ±0.16)\\n  - 1202 layers: 7.93%\\n\\nOverall, deeper ResNet architectures tend to achieve lower error rates.',\n",
       " 'The table compares the performance of two metrics, VGG-16 and ResNet-101, based on mean Average Precision (mAP) at different thresholds. \\n\\n- For mAP at 0.5, VGG-16 scores 41.5, while ResNet-101 scores higher at 48.4.\\n- For mAP at the range [0.5, 0.95], VGG-16 achieves 21.2, and ResNet-101 shows an improved score of 27.2. \\n\\nOverall, ResNet-101 outperforms VGG-16 in both metrics.',\n",
       " 'The table presents the performance of two models, VGG-16 and ResNet-101, on different training and test datasets. \\n\\n- For the training data \"07+12\":\\n  - On the VOC 07 test, VGG-16 achieved 73.2, while ResNet-101 scored 76.4.\\n  \\n- For the training data \"07++12\":\\n  - On the VOC 12 test, VGG-16 scored 70.4, and ResNet-101 scored 73.8.\\n\\nOverall, ResNet-101 consistently outperformed VGG-16 across both test datasets.']"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cd87619c-75c7-4233-b3b3-6a9e9ea15db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Storing raw text only in vecotre store\n",
    "# We will use GPT4ALL embeddings and chroma vectore storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "a12982c5-d640-4bb5-a315-ef8c057bc33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "3a67fdd1-da89-4c31-8f55-ff8dd3eeeff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "f6c13d4d-fdfa-49a2-9543-3d9c7bd2857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"summary_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"C:\\chroma\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "899df48f-c477-4e56-bd59-71fc8f0d22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "doc_id = [str(uuid.uuid4()) for i in text_docs ]\n",
    "\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "Text_docs_document = [\n",
    "    Document(page_content=s.page_content, metadata={\"id_key\": doc_id[i],\"type\": \"text\"})\n",
    "    for i,s in enumerate(text_docs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "cb261ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id_key': '989f1970-8e8e-408a-a968-ce5caa835e54', 'type': 'text'}, page_content=\"arXiv:1409.1556v6 [cs.CV] 10 Apr 2015 Published as a conference paper at ICLR 2015 Karen Simonyan* & Andrew Zisserman* Visual Geometry Group, Department of Engineering Science, University of Oxford {karen,az}@robots.ox.ac.uk In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 x 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa- tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili- tate further research on the use of deep visual representations in computer vision. 1 INTRODUCTION Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale _im- age and video recognition (Krizhevsky et al. |2012; |Zeiler & Fergus, 20135 Sermanet et all 2014; [Simonyan & Zisserman|,|2014) which has become possible due to the large public image reposito- ries, such as ImageNet (Deng et al |2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al.)|2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recog- nition Challenge (ILSVRC) , which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature en- codings (the winner of ILSVRC-2011) to deep ConvNets (the winner of ILSVRC-2012). With ConvNets becoming more of a commodity in the computer vision field, a number of at- tempts have been made to improve the original architecture of [Krizhevsky et al] (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC- 2013 (Zeiler & Fergus, (2013; [Sermanet et al |2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt_with training and testing the networks densely over the whole image and over multiple scales 014 [Howard D014), In this paper, we address another important aspect of ConvNet architecture design — its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 x 3) convolution filters in all layers. As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning). We have released our two best-performing model] to facilitate further research. The rest of the paper is organised as follows. In Sect. [2] we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect.[3] and the Published as a conference paper at ICLR 2015 configurations are compared on the ILSVRC classification task in Sect. [4] Sect. [5] concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix[A] and discuss the generalisation of very deep features to other datasets in Appendix] Finally, Appendix[C|contains the list of major paper revisions. 2 CONVNET CONFIGURATIONS To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by (Citesan et ail (2011): |Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect.[2.1) and then detail the specific configurations used in the evaluation (Sect.[2.2). Our design choices are then discussed and compared to the prior art in Sect. [2.3] 2.1 ARCHITECTURE During training, the input to our ConvNets is a fixed-size 224 x 224 RGB image. The only pre- processing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 x 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 x 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 x 3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 x 2 pixel window, with stride 2. A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000- way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks. All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al} |2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al 12012): as will be shown in Sect. such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory con- sumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al.||2012). 2.2 CONFIGURATIONS The ConvNet configurations, evaluated in this paper, are outlined in Table[I] one per column. In the following we will refer to the nets by their names (A—E). All configurations follow the generic design presented in Sect. [2.1] and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512. In Table[2]we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in [2014)). 2.3. DISCUSSION Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al.) |2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; [Sermanet et al 2014). Rather than using relatively large receptive fields in the first conv. lay- ers (e.g. 11 x 11 with stride 4 in (Krizhevsky et al|,|2012), or 7 x 7 with stride 2 in (Zeiler & Fergus, 2013; 2014)), we use very small 3 x 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 x 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 x 5; three Published as a conference paper at ICLR 2015 Table 1: ConvNet configurations (shown in columns). The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as “conv (receptive field size)-(number of channels)”. The ReLU activation function is not shown for brevity. 16 weight layers layers input (224 x 224 RGB image) conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 19 weight layers conv3-128 | conv3-128 | conv3-128 | conv3-128 | conv3-128 | conv3-128 conv3-128 | conv3-128 | conv3-128 | conv3-128 maxpool conv3-256 conv3-256 conv3-256 | conv3-256 conv3-256 | conv3-256 conv3-256 | conv3-256 | conv3-256 conv3-256 | conv3-256 | conv3-256 convl-256 | conv3-256 | conv3-256 conv3-256 maxpool conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv1-512 | conv3-512 | conv3-512 conv3-512 maxpool conv3-512 | conv3-512 | conv3-512 |] conv3-512 | conv3-512 | conv3-512 conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv1-512 | conv3-512 | conv3-512 conv3-512 Table 2: Number of parameters (in millions). Network A,A-LRN B C D E such layers have a 7 x 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3 x 3 conv. layers instead of a single 7 x 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 x 3 convolution stack has C channels, the stack is parametrised by 3 (3°C?) = 270? weights; at the same time, a single 7 x 7 conv. layer would require 77C? = 49C? parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7 x 7 conv. filters, forcing them to have a decomposition through the 3 x 3 filters (with non-linearity injected in between). The incorporation of 1 x 1 conv. layers (configuration C, Table[I) is a way to increase the non- linearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 x 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1 x 1 conv. layers have recently been utilised in the “Network in Network” architecture of|Lin et al] (2014). Small-size convolution filters have been previously used by (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. [Goodfellow et al] (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al.,|2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets Published as a conference paper at ICLR 2015 (22 weight layers) and small convolution filters (apart from 3 x 3, they also use 1 x 1 and 5 x 5 convolutions). Their network topology is, however, more complex than ours, and the spatial reso- lution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. our model is outperforming that of |Szegedy et al] in terms of the single-network classification accuracy. 3. CLASSIFICATION FRAMEWORK In the previous section we presented the details of our network configurations. In this section, we describe the details of classification ConvNet training and evaluation. 3.1 TRAINING The ConvNet training procedure generally follows (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et all (1929) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the Lz penalty multiplier set to 5-10~4) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to 10~?, and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al||2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers. The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table[I), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully- connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and 10~? variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of|Glorot & Bengio (2010). To obtain the fixed-size 224 x 224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al||2012). Training image rescaling is explained below. Training image size. Let S be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to S' as the training scale). While the crop size is fixed to 224 x 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for S' > 224 the crop will correspond to a small part of the image, containing a small object or an object part. We consider two approaches for setting the training scale S. The first is to fix S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi- scale image statistics). In our experiments, we evaluated models trained at two fixed scales:_S = 256 (which has been widely used in the prior art Somnnetetail Dora) and S = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of 107°. The second approach to setting S' is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [Smin, Smax] (we used Sirin = 256 and Smax = 512). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single Published as a conference paper at ICLR 2015 model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same configuration, pre-trained with fixed S = 384. 3.2 TESTING At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect.4] using several values of Q for each S' leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sma TO Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 x 7 conv. layer, the last two FC layers to 1 x 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image. Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time 2012), which is less efficient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szewedy etal] (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 x 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by oeeed otal (2014). 3.3. IMPLEMENTATION DETAILS Our implementation is derived from the publicly available C++ Caffe toolbox ( (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU. While more sophisticated methods of speeding up ConvNet training have been recently pro- posed (Krizhevsky, (2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2-3 weeks depending on the architecture. 4 CLASSIFICATION EXPERIMENTS Dataset. In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012-2014 chal- lenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The clas- sification performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the Published as a conference paper at ICLR 2015 main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories. For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al. 2014). 4.1 SINGLE SCALE EVALUATION We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in Sect.[2.2] The test image size was set as follows: Q = S for fixed S,and Q =0.5(Smin + Smax) for jittered S € [Simin, Smax]. The results of are shown in TableB] First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B—E). Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 x 1 conv. layers), performs worse than the configuration D, which uses 3 x 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 x 5 conv. layers, which was derived from B by replacing each pair of 3 x 3 conv. layers with a single 5 x 5 conv. layer (which has the same receptive field as explained in Sect.[2.3). The top-! error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters. Finally, scale jittering at training time (S € [256;512]) leads to significantly better results than training on images with fixed smallest side (S = 256 or S = 384), even though a single scale is used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics. ConvNet config. (TableLI) [smallest image side | top-I val. error (%) | top-5 val. error (%) train(S) | test (Q) A A-LRN B Cc [2563512] 256 D 384 E 4.2 MULTI-SCALE EVALUATION Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: Q = {S — 32,5,5 + 32}. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable S € [Sinin3 Smaz] was evaluated over a larger range of sizes Q = {Smin,0.5(Simin + Smazx); Smaz }- Published as a conference paper at ICLR 2015 The results, presented in Table[4] indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table B). As before, the deepest configurations (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table[4). On the test set, the configuration E achieves 7.3% top-5 error. ConvNet config. (Table[I) smallest image si top-1 val. error (%) | top-5 val. error (%) train (S) B 256 256 256,288 : 5 Cc 384 352,384,416 27.8 9.2 [256; 512] | 256,384,512 256 224,256,288 384,416 256; 512] | 256,384,512 ; 256 224,256,288 26.9 &7 RA 84.4 i 3 0 256; 512] | 256,384,512 4.3. MULTI-CROP EVALUATION In Table [5] we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2] for de- tails). We also assess the complementarity of the two evaluation techniques by averaging their soft- max outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions. Table 5: ConvNet evaluation techniques comparison. In all experiments the training scale S was sampled from [256; 512], and three test scales Q were considered: {256, 384,512}. ConvNet config. (Tablel) [ Evaluation method | top-I val. error (%) | top-5 val. error (%) lense 24.8 75 D ti-crop multi-crop & 4.4 CONVNET FUSION Up until now, we evaluated the performance of individual ConvNet models. In this part of the exper- iments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et all, and 2013 (Zeiler & Fergus, 2013: [Sermanet et al} 2014). The results are shown in Table [6] By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table[5). 4.5 COMPARISON WITH THE STATE OF THE ART Finally, we compare our results with the state of the art in Table [7] In the classification task of ILSVRC-2014 challenge (Russakovsky et al (2014), our “VGG” team secured the 2nd place with Published as a conference paper at ICLR 2015 Combined ConvNet models top-I val] top-5 val ILSVRC submission (D/256/224,256,288), (D/384/352,384,416), (D/[256;512]/256,384,512) (C/256/224,256,288), (C/384/352,384,416) (E/256/224,256,288), (E/384/352,384,416) 15 73 (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), dense eval. D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop & dense eval. 7A 7.0 7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models. As can be seen from Table[J] our very deep ConvNets significantly outperform the previous gener- ation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competi- tions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models — significantly less than used in most ILSVRC submissions. In terms of the single- -net performance, our architecture achieves the best result (7.0% test error), outperforming a single Goog et by 0.9%. Notably, we did not depart from the classical ConvNet architecture of [LeCun et al] a Goed , but improved it by substantially increasing the depth. Table 7: Comparison with the state of the art in ILSVRC classification. Our method is denoted as “VGG”. Only the results obtained without outside training data are reported. Method top-1 val.\"),\n",
       " Document(metadata={'id_key': '89925c79-6a9e-4230-9c2c-5b1d6e7e43ea', 'type': 'text'}, page_content='error (%)|top-5 val. error (%)|top-5 test error (%) VGG (2 nets, multi-crop & dense eval.) 23.7 6.8 6.8 VGG (I net, multi-crop & dense eval.) 24.4 7A 7.0 VGG (ILSVRC submission, 7 nets, dense eval.) 24.7 75 7.3 GoogLeNet (Szegedy et al., 2014) (I net) - 79 MSRA (He et al. sor tt nets) MSRA (He et al., 2014) (1 net) Clarifai Russakousiy et a 2014) (multiple nets) Zeiler & Forus Zeiler & Feraus st a sat OverFeat (Sermanet et al., 2014) (7 nets) OverFeat (Sermanet ct al., 2014) (1 net) Krizhevsky et al. Rrchesky et al., 2012) (1 net) 5 CONCLUSION In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large- scale image classification. It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al.,{1989%|Krizhevsky et al.,|2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again confirm the importance of depth in visual representations. This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. Published as a conference paper at ICLR 2015 Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context database. CoRR, abs/1412.0623, 2014. Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. In Proc.'),\n",
       " Document(metadata={'id_key': 'b036192f-a43d-4be9-bd58-e3732e120f0b', 'type': 'text'}, page_content='BMVC., 2014. Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation. CoRR, abs/1411.6836, 2014. Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance convolutional neural networks for image classification. In LJCAI, pp. 1237-1242, 2011. Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In N/PS, pp. 1232-1240, 2012. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proc. CVPR, 2009. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013. Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual object classes challenge: A retrospective. LJCV, 111(1):98-136, 2015. Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In EEE CVPR Workshop of Generative Model Based Vision, 2004. Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014. Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604, 2014. Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. AISTATS, volume 9, pp. 249-256, 2010. Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street view imagery using deep convolutional neural networks. In Proc.'),\n",
       " Document(metadata={'id_key': '42df3c41-c29a-4f95-9514-c08cc6235aaa', 'type': 'text'}, page_content='ICLR, 2014. Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR, abs/1406.4729v2, 2014. Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014. Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc. ICLR, 2014. Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR, abs/1412.2306, 2014. Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539, 2014. Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014. Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural net- works. In N/PS, pp. 1106-1114, 2012. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropa- gation applied to handwritten zip code recognition. Neural Computation, 1(4):541-551, 1989. Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. JCLR, 2014. Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038, 2014. Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks. In Proc. CVPR, 2014. Perronnin, F., Sanchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In Proc. ECCV, 2010. Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline for Recognition. CoRR, abs/1403.6382, 2014. Published as a conference paper at ICLR 2015 Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014. Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR, abs/1406.2199, 2014. Published in Proc. NIPS, 2014. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR, abs/1406.5726, 2014. Zeiler, M. D.'),\n",
       " Document(metadata={'id_key': '2595bbaf-1d31-4508-9a03-813b4375ef97', 'type': 'text'}, page_content=\"and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. Published in Proc. ECCV, 2014. In the main body of the paper we have considered the classification task of the ILSVRC challenge, and performed a thorough evaluation of ConvNet architectures of different depth. In this section, we turn to the localisation task of the challenge, which we have won in 2014 with 25.3% error. It can be seen as a special case of object detection, where a single object bounding box should be predicted for each of the top-5 classes, irrespective of the actual number of objects of the class. For this we adopt the approach of|Sermanet et al! (2014), the winners of the ILSVRC-2013 localisation challenge, with a few modifications. Our method is described in Sect.[A_Iand evaluated in Sect.[A.2] To perform object localisation, we use a very deep ConvNet, where the last fully connected layer predicts the bounding box location instead of the class scores. A bounding box is represented by a 4-D vector storing its center coordinates, width, and height. There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR [2014)) or is class-specific (per-class regression, PCR). In the former case, the last layer is 4-D, while in the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table[I), which contains 16 weight layers and was found to be the best-performing in the classification task (Sect.[4). Training. Training of localisation ConvNets is similar to that of the classification ConvNets (Sect.B.I). The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth. We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 submission). Training was initialised with the corresponding classification models (trained on the same scales), and the initial learning rate was set to 10-3. We explored both fine-tuning all layers and fine-tuning only the first two fully-connected layers, as done in (Sermanet et all 2014), The last fully-connected layer was initialised randomly and trained from scratch. Testing. We consider two testing protocols. The first is used for comparing different network modifications on the validation set, and considers only the bounding box prediction for the ground truth class (to factor out the classification errors). The bounding box is obtained by applying the network only to the central crop of the image. The second, fully-fledged, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classification task (Sect. [3.2). The difference is that instead of the class score map, the output of the last fully-connected layer is a set of bounding box predictions. To come up with the final prediction, we utilise the greedy merging procedure of (2014), which first merges spatially close predictions (by averaging their coor- dinates), and then rates them based on the class scores, obtained from the classification ConvNet. When several localisation ConvNets are used, we first take the union of their sets of bounding box predictions, and then run the merging procedure on the union. We did not use the multiple pooling 10 Published as a conference paper at ICLR 2015 offsets technique of|Sermanet et al] (2014), which increases the spatial resolution of the bounding box predictions and can further improve the results. A.2 LOCALISATION EXPERIMENTS In this section we first determine the best-performing localisation setting (using the first test proto- col), and then evaluate it in a fully-fledged scenario (the second protocol). The localisation error is measured according to the ILSVRC criterion iRuasakoweky ol DOTS, ie. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5. Settings comparison. As can be seen from Table[8] per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of Bemmanet et all (2014), where PCR was outperformed by SCR. We also note that fine-tuning all layers for the lo- calisation task leads to noticeably better results than fine-tuning only the fully-connected layers (as done in (Sermanet et al.,|2014)). In these experiments, the smallest images side was set to S = 384; the results with S' = 256 exhibit the same behaviour and are not shown for brevity. Table 8: Localisation error for different modifications with the simplified testing protocol: the bounding box is predicted from a single central image crop, and the ground-truth class is used. All ConvNet layers (except for the last one) have the configuration D (Table [I), while the last layer performs either single-class regression (SCR) or per-class regression (PCR). Fine-tuned layers]regression type]GT class localisation error 1st and 2nd FC Fully-fledged evaluation. Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted us- ing our best-performing classification system (Sect. 45) ), and multiple densely-computed bounding box predictions are merged using the method of [Sermanet et al] (014). As can be seen from Ta- ble[9] application of the localisation ConvNet to the whole image substantially improves the results compared to using a center crop (Table[8), despite using the top-5 predicted class labels instead of the ground truth. Similarly to the classification task (Sect.[4), testing at several scales and combining the predictions of multiple networks further improves the performance. Table 9: Localisation error smallest image side top-5 localisation error (%) test (Q) val. test. 256 29.5 26.7 384 352,384 [ fusion: 256/256 and 384/352,384 | 26.9 25.3 Comparison with the state of the art. We compare our best localisation result with the state of the art in Table ma With 25.3% test error, our “VGG” team won the localisation challenge of ILSVRC-2014 (Russakovsky et al |2014). Notably, our results are considerably better than those of the ILSVRC-2013 winner Overfeat (Sermanet et al] (2014), even though we used less scales and did not employ their resolution enhancement technique. We envisage that better localisation per- formance can be achieved if this technique is incorporated into our method. This indicates the performance advancement brought by our very deep ConvNets — we got better results with a simpler localisation method, but a more powerful representation. In the previous sections we have discussed training and evaluation of very deep ConvNets on the ILSVRC dataset. In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature 11 Published as a conference paper at ICLR 2015 Table 10: Comparison with the state of the art in ILSVRC localisation. Our method is denoted Krizhevsky et al. (Krizhevsky et al., 2012 extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting. Recently, there has been a lot of interest in such a use case Donahue et alJ, 2013} [Razavian et al) [20144 Chatfield et al 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin. Following that line of work, we investigate if our models lead to better performance than more shallow models utilised in the state-of-the-art methods. In this evaluation, we consider two models with the best classification performance on ILSVRC (Sect.[4) — configurations “Net-D” and “Net-E” (which we made publicly available). To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales. The resulting image descriptor is L2-normalised and combined with a linear SVM classifier, trained on the target dataset. For simplicity, pre-trained ConvNet weights are kept fixed (no fine-tuning is performed). Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure (Sect. [3.2). Namely, an image is first rescaled so that its smallest side equals Q, and then the net- work is densely applied over the image plane (which is possible when all weight layers are treated as convolutional). We then perform global average pooling on the resulting feature map, which produces a 4096-D image descriptor. The descriptor is then averaged with the descriptor of a hori- zontally flipped image. As was shown in Sect. [4.2] evaluation over multiple scales is beneficial, so we extract features over several scales Q. The resulting multi-scale features can be either stacked or pooled across scales. Stacking allows a subsequent classifier to learn how to optimally combine image statistics over a range of scales; this, however, comes at the cost of the increased descriptor dimensionality. We return to the discussion of this design choice in the experiments below. We also assess late fusion of features, computed using two networks, which is performed by stacking their respective image descriptors. Table 11: Comparison with the state of the art in image classification on VOC-2007, VOC-2012, Caltech-101, and Caltech-256. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (2000 classes). Method VOC-2007 Caltech-101 Caltech-256 (mean AP) | (mean AP) |(mean class recall) | (mean class recall) Zeiler & Fergus (Zeiler & Fergus, 2013) - . 86.5+£0.5 74.2 £0.3 Chatfield et al. (Chatfield et al., 2014) 82.4 . 88.4 + 0.6 776+0.1 He et al. (He et al., 2014) 82.4 93.4 + 0.5 - Wei et al. (Wei et al., 2014) 81.5 (85.2*) - - VGG Net-D (16 layers) ; . 918+ 71.0 85.0 £ 0.2 VGG Net-E (19 layers) . . 92.3 + 0.5 85.1 40.3 VGG Net-D & Net-E . . 92.7 + 0.5 86.2 + 0.3 Image Classification on VOC-2007 and VOC-2012. We begin with the evaluation on the imag classification task of PASCAL VOC-2007 and VOC-2012 benchmarks Bois), These datasets contain 10K and 22.5K images respectively, and each image is annotated with one or several labels, corresponding to 20 object categories. The VOC organisers provide a pre-defined split into training, validation, and test data (the test data for VOC-2012 is not publicly available; instead, an official evaluation server is provided). Recognition performance is measured using mean average precision (mAP) across classes. Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs sim- 12 Published as a conference paper at ICLR 2015 ilarly to the aggregation by stacking. We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-specific seman- tics which a classifier could exploit. Since averaging has a benefit of not inflating the descrip- tor dimensionality, we were able to aggregated image descriptors over a wide range of scales: Q © {256, 384,512,640, 768}. It is worth noting though that the improvement over a smaller range of {256, 384, 512} was rather marginal (0.3%). The test set performance is reported and compared with other approaches in Table[I1] Our networks “Net-D” and “Net-E” exhibit identical performance on VOC datasets, and their combination slightly improves the results. Our methods set the new state of the art across image representations, pre- trained on the ILSVRC dataset, outperforming the previous best result oftChatfield et all (2014) by more than 6%. It should be noted that the method of (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, semantically close to those in VOC datasets. It also benefits from the fusion with an object detection-assisted classification pipeline. Image Classification on Caltech-101 and Caltech-256. In this section we evaluate very deep fea- tures on Caltech-101 and Caltech-256 image classification benchmarks. Caltech-101 contains 9K images labelled into 102 classes (101 object categories and a background class), while Caltech-256 is larger with 31K images and 257 classes. A standard eval- uation protocol on these datasets is to generate several random splits into training and test data and report the average recognition performance across the splits, which is measured by the mean class recall (which compensates for a different number of test images per class). Following|Chatfield et al (2014); (2013); [He et al] (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training images per class (and the rest is used for testing). In each split, 20% of training images were used as a validation set for hyper-parameter selection. We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multi- ple scales, performs better than averaging or max-pooling. This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are se- mantically different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such scale-specific representations. We used three scales Q € {256, 384, 512}. Our models are compared to each other and the state of the art in Table[11] As can be seen, the deeper 19-layer Net-E performs better than the 16-layer Net-D, and their combination further improves the performance. On Caltech-101, our representations are competitive with the approach of (014, which, however, performs significantly worse than our nets on VOC-2007. On Caltech-256, our features outperform the state of the art (Chatfield et al] by a large margin (8.6%). Action Classification on VOC-2012. We also evaluated our best-performing image representa- tion (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task Eecinshae call BOTs) which consists in predicting an action class from a single image, given a bounding box of the person performing the action. The dataset contains 4.6K training im- ages, labelled into 11 classes. Similarly to the VOC-2012 object classification task, the performance is measured using the mAP. We considered two training settings: (i) computing the ConvNet fea- tures on the whole image and ignoring the provided bounding box; (ii) computing the features on the whole image and on the provided bounding box, and stacking them to obtain the final representation. The results are compared to other approaches in Table[I2] Our representation achieves the state of art on the VOC action classification task even without using the provided bounding boxes, and the results are further improved when using both images and bounding boxes. Unlike other approaches, we did not incorporate any task-specific heuristics, but relied on the representation power of very deep convolutional features. Other Recognition Tasks. Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, consistently outperform- ing more shallow representations. For instance, (2014) achieve the state of the object detection results by replacing the ConvNet of |Krizhevsky et al| (2012) with our 16-layer 2012) have been ob- 13 Published as a conference paper at ICLR 2015 Table 12: Comparison with the state of the art in single-image action classification on VOC- 2012. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (1512 classes). Hoai, 2014 VGG Net-D & Net-E, image and bounding box 84.0 | served in semantic segmentation (Long et al, [2014), image caption generation (Kiros etal} 2014; Karpathy & Fei-Feil|2014), texture and material recognition 2014: |Bell et al.,|2014). Here we present the list of major paper revisions, outlining the substantial changes for the conve- nience of the reader.\"),\n",
       " Document(metadata={'id_key': '60bdeacd-4f04-46ee-8bcc-83f4548b8d38', 'type': 'text'}, page_content='v1 Initial version. Presents the experiments carried out before the ILSVRC submission. v2 Adds post-submission ILSVRC experiments with training set augmentation using scale jittering, which improves the performance. v3 Adds generalisation experiments (Appendix[B) on PASCAL VOC and Caltech image classifica- tion datasets. The models used for these experiments are publicly available. v4 The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple crops for classification. v6 Camera-ready ICLR-2015 conference paper. Adds a comparison of the net B with a shallow net and the results on PASCAL VOC action classification benchmark. 14')]"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text_docs_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "58ff3c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['989f1970-8e8e-408a-a968-ce5caa835e54',\n",
       " '89925c79-6a9e-4230-9c2c-5b1d6e7e43ea',\n",
       " 'b036192f-a43d-4be9-bd58-e3732e120f0b',\n",
       " '42df3c41-c29a-4f95-9514-c08cc6235aaa',\n",
       " '2595bbaf-1d31-4508-9a03-813b4375ef97',\n",
       " '60bdeacd-4f04-46ee-8bcc-83f4548b8d38']"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=Text_docs_document, ids=doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "1050e109-6c4e-4688-a80e-12c8eb9a62d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here, I am doing similarity search between images_summaries and Text_vectore_embeddings \\nThen, finding image summaries and text which has high similarity and appending them together.\\nThen, Passing those appended_content to the llm context to get detail summary'"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Here, I am doing similarity search between images_summaries and Text_vectore_embeddings \n",
    "Then, finding image summaries and text which has high similarity and appending them together.\n",
    "Then, Passing those appended_content to the llm context to get detail summary'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "aeddbd34-ba47-40f2-adf5-a670b33810d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.schema import Document\n",
    "\n",
    "for summary in image_summaries:\n",
    "    # Perform similarity search to get top 1 similar documents\n",
    "    results = vector_store.similarity_search(summary, k=1)\n",
    "\n",
    "    for doc in results:\n",
    "        updated_page_content = f\"{doc.page_content} [RELATED_IMAGE_SUMMARY: {summary}]\"\n",
    "        updated_doc = Document( metadata=doc.metadata,page_content=updated_page_content)\n",
    "        vector_store.update_document(document_id=doc.metadata[\"id_key\"], document=updated_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "a2637d0d-40dd-48a4-be6e-aae537a05d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Here, i am extracting entire content stored in vectore database'''\n",
    "\n",
    "result = vector_store.get()\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "updated_image_text_doc3 = []\n",
    "\n",
    "for doc, meta in zip(result['documents'], result['metadatas']):\n",
    "    # Create a Document object with proper arguments\n",
    "    document = Document(metadata=meta, page_content=doc)\n",
    "    updated_image_text_doc3.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "824a7e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id_key': '989f1970-8e8e-408a-a968-ce5caa835e54', 'type': 'text'}, page_content=\"arXiv:1409.1556v6 [cs.CV] 10 Apr 2015 Published as a conference paper at ICLR 2015 Karen Simonyan* & Andrew Zisserman* Visual Geometry Group, Department of Engineering Science, University of Oxford {karen,az}@robots.ox.ac.uk In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 x 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa- tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili- tate further research on the use of deep visual representations in computer vision. 1 INTRODUCTION Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale _im- age and video recognition (Krizhevsky et al. |2012; |Zeiler & Fergus, 20135 Sermanet et all 2014; [Simonyan & Zisserman|,|2014) which has become possible due to the large public image reposito- ries, such as ImageNet (Deng et al |2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al.)|2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recog- nition Challenge (ILSVRC) , which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature en- codings (the winner of ILSVRC-2011) to deep ConvNets (the winner of ILSVRC-2012). With ConvNets becoming more of a commodity in the computer vision field, a number of at- tempts have been made to improve the original architecture of [Krizhevsky et al] (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC- 2013 (Zeiler & Fergus, (2013; [Sermanet et al |2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt_with training and testing the networks densely over the whole image and over multiple scales 014 [Howard D014), In this paper, we address another important aspect of ConvNet architecture design — its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 x 3) convolution filters in all layers. As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning). We have released our two best-performing model] to facilitate further research. The rest of the paper is organised as follows. In Sect. [2] we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect.[3] and the Published as a conference paper at ICLR 2015 configurations are compared on the ILSVRC classification task in Sect. [4] Sect. [5] concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix[A] and discuss the generalisation of very deep features to other datasets in Appendix] Finally, Appendix[C|contains the list of major paper revisions. 2 CONVNET CONFIGURATIONS To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by (Citesan et ail (2011): |Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect.[2.1) and then detail the specific configurations used in the evaluation (Sect.[2.2). Our design choices are then discussed and compared to the prior art in Sect. [2.3] 2.1 ARCHITECTURE During training, the input to our ConvNets is a fixed-size 224 x 224 RGB image. The only pre- processing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 x 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 x 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 x 3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 x 2 pixel window, with stride 2. A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000- way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks. All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al} |2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al 12012): as will be shown in Sect. such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory con- sumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al.||2012). 2.2 CONFIGURATIONS The ConvNet configurations, evaluated in this paper, are outlined in Table[I] one per column. In the following we will refer to the nets by their names (A—E). All configurations follow the generic design presented in Sect. [2.1] and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512. In Table[2]we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in [2014)). 2.3. DISCUSSION Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al.) |2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; [Sermanet et al 2014). Rather than using relatively large receptive fields in the first conv. lay- ers (e.g. 11 x 11 with stride 4 in (Krizhevsky et al|,|2012), or 7 x 7 with stride 2 in (Zeiler & Fergus, 2013; 2014)), we use very small 3 x 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 x 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 x 5; three Published as a conference paper at ICLR 2015 Table 1: ConvNet configurations (shown in columns). The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as “conv (receptive field size)-(number of channels)”. The ReLU activation function is not shown for brevity. 16 weight layers layers input (224 x 224 RGB image) conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 19 weight layers conv3-128 | conv3-128 | conv3-128 | conv3-128 | conv3-128 | conv3-128 conv3-128 | conv3-128 | conv3-128 | conv3-128 maxpool conv3-256 conv3-256 conv3-256 | conv3-256 conv3-256 | conv3-256 conv3-256 | conv3-256 | conv3-256 conv3-256 | conv3-256 | conv3-256 convl-256 | conv3-256 | conv3-256 conv3-256 maxpool conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv1-512 | conv3-512 | conv3-512 conv3-512 maxpool conv3-512 | conv3-512 | conv3-512 |] conv3-512 | conv3-512 | conv3-512 conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 | conv3-512 conv1-512 | conv3-512 | conv3-512 conv3-512 Table 2: Number of parameters (in millions). Network A,A-LRN B C D E such layers have a 7 x 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3 x 3 conv. layers instead of a single 7 x 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 x 3 convolution stack has C channels, the stack is parametrised by 3 (3°C?) = 270? weights; at the same time, a single 7 x 7 conv. layer would require 77C? = 49C? parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7 x 7 conv. filters, forcing them to have a decomposition through the 3 x 3 filters (with non-linearity injected in between). The incorporation of 1 x 1 conv. layers (configuration C, Table[I) is a way to increase the non- linearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 x 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1 x 1 conv. layers have recently been utilised in the “Network in Network” architecture of|Lin et al] (2014). Small-size convolution filters have been previously used by (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. [Goodfellow et al] (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al.,|2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets Published as a conference paper at ICLR 2015 (22 weight layers) and small convolution filters (apart from 3 x 3, they also use 1 x 1 and 5 x 5 convolutions). Their network topology is, however, more complex than ours, and the spatial reso- lution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. our model is outperforming that of |Szegedy et al] in terms of the single-network classification accuracy. 3. CLASSIFICATION FRAMEWORK In the previous section we presented the details of our network configurations. In this section, we describe the details of classification ConvNet training and evaluation. 3.1 TRAINING The ConvNet training procedure generally follows (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et all (1929) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the Lz penalty multiplier set to 5-10~4) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to 10~?, and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al||2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers. The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table[I), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully- connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and 10~? variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of|Glorot & Bengio (2010). To obtain the fixed-size 224 x 224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al||2012). Training image rescaling is explained below. Training image size. Let S be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to S' as the training scale). While the crop size is fixed to 224 x 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for S' > 224 the crop will correspond to a small part of the image, containing a small object or an object part. We consider two approaches for setting the training scale S. The first is to fix S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi- scale image statistics). In our experiments, we evaluated models trained at two fixed scales:_S = 256 (which has been widely used in the prior art Somnnetetail Dora) and S = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of 107°. The second approach to setting S' is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [Smin, Smax] (we used Sirin = 256 and Smax = 512). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single Published as a conference paper at ICLR 2015 model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same configuration, pre-trained with fixed S = 384. 3.2 TESTING At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect.4] using several values of Q for each S' leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sma TO Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 x 7 conv. layer, the last two FC layers to 1 x 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image. Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time 2012), which is less efficient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szewedy etal] (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 x 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by oeeed otal (2014). 3.3. IMPLEMENTATION DETAILS Our implementation is derived from the publicly available C++ Caffe toolbox ( (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU. While more sophisticated methods of speeding up ConvNet training have been recently pro- posed (Krizhevsky, (2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2-3 weeks depending on the architecture. 4 CLASSIFICATION EXPERIMENTS Dataset. In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012-2014 chal- lenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The clas- sification performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the Published as a conference paper at ICLR 2015 main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories. For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al. 2014). 4.1 SINGLE SCALE EVALUATION We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in Sect.[2.2] The test image size was set as follows: Q = S for fixed S,and Q =0.5(Smin + Smax) for jittered S € [Simin, Smax]. The results of are shown in TableB] First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B—E). Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 x 1 conv. layers), performs worse than the configuration D, which uses 3 x 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 x 5 conv. layers, which was derived from B by replacing each pair of 3 x 3 conv. layers with a single 5 x 5 conv. layer (which has the same receptive field as explained in Sect.[2.3). The top-! error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters. Finally, scale jittering at training time (S € [256;512]) leads to significantly better results than training on images with fixed smallest side (S = 256 or S = 384), even though a single scale is used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics. ConvNet config. (TableLI) [smallest image side | top-I val. error (%) | top-5 val. error (%) train(S) | test (Q) A A-LRN B Cc [2563512] 256 D 384 E 4.2 MULTI-SCALE EVALUATION Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: Q = {S — 32,5,5 + 32}. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable S € [Sinin3 Smaz] was evaluated over a larger range of sizes Q = {Smin,0.5(Simin + Smazx); Smaz }- Published as a conference paper at ICLR 2015 The results, presented in Table[4] indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table B). As before, the deepest configurations (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table[4). On the test set, the configuration E achieves 7.3% top-5 error. ConvNet config. (Table[I) smallest image si top-1 val. error (%) | top-5 val. error (%) train (S) B 256 256 256,288 : 5 Cc 384 352,384,416 27.8 9.2 [256; 512] | 256,384,512 256 224,256,288 384,416 256; 512] | 256,384,512 ; 256 224,256,288 26.9 &7 RA 84.4 i 3 0 256; 512] | 256,384,512 4.3. MULTI-CROP EVALUATION In Table [5] we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2] for de- tails). We also assess the complementarity of the two evaluation techniques by averaging their soft- max outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions. Table 5: ConvNet evaluation techniques comparison. In all experiments the training scale S was sampled from [256; 512], and three test scales Q were considered: {256, 384,512}. ConvNet config. (Tablel) [ Evaluation method | top-I val. error (%) | top-5 val. error (%) lense 24.8 75 D ti-crop multi-crop & 4.4 CONVNET FUSION Up until now, we evaluated the performance of individual ConvNet models. In this part of the exper- iments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et all, and 2013 (Zeiler & Fergus, 2013: [Sermanet et al} 2014). The results are shown in Table [6] By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table[5). 4.5 COMPARISON WITH THE STATE OF THE ART Finally, we compare our results with the state of the art in Table [7] In the classification task of ILSVRC-2014 challenge (Russakovsky et al (2014), our “VGG” team secured the 2nd place with Published as a conference paper at ICLR 2015 Combined ConvNet models top-I val] top-5 val ILSVRC submission (D/256/224,256,288), (D/384/352,384,416), (D/[256;512]/256,384,512) (C/256/224,256,288), (C/384/352,384,416) (E/256/224,256,288), (E/384/352,384,416) 15 73 (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), dense eval. D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop & dense eval. 7A 7.0 7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models. As can be seen from Table[J] our very deep ConvNets significantly outperform the previous gener- ation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competi- tions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models — significantly less than used in most ILSVRC submissions. In terms of the single- -net performance, our architecture achieves the best result (7.0% test error), outperforming a single Goog et by 0.9%. Notably, we did not depart from the classical ConvNet architecture of [LeCun et al] a Goed , but improved it by substantially increasing the depth. Table 7: Comparison with the state of the art in ILSVRC classification. Our method is denoted as “VGG”. Only the results obtained without outside training data are reported. Method top-1 val.\"),\n",
       " Document(metadata={'id_key': '89925c79-6a9e-4230-9c2c-5b1d6e7e43ea', 'type': 'text'}, page_content='error (%)|top-5 val. error (%)|top-5 test error (%) VGG (2 nets, multi-crop & dense eval.) 23.7 6.8 6.8 VGG (I net, multi-crop & dense eval.) 24.4 7A 7.0 VGG (ILSVRC submission, 7 nets, dense eval.) 24.7 75 7.3 GoogLeNet (Szegedy et al., 2014) (I net) - 79 MSRA (He et al. sor tt nets) MSRA (He et al., 2014) (1 net) Clarifai Russakousiy et a 2014) (multiple nets) Zeiler & Forus Zeiler & Feraus st a sat OverFeat (Sermanet et al., 2014) (7 nets) OverFeat (Sermanet ct al., 2014) (1 net) Krizhevsky et al. Rrchesky et al., 2012) (1 net) 5 CONCLUSION In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large- scale image classification. It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al.,{1989%|Krizhevsky et al.,|2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again confirm the importance of depth in visual representations. This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. Published as a conference paper at ICLR 2015 Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context database. CoRR, abs/1412.0623, 2014. Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. In Proc. [RELATED_IMAGE_SUMMARY: The image consists of two graphs comparing the training and test errors of two neural network architectures with different depths: a 56-layer model and a 20-layer model.\\n\\n### Left Graph (Training Error):\\n- **Y-axis**: Training error (%) \\n- **X-axis**: Iterations (in thousands)\\n- The 56-layer model (red) starts with a higher error but decreases more rapidly, approaching lower error values.\\n- The 20-layer model (yellow) has a consistently lower error but does not decrease as significantly as the 56-layer model.\\n\\n### Right Graph (Test Error):\\n- **Y-axis**: Test error (%) \\n- **X-axis**: Iterations (in thousands)\\n- Similar trends: the 56-layer model shows a higher initial test error that decreases over time, while the 20-layer model maintains a lower but less decreasing trend.\\n\\nOverall, the 56-layer model shows better performance in both training and test error reduction compared to the 20-layer model over the iterations.] [RELATED_IMAGE_SUMMARY: The image illustrates a neural network architecture featuring a residual block. It starts with an input \\\\( x \\\\), which passes through two weight layers followed by a ReLU activation function. The output of this sequence is added to the original input \\\\( x \\\\), and the sum is then processed by another ReLU activation. This architecture allows for the learning of residual functions, facilitating better training of deep networks.] [RELATED_IMAGE_SUMMARY: The image shows two line graphs comparing error rates (%) over iterations for different neural network architectures. \\n\\n**Left Graph:**\\n- Compares \"plain\" networks with 18 and 34 layers.\\n- The error for the 34-layer model starts higher but decreases more effectively over iterations compared to the 18-layer model, which shows a less steep decline.\\n\\n**Right Graph:**\\n- Compares ResNet architectures with 18 and 34 layers.\\n- Similar trends are observed; the ResNet-34 model achieves lower error rates compared to ResNet-18, indicating better performance as training progresses.\\n\\nOverall, deeper networks (34 layers) tend to perform better than shallower ones (18 layers) in both plain and ResNet architectures.] [RELATED_IMAGE_SUMMARY: The image presents two neural network architectures, showcasing different configurations for processing data. \\n\\n1. **64-dimensional (64-d) Architecture**:\\n   - Consists of two sequential layers of 3x3 convolutions, each with 64 filters.\\n   - A ReLU activation function is applied after each convolution.\\n   - Outputs are combined using an addition operation before passing through another ReLU.\\n\\n2. **256-dimensional (256-d) Architecture**:\\n   - Features a 1x1 convolution with 64 filters, followed by a 3x3 convolution (64 filters), and another 1x1 convolution with 256 filters.\\n   - Similar to the first architecture, ReLU activations are applied, and outputs are summed before the final activation.\\n\\nBoth architectures highlight the use of convolutional layers and residual connections.] [RELATED_IMAGE_SUMMARY: The image consists of three plots showing the error rates (%) over time for various neural network architectures, specifically different configurations of ResNet layers:\\n\\n1. **Left Plot**: Displays the error rates for plain networks with 20, 32, 44, and 56 layers. The 56-layer network (in red) shows a higher error rate initially but improves significantly over time, while the 20-layer network (in blue) achieves a lower error rate earlier on.\\n\\n2. **Middle Plot**: Illustrates the error rates for ResNet architectures with 20, 32, 44, 56, and 110 layers. The 56-layer ResNet (in red) performs well, with a noticeable improvement in error reduction over time. The 110-layer ResNet (in black) maintains a lower error rate compared to the others.\\n\\n3. **Right Plot**: Compares two residual networks: 110 layers (black) and 1202 layers (magenta). Both show low error rates, with the 1202-layer network demonstrating slightly better performance, especially over extended training.\\n\\nOverall, the plots suggest that deeper networks generally achieve lower error rates, albeit with varying initial performances.] [RELATED_IMAGE_SUMMARY: The image presents two graphs comparing the standard deviation (std) of different neural network architectures across their layer indices. \\n\\n1. **Top Graph**: Displays std values against original layer indices. The lines represent:\\n   - Plain-20 (dashed yellow)\\n   - Plain-56 (dashed red)\\n   - ResNet-20 (solid red)\\n   - ResNet-56 (dashed green)\\n   - ResNet-110 (solid black)\\n   The std values vary significantly, with ResNet-110 showing a relatively stable trend.\\n\\n2. **Bottom Graph**: Shows std values sorted by magnitude across the same layer indices. The trends are more consistent, with ResNet-110 maintaining lower std values compared to the others, indicating better performance or stability across layers.\\n\\nOverall, the graphs illustrate the differences in layer-wise standard deviation for various neural network designs, highlighting ResNet-110\\'s superior stability.] [RELATED_IMAGE_SUMMARY: The image presents a flowchart illustrating two convolutional neural network architectures for image processing. Each architecture consists of layers that perform convolution operations followed by pooling layers, which downsample the feature maps. \\n\\n1. **Left Architecture**:\\n   - Starts with a 224x224 input image.\\n   - Contains multiple convolution layers with varying filter sizes (3x3 and 7x7) and increasing filter counts (64, 128, 256, 512).\\n   - Progressive downsampling through pooling layers reduces the output size from 224 to 7.\\n   - Ends with a fully connected layer (fc 4096) and an average pooling layer.\\n\\n2. **Right Architecture**:\\n   - Also processes a 224x224 input image.\\n   - Features similar convolution layers but with a 7x7 convolution at the beginning and a different arrangement of layer sizes.\\n   - The output size is reduced to 1 at the final layer.\\n\\nBoth architectures are designed for deep learning tasks, likely focused on image classification or recognition.] [RELATED_IMAGE_SUMMARY: The table presents a summary of convolutional layers in a neural network architecture across various depths (18-layer, 34-layer, 50-layer, 101-layer, and 152-layer). \\n\\n- **Layer Names**: The layers are named conv1, conv2_x, conv3_x, conv4_x, and conv5_x.\\n- **Output Size**: The output sizes decrease progressively from 112x112 in conv1 to 1x1 in conv5_x.\\n- **Layer Configuration**: Each layer varies in the number of filter sizes and quantities:\\n  - **conv2_x**: 56x56 output, with multiple configurations of 3x3 filters.\\n  - **conv3_x**: 28x28 output, also with 3x3 filters, but with higher filter counts.\\n  - **conv4_x**: 14x14 output, using 3x3 filters with increased complexity.\\n  - **conv5_x**: 7x7 output, with the most filters and configurations.\\n- **Final Layer**: The last layer is an average pooling followed by a fully connected (fc) layer with 1000 outputs and a softmax activation.\\n- **FLOPs**: The table also lists the number of floating point operations (FLOPs) for each architecture variation, ranging from 1.8 x 10^9 for the 18-layer model to 11.3 x 10^9 for the 152-layer model. \\n\\nThis structure indicates the increasing complexity and computational demand of deeper network architectures.] [RELATED_IMAGE_SUMMARY: The table compares the performance of two neural network architectures (plain and ResNet) across two different layer configurations (18 layers and 34 layers). \\n\\n- For 18 layers: \\n  - Plain model achieves a score of 27.94.\\n  - ResNet model achieves a score of 27.88.\\n\\n- For 34 layers:\\n  - Plain model achieves a higher score of 28.54.\\n  - ResNet model has a significantly lower score of 25.03.\\n\\nOverall, the plain model consistently outperforms the ResNet model in both configurations.] [RELATED_IMAGE_SUMMARY: The table compares the performance of various models based on their top-1 and top-5 error rates. \\n\\n- **VGG-16** has a top-1 error rate of 28.07 and a top-5 error rate of 9.33.\\n- **GoogLeNet** shows a top-1 error rate as not applicable (-) and a top-5 error rate of 9.15.\\n- **PReLU-net** achieves a top-1 error of 24.27 and a top-5 error of 7.38.\\n\\nFor the other models:\\n- **plain-34** has a top-1 error of 28.54 and a top-5 error of 10.02.\\n- **ResNet-34 A**: 25.03 (top-1), 7.76 (top-5)\\n- **ResNet-34 B**: 24.52 (top-1), 7.46 (top-5)\\n- **ResNet-34 C**: 24.19 (top-1), 7.40 (top-5)\\n- **ResNet-50**: 22.85 (top-1), 6.71 (top-5)\\n- **ResNet-101**: 21.75 (top-1), 6.05 (top-5)\\n- **ResNet-152**: 21.43 (top-1), 5.71 (top-5)\\n\\nOverall, ResNet-152 shows the best performance with the lowest top-1 and top-5 error rates.] [RELATED_IMAGE_SUMMARY: The table summarizes the performance of various neural network architectures in terms of their top-1 and top-5 error rates on the ILSVRC\\'14 dataset. \\n\\nKey points include:\\n\\n- VGG and GoogLeNet do not have top-1 error rates listed, but GoogLeNet has a top-5 error of 7.89.\\n- The VGG model (v5) has a top-1 error of 24.4 and a top-5 error of 7.1.\\n- Among other models, ResNet-152 shows the best performance with a top-1 error of 19.38 and a top-5 error of 4.49, followed closely by ResNet-101 with top-1 error of 19.87 and top-5 error of 4.60. \\n- Overall, newer architectures like ResNet models outperform the earlier VGG and GoogLeNet models.] [RELATED_IMAGE_SUMMARY: The table summarizes the top-5 error rates for various image classification methods tested in the ILSVRC (ImageNet Large Scale Visual Recognition Challenge). \\n\\n- **ResNet (ILSVC\\'15)** shows the lowest error rate at **3.57%**.\\n- The next best is **BN-inception** at **4.82%**, followed closely by **PReLU-net** at **4.94%**.\\n- **VGG (v5)** has an error rate of **6.8%**, while **GoogLeNet** stands at **6.66%**.\\n- The original **VGG** method has the highest error rate in this list at **7.32%**.] [RELATED_IMAGE_SUMMARY: The table presents information about three different output map sizes (32x32, 16x16, and 8x8) in relation to the number of layers and filters in a neural network:\\n\\n- For the **32x32** map size:\\n  - **# layers**: \\\\(1 + 2n\\\\)\\n  - **# filters**: 16\\n\\n- For the **16x16** map size:\\n  - **# layers**: \\\\(2n\\\\)\\n  - **# filters**: 32\\n\\n- For the **8x8** map size:\\n  - **# layers**: \\\\(2n\\\\)\\n  - **# filters**: 64\\n\\nThis indicates a relationship between the output size, the number of layers, and the number of filters, with larger maps generally having fewer filters and varying layer counts based on a variable \\\\(n\\\\).] [RELATED_IMAGE_SUMMARY: The table presents various neural network methods along with their corresponding error rates, number of layers, and parameters. \\n\\n- **Maxout**, **NIN**, and **DSN** are methods with error rates of 9.38%, 8.81%, and 8.22%, respectively.\\n- **FitNet** (19 layers, 2.5M params) has an error of 8.39%.\\n- **Highway Networks**: One with 19 layers and 2.3M params shows 7.54% error; another with 32 layers and 1.25M params has 8.80% error.\\n- **ResNet** variations exhibit decreasing error rates with increasing layers: \\n  - 20 layers: 8.75%\\n  - 32 layers: 7.51%\\n  - 44 layers: 7.17%\\n  - 56 layers: 6.97%\\n  - 110 layers: 6.43% (with a reported variance of ±0.16)\\n  - 1202 layers: 7.93%\\n\\nOverall, deeper ResNet architectures tend to achieve lower error rates.]'),\n",
       " Document(metadata={'id_key': 'b036192f-a43d-4be9-bd58-e3732e120f0b', 'type': 'text'}, page_content='BMVC., 2014. Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation. CoRR, abs/1411.6836, 2014. Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance convolutional neural networks for image classification. In LJCAI, pp. 1237-1242, 2011. Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In N/PS, pp. 1232-1240, 2012. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proc. CVPR, 2009. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013. Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual object classes challenge: A retrospective. LJCV, 111(1):98-136, 2015. Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In EEE CVPR Workshop of Generative Model Based Vision, 2004. Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014. Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604, 2014. Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. AISTATS, volume 9, pp. 249-256, 2010. Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street view imagery using deep convolutional neural networks. In Proc.'),\n",
       " Document(metadata={'id_key': '42df3c41-c29a-4f95-9514-c08cc6235aaa', 'type': 'text'}, page_content='ICLR, 2014. Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR, abs/1406.4729v2, 2014. Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014. Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc. ICLR, 2014. Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR, abs/1412.2306, 2014. Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539, 2014. Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014. Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural net- works. In N/PS, pp. 1106-1114, 2012. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropa- gation applied to handwritten zip code recognition. Neural Computation, 1(4):541-551, 1989. Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. JCLR, 2014. Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038, 2014. Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks. In Proc. CVPR, 2014. Perronnin, F., Sanchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In Proc. ECCV, 2010. Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline for Recognition. CoRR, abs/1403.6382, 2014. Published as a conference paper at ICLR 2015 Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014. Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR, abs/1406.2199, 2014. Published in Proc. NIPS, 2014. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR, abs/1406.5726, 2014. Zeiler, M. D.'),\n",
       " Document(metadata={'id_key': '2595bbaf-1d31-4508-9a03-813b4375ef97', 'type': 'text'}, page_content=\"and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. Published in Proc. ECCV, 2014. In the main body of the paper we have considered the classification task of the ILSVRC challenge, and performed a thorough evaluation of ConvNet architectures of different depth. In this section, we turn to the localisation task of the challenge, which we have won in 2014 with 25.3% error. It can be seen as a special case of object detection, where a single object bounding box should be predicted for each of the top-5 classes, irrespective of the actual number of objects of the class. For this we adopt the approach of|Sermanet et al! (2014), the winners of the ILSVRC-2013 localisation challenge, with a few modifications. Our method is described in Sect.[A_Iand evaluated in Sect.[A.2] To perform object localisation, we use a very deep ConvNet, where the last fully connected layer predicts the bounding box location instead of the class scores. A bounding box is represented by a 4-D vector storing its center coordinates, width, and height. There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR [2014)) or is class-specific (per-class regression, PCR). In the former case, the last layer is 4-D, while in the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table[I), which contains 16 weight layers and was found to be the best-performing in the classification task (Sect.[4). Training. Training of localisation ConvNets is similar to that of the classification ConvNets (Sect.B.I). The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth. We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 submission). Training was initialised with the corresponding classification models (trained on the same scales), and the initial learning rate was set to 10-3. We explored both fine-tuning all layers and fine-tuning only the first two fully-connected layers, as done in (Sermanet et all 2014), The last fully-connected layer was initialised randomly and trained from scratch. Testing. We consider two testing protocols. The first is used for comparing different network modifications on the validation set, and considers only the bounding box prediction for the ground truth class (to factor out the classification errors). The bounding box is obtained by applying the network only to the central crop of the image. The second, fully-fledged, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classification task (Sect. [3.2). The difference is that instead of the class score map, the output of the last fully-connected layer is a set of bounding box predictions. To come up with the final prediction, we utilise the greedy merging procedure of (2014), which first merges spatially close predictions (by averaging their coor- dinates), and then rates them based on the class scores, obtained from the classification ConvNet. When several localisation ConvNets are used, we first take the union of their sets of bounding box predictions, and then run the merging procedure on the union. We did not use the multiple pooling 10 Published as a conference paper at ICLR 2015 offsets technique of|Sermanet et al] (2014), which increases the spatial resolution of the bounding box predictions and can further improve the results. A.2 LOCALISATION EXPERIMENTS In this section we first determine the best-performing localisation setting (using the first test proto- col), and then evaluate it in a fully-fledged scenario (the second protocol). The localisation error is measured according to the ILSVRC criterion iRuasakoweky ol DOTS, ie. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5. Settings comparison. As can be seen from Table[8] per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of Bemmanet et all (2014), where PCR was outperformed by SCR. We also note that fine-tuning all layers for the lo- calisation task leads to noticeably better results than fine-tuning only the fully-connected layers (as done in (Sermanet et al.,|2014)). In these experiments, the smallest images side was set to S = 384; the results with S' = 256 exhibit the same behaviour and are not shown for brevity. Table 8: Localisation error for different modifications with the simplified testing protocol: the bounding box is predicted from a single central image crop, and the ground-truth class is used. All ConvNet layers (except for the last one) have the configuration D (Table [I), while the last layer performs either single-class regression (SCR) or per-class regression (PCR). Fine-tuned layers]regression type]GT class localisation error 1st and 2nd FC Fully-fledged evaluation. Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted us- ing our best-performing classification system (Sect. 45) ), and multiple densely-computed bounding box predictions are merged using the method of [Sermanet et al] (014). As can be seen from Ta- ble[9] application of the localisation ConvNet to the whole image substantially improves the results compared to using a center crop (Table[8), despite using the top-5 predicted class labels instead of the ground truth. Similarly to the classification task (Sect.[4), testing at several scales and combining the predictions of multiple networks further improves the performance. Table 9: Localisation error smallest image side top-5 localisation error (%) test (Q) val. test. 256 29.5 26.7 384 352,384 [ fusion: 256/256 and 384/352,384 | 26.9 25.3 Comparison with the state of the art. We compare our best localisation result with the state of the art in Table ma With 25.3% test error, our “VGG” team won the localisation challenge of ILSVRC-2014 (Russakovsky et al |2014). Notably, our results are considerably better than those of the ILSVRC-2013 winner Overfeat (Sermanet et al] (2014), even though we used less scales and did not employ their resolution enhancement technique. We envisage that better localisation per- formance can be achieved if this technique is incorporated into our method. This indicates the performance advancement brought by our very deep ConvNets — we got better results with a simpler localisation method, but a more powerful representation. In the previous sections we have discussed training and evaluation of very deep ConvNets on the ILSVRC dataset. In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature 11 Published as a conference paper at ICLR 2015 Table 10: Comparison with the state of the art in ILSVRC localisation. Our method is denoted Krizhevsky et al. (Krizhevsky et al., 2012 extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting. Recently, there has been a lot of interest in such a use case Donahue et alJ, 2013} [Razavian et al) [20144 Chatfield et al 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin. Following that line of work, we investigate if our models lead to better performance than more shallow models utilised in the state-of-the-art methods. In this evaluation, we consider two models with the best classification performance on ILSVRC (Sect.[4) — configurations “Net-D” and “Net-E” (which we made publicly available). To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales. The resulting image descriptor is L2-normalised and combined with a linear SVM classifier, trained on the target dataset. For simplicity, pre-trained ConvNet weights are kept fixed (no fine-tuning is performed). Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure (Sect. [3.2). Namely, an image is first rescaled so that its smallest side equals Q, and then the net- work is densely applied over the image plane (which is possible when all weight layers are treated as convolutional). We then perform global average pooling on the resulting feature map, which produces a 4096-D image descriptor. The descriptor is then averaged with the descriptor of a hori- zontally flipped image. As was shown in Sect. [4.2] evaluation over multiple scales is beneficial, so we extract features over several scales Q. The resulting multi-scale features can be either stacked or pooled across scales. Stacking allows a subsequent classifier to learn how to optimally combine image statistics over a range of scales; this, however, comes at the cost of the increased descriptor dimensionality. We return to the discussion of this design choice in the experiments below. We also assess late fusion of features, computed using two networks, which is performed by stacking their respective image descriptors. Table 11: Comparison with the state of the art in image classification on VOC-2007, VOC-2012, Caltech-101, and Caltech-256. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (2000 classes). Method VOC-2007 Caltech-101 Caltech-256 (mean AP) | (mean AP) |(mean class recall) | (mean class recall) Zeiler & Fergus (Zeiler & Fergus, 2013) - . 86.5+£0.5 74.2 £0.3 Chatfield et al. (Chatfield et al., 2014) 82.4 . 88.4 + 0.6 776+0.1 He et al. (He et al., 2014) 82.4 93.4 + 0.5 - Wei et al. (Wei et al., 2014) 81.5 (85.2*) - - VGG Net-D (16 layers) ; . 918+ 71.0 85.0 £ 0.2 VGG Net-E (19 layers) . . 92.3 + 0.5 85.1 40.3 VGG Net-D & Net-E . . 92.7 + 0.5 86.2 + 0.3 Image Classification on VOC-2007 and VOC-2012. We begin with the evaluation on the imag classification task of PASCAL VOC-2007 and VOC-2012 benchmarks Bois), These datasets contain 10K and 22.5K images respectively, and each image is annotated with one or several labels, corresponding to 20 object categories. The VOC organisers provide a pre-defined split into training, validation, and test data (the test data for VOC-2012 is not publicly available; instead, an official evaluation server is provided). Recognition performance is measured using mean average precision (mAP) across classes. Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs sim- 12 Published as a conference paper at ICLR 2015 ilarly to the aggregation by stacking. We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-specific seman- tics which a classifier could exploit. Since averaging has a benefit of not inflating the descrip- tor dimensionality, we were able to aggregated image descriptors over a wide range of scales: Q © {256, 384,512,640, 768}. It is worth noting though that the improvement over a smaller range of {256, 384, 512} was rather marginal (0.3%). The test set performance is reported and compared with other approaches in Table[I1] Our networks “Net-D” and “Net-E” exhibit identical performance on VOC datasets, and their combination slightly improves the results. Our methods set the new state of the art across image representations, pre- trained on the ILSVRC dataset, outperforming the previous best result oftChatfield et all (2014) by more than 6%. It should be noted that the method of (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, semantically close to those in VOC datasets. It also benefits from the fusion with an object detection-assisted classification pipeline. Image Classification on Caltech-101 and Caltech-256. In this section we evaluate very deep fea- tures on Caltech-101 and Caltech-256 image classification benchmarks. Caltech-101 contains 9K images labelled into 102 classes (101 object categories and a background class), while Caltech-256 is larger with 31K images and 257 classes. A standard eval- uation protocol on these datasets is to generate several random splits into training and test data and report the average recognition performance across the splits, which is measured by the mean class recall (which compensates for a different number of test images per class). Following|Chatfield et al (2014); (2013); [He et al] (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training images per class (and the rest is used for testing). In each split, 20% of training images were used as a validation set for hyper-parameter selection. We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multi- ple scales, performs better than averaging or max-pooling. This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are se- mantically different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such scale-specific representations. We used three scales Q € {256, 384, 512}. Our models are compared to each other and the state of the art in Table[11] As can be seen, the deeper 19-layer Net-E performs better than the 16-layer Net-D, and their combination further improves the performance. On Caltech-101, our representations are competitive with the approach of (014, which, however, performs significantly worse than our nets on VOC-2007. On Caltech-256, our features outperform the state of the art (Chatfield et al] by a large margin (8.6%). Action Classification on VOC-2012. We also evaluated our best-performing image representa- tion (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task Eecinshae call BOTs) which consists in predicting an action class from a single image, given a bounding box of the person performing the action. The dataset contains 4.6K training im- ages, labelled into 11 classes. Similarly to the VOC-2012 object classification task, the performance is measured using the mAP. We considered two training settings: (i) computing the ConvNet fea- tures on the whole image and ignoring the provided bounding box; (ii) computing the features on the whole image and on the provided bounding box, and stacking them to obtain the final representation. The results are compared to other approaches in Table[I2] Our representation achieves the state of art on the VOC action classification task even without using the provided bounding boxes, and the results are further improved when using both images and bounding boxes. Unlike other approaches, we did not incorporate any task-specific heuristics, but relied on the representation power of very deep convolutional features. Other Recognition Tasks. Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, consistently outperform- ing more shallow representations. For instance, (2014) achieve the state of the object detection results by replacing the ConvNet of |Krizhevsky et al| (2012) with our 16-layer 2012) have been ob- 13 Published as a conference paper at ICLR 2015 Table 12: Comparison with the state of the art in single-image action classification on VOC- 2012. Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (1512 classes). Hoai, 2014 VGG Net-D & Net-E, image and bounding box 84.0 | served in semantic segmentation (Long et al, [2014), image caption generation (Kiros etal} 2014; Karpathy & Fei-Feil|2014), texture and material recognition 2014: |Bell et al.,|2014). Here we present the list of major paper revisions, outlining the substantial changes for the conve- nience of the reader.\"),\n",
       " Document(metadata={'id_key': '60bdeacd-4f04-46ee-8bcc-83f4548b8d38', 'type': 'text'}, page_content='v1 Initial version. Presents the experiments carried out before the ILSVRC submission. v2 Adds post-submission ILSVRC experiments with training set augmentation using scale jittering, which improves the performance. v3 Adds generalisation experiments (Appendix[B) on PASCAL VOC and Caltech image classifica- tion datasets. The models used for these experiments are publicly available. v4 The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple crops for classification. v6 Camera-ready ICLR-2015 conference paper. Adds a comparison of the net B with a shallow net and the results on PASCAL VOC action classification benchmark. 14 [RELATED_IMAGE_SUMMARY: The table presents mean Average Precision (mAP) results for different configurations of the Faster R-CNN model on the COCO dataset. \\n\\nKey points include:\\n\\n- **Training Data**: COCO train and COCO trainval.\\n- **Test Data**: COCO val and COCO test-dev.\\n- **mAP Scores**:\\n  - Baseline models show mAP scores of 41.5 (VGG-16) and 48.4 (ResNet-101) at IoU threshold 0.5.\\n  - Improvements with additional techniques:\\n    - Box refinement: 49.9\\n    - Context: 51.1\\n    - Multi-scale testing: 53.8\\n- **Ensemble Model**: Achieves the highest scores of 59.0 for COCO trainval and 37.4 for COCO test-dev.\\n\\nOverall, the addition of techniques consistently improves performance across both validation and test datasets.] [RELATED_IMAGE_SUMMARY: The table presents performance metrics for various models evaluated in the ILSVRC challenges. It includes:\\n\\n- **GoogLeNet (ILSVRC\\'14)**: No validation score provided, with a test score of 43.9.\\n- **Our single model (ILSVRC\\'15)**: Validation score of 60.5 and a test score of 58.8.\\n- **Our ensemble (ILSVRC\\'15)**: Validation score of 63.6 and a test score of 62.1.\\n\\nOverall, the ensemble model shows the highest performance in both validation and testing compared to the single model and GoogLeNet.] [RELATED_IMAGE_SUMMARY: The table summarizes the performance of various localization (LOC) methods and networks in terms of their classification error rates. Key points include:\\n\\n- **LOC Methods**: VGG, RPN (Region Proposal Network).\\n- **Networks Used**: VGG-16 and ResNet-101.\\n- **Testing Types**: \"1-crop\" and \"dense.\"\\n- **LOC Error on GT CLS**: \\n  - VGG-16: 33.1%\\n  - ResNet-101 (1-crop): 13.3%\\n  - ResNet-101 (dense): 11.7%\\n- **Classification Network**: Primarily ResNet-101 and an ensemble approach.\\n- **Top-5 LOC Error on Predicted CLS**:\\n  - ResNet-101: 14.4%\\n  - RPN+ResNet-101: 10.6%\\n  - RPN+RCNN ensemble: 8.9%\\n\\nOverall, the results indicate that using ensemble methods and dense testing improves classification performance.] [RELATED_IMAGE_SUMMARY: The table presents the top-5 localization error rates (in percentage) for various methods evaluated on the ILSVRC dataset across validation and test sets. \\n\\n- **OverFeat (ILSVRC\\'13)**: 30.0 (val), 29.9 (test)\\n- **GoogLeNet (ILSVRC\\'14)**: Not applicable (val), 26.7 (test)\\n- **VGG (ILSVRC\\'14)**: 26.9 (val), 25.3 (test)\\n- **Ours (ILSVRC\\'15)**: 8.9 (val), 9.0 (test)\\n\\nThe method \"Ours\" shows the lowest localization error, significantly outperforming the other methods listed.] [RELATED_IMAGE_SUMMARY: The table compares the performance of two metrics, VGG-16 and ResNet-101, based on mean Average Precision (mAP) at different thresholds. \\n\\n- For mAP at 0.5, VGG-16 scores 41.5, while ResNet-101 scores higher at 48.4.\\n- For mAP at the range [0.5, 0.95], VGG-16 achieves 21.2, and ResNet-101 shows an improved score of 27.2. \\n\\nOverall, ResNet-101 outperforms VGG-16 in both metrics.] [RELATED_IMAGE_SUMMARY: The table presents the performance of two models, VGG-16 and ResNet-101, on different training and test datasets. \\n\\n- For the training data \"07+12\":\\n  - On the VOC 07 test, VGG-16 achieved 73.2, while ResNet-101 scored 76.4.\\n  \\n- For the training data \"07++12\":\\n  - On the VOC 12 test, VGG-16 scored 70.4, and ResNet-101 scored 73.8.\\n\\nOverall, ResNet-101 consistently outperformed VGG-16 across both test datasets.]')]"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_image_text_doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645a762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "48fa2e9f-bb1d-4fed-9195-0503b72d0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CASE-3: Image_summaries and pdf raw text : by changing the prompt and it will be given as a context to LLM and asked to summarize'''\n",
    "\n",
    "# Define prompt\n",
    "prompt1 = ChatPromptTemplate.from_messages(\n",
    "    [(\n",
    "            \"system\",\n",
    "            \"You are a summarization assistant specialized in scientific content. Given the context of a scientific paper, which includes raw text and related image summaries, generate a comprehensive and concise summary. Integrate key points from both the text and image summaries:\\n\"\n",
    "            \"Context:\\n\\n{context} [RELATED_IMAGE_SUMMARY]\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Instantiate chains \n",
    "chain1_1 = create_stuff_documents_chain(llm1, prompt1)\n",
    "chain2_1 = create_stuff_documents_chain(llm2, prompt1)\n",
    "\n",
    "gpt_doc_img_chain_result = chain1_1.invoke({\"context\": updated_image_text_doc3})\n",
    "mistralai_doc_img_chain_result = chain2_1.invoke({\"context\": updated_image_text_doc3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "70a789cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPt img response result\n",
      "rouge1 - Precision: 0.2537, Recall: 0.6159, Fmeasure: 0.3594\n",
      "rouge2 - Precision: 0.0868, Recall: 0.2117, Fmeasure: 0.1231\n",
      "rougeL - Precision: 0.1433, Recall: 0.3478, Fmeasure: 0.2030\n",
      "\n",
      "\n",
      "mistral img response result\n",
      "rouge1 - Precision: 0.2047, Recall: 0.6884, Fmeasure: 0.3156\n",
      "rouge2 - Precision: 0.0821, Recall: 0.2774, Fmeasure: 0.1267\n",
      "rougeL - Precision: 0.1121, Recall: 0.3768, Fmeasure: 0.1728\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_Score(scores):\n",
    "\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"{metric} - Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, Fmeasure: {score.fmeasure:.4f}\") \n",
    "\n",
    "\n",
    "# ROUGE Score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "\n",
    "score_gpt_mini_img_doc = scorer.score(abstract,gpt_doc_img_chain_result)\n",
    "\n",
    "score_mistral_img_doc = scorer.score(abstract,mistralai_doc_img_chain_result)\n",
    "\n",
    "\n",
    "print(\"GPt img response result\")\n",
    "get_Score(score_gpt_mini_img_doc)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"mistral img response result\")\n",
    "get_Score(score_mistral_img_doc)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "493c370a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2412\n",
      "\n",
      "\n",
      "3241\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(gpt_doc_img_chain_result))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(len(mistralai_doc_img_chain_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "f5c1d690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GPT summary BERT (text only)  F1 Score: 0.8731822967529297\n",
      "\n",
      "\n",
      "Mistral AI summary (BERT text) only  F1 Score: 0.8599469661712646\n",
      "\n",
      "\n",
      "GPT summary BERT (text and Images)  F1 Score: 0.8617427349090576\n",
      "\n",
      "\n",
      "Mistral AI summary BERT (text  and Images)  F1 Score:  0.8399030566215515\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "scorer = BERTScorer(lang=\"en\")\n",
    "\n",
    "P1, R1, F1_1 = scorer.score([gpt_chain_result], [abstract])\n",
    "\n",
    "P1_1, R1_1, F1_1_1 = scorer.score([gpt_doc_img_chain_result], [abstract])\n",
    "\n",
    "P2, R2, F1_2 = scorer.score([MistralAI_chain_result], [abstract])\n",
    "\n",
    "P2_1, R2_1, F2_2_1 = scorer.score([mistralai_doc_img_chain_result], [abstract])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"GPT summary BERT (text only)  F1 Score:\", F1_1.tolist()[0])\n",
    "print(\"\\n\")\n",
    "print(\"Mistral AI summary (BERT text) only  F1 Score:\", F1_1_1.tolist()[0])\n",
    "print(\"\\n\")\n",
    "print(\"GPT summary BERT (text and Images)  F1 Score:\", F1_2.tolist()[0])\n",
    "print(\"\\n\")\n",
    "print(\"Mistral AI summary BERT (text  and Images)  F1 Score: \", F2_2_1.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "0604f17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Article    Model BLEU Score\n",
      "0  Article_1                    \n",
      "1                 GPT     0.3059\n",
      "2             Mistral     0.2365\n"
     ]
    }
   ],
   "source": [
    "bleu_score_gpt2 = sentence_bleu([abstract], gpt_doc_img_chain_result)\n",
    "bleu_score_mistal2 = sentence_bleu([abstract], mistralai_doc_img_chain_result)\n",
    "\n",
    "bleu_score_data1 = {\n",
    "    \"Article\": [\"Article_1\", \"\", \"\"],\n",
    "    \"Model\": [\"\", \"GPT\", \"Mistral\"],\n",
    "    \"BLEU Score\": [\"\", round(bleu_score_gpt2, 4), round(bleu_score_mistal2, 4)]\n",
    "}\n",
    "\n",
    "bleu_scores_df1 = pd.DataFrame(bleu_score_data1)\n",
    "\n",
    "print(bleu_scores_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "bcdad72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation prompt template based on G-Eval\n",
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions very carefully. \n",
    "Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "{steps}\n",
    "\n",
    "Example:\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{document}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "- {metric_name}\n",
    "\"\"\"\n",
    "\n",
    "# Metric 1: Relevance\n",
    "\n",
    "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
    "Relevance(1-5) - selection of important content from the source. \\\n",
    "The summary should include only important information from the source document. \\\n",
    "Annotators were instructed to penalize summaries which contained redundancies and excess information.\n",
    "\"\"\"\n",
    "\n",
    "RELEVANCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the summary and the source document carefully.\n",
    "2. Compare the summary to the source document and identify the main points of the article.\n",
    "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 2: Coherence\n",
    "\n",
    "COHERENCE_SCORE_CRITERIA = \"\"\"\n",
    "Coherence(1-5) - the collective quality of all sentences. \\\n",
    "We align this dimension with the DUC quality question of structure and coherence \\\n",
    "whereby \"the summary should be well-structured and well-organized. \\\n",
    "The summary should not just be a heap of related information, but should build from sentence to a\\\n",
    "coherent body of information about a topic.\"\n",
    "\"\"\"\n",
    "\n",
    "COHERENCE_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main topic and key points.\n",
    "2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,\n",
    "and if it presents them in a clear and logical order.\n",
    "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 3: Consistency\n",
    "\n",
    "CONSISTENCY_SCORE_CRITERIA = \"\"\"\n",
    "Consistency(1-5) - the factual alignment between the summary and the summarized source. \\\n",
    "A factually consistent summary contains only statements that are entailed by the source document. \\\n",
    "Annotators were also asked to penalize summaries that contained hallucinated facts.\n",
    "\"\"\"\n",
    "\n",
    "CONSISTENCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main facts and details it presents.\n",
    "2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.\n",
    "3. Assign a score for consistency based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 4: Fluency\n",
    "\n",
    "FLUENCY_SCORE_CRITERIA = \"\"\"\n",
    "Fluency(1-5): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
    "1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n",
    "2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "3: Good. The summary has few or no errors and is easy to read and follow.\n",
    "4: Very Good. The summary is almost perfect human readibale but also can be improved.\n",
    "5: Excellent. The summary is pefect and completlely human readibale no improvements needed.\n",
    "\"\"\"\n",
    "\n",
    "FLUENCY_SCORE_STEPS = \"\"\"\n",
    "Read the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_geval_score(\n",
    "    criteria: str, steps: str, document: str, summary: str, metric_name: str\n",
    "):\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        criteria=criteria,\n",
    "        steps=steps,\n",
    "        metric_name=metric_name,\n",
    "        document=document,\n",
    "        summary=summary,\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=5,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
    "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
    "    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n",
    "    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n",
    "}\n",
    "\n",
    "summaries = {\"gpt_chain_result 1 \" : gpt_chain_result , \"gpt_doc_img_chain_result 1\": gpt_doc_img_chain_result , \"mistralAI_chain_result 1\" : MistralAI_chain_result, \"mistralai_doc_img_chain_result 1\" : mistralai_doc_img_chain_result}\n",
    "\n",
    "data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n",
    "\n",
    "for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
    "    for summ_type, summary in summaries.items():\n",
    "        data[\"Evaluation Type\"].append(eval_type)\n",
    "        data[\"Summary Type\"].append(summ_type)\n",
    "        result = get_geval_score(criteria, steps, abstract, summary, eval_type)\n",
    "        score_num = int(result.strip())\n",
    "        data[\"Score\"].append(score_num)\n",
    "\n",
    "import pandas as pd\n",
    "pivot_df = pd.DataFrame(data, index=None).pivot(\n",
    "    index=\"Evaluation Type\", columns=\"Summary Type\", values=\"Score\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "f68b3727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Type     gpt_chain_result 1   gpt_doc_img_chain_result 1  \\\n",
      "Evaluation Type                                                    \n",
      "Coherence                          5                           5   \n",
      "Consistency                        5                           5   \n",
      "Fluency                            5                           5   \n",
      "Relevance                          5                           5   \n",
      "\n",
      "Summary Type     mistralAI_chain_result 1  mistralai_doc_img_chain_result 1  \n",
      "Evaluation Type                                                              \n",
      "Coherence                               5                                 5  \n",
      "Consistency                             5                                 5  \n",
      "Fluency                                 5                                 5  \n",
      "Relevance                               5                                 5  \n"
     ]
    }
   ],
   "source": [
    "print(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c0708",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_results = []\n",
    "bert_results = []\n",
    "bleu_results = []\n",
    "final_prompt_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "74568ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In this paper, Simonyan and Zisserman present a comprehensive evaluation of deep convolutional networks (ConvNets) specifically focused on the impact of network depth on image classification accuracy. They utilize a novel architecture characterized by small (3x3) convolutional filters, demonstrating that increasing the depth of networks to between 16 to 19 weight layers results in significant accuracy improvements in large-scale image recognition tasks, particularly noted during their participation in the ImageNet Challenge 2014, where they achieved first and second place in localization and classification tracks, respectively.\\n\\nThe authors detail the architecture of their ConvNet configurations, which includes a series of convolutional layers followed by fully connected layers, employing a fixed input size of 224x224 RGB images. They enhance the networks' representational power through the use of multiple small convolutional layers rather than fewer large filters, which leads to a more effective learning process due to increased non-linearity and reduced parameters. The configurations range from 11 layers in the shallowest model to 19 layers in the deepest, with their results indicating a consistent decrease in classification error as depth increases.\\n\\nTraining methodologies are also discussed, where they employed techniques such as multi-scale training, dropout regularization, and the use of mini-batch gradient descent. Their experiments confirm that employing scale jittering during training and multi-scale evaluation during testing leads to superior performance compared to models trained on fixed scales.\\n\\nThey provide a detailed analysis of their results on the ILSVRC-2012 dataset, showing that their networks consistently outperform previous architectures, including the winning models from earlier competitions. The authors also release their best-performing models to support further research in deep learning applications in computer vision.\\n\\nThe paper concludes by emphasizing the importance of depth in ConvNet architectures, demonstrating their models' ability to generalize well across various datasets and tasks, which significantly outperforms shallower models in both classification and localization tasks. Furthermore, they highlight their models' robustness in other recognition tasks, establishing a new benchmark in the field of computer vision with their findings.\""
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_doc_img_chain_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c9281940-5385-465e-ab99-6269465965c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'query = \"Amazone inductive task value\"\\n\\ndef relavent_info(query):\\n    result = vector_store.similarity_search(query)\\n    print(result)\\n    aggregated_text = \" \".join([doc.page_content for doc in result])\\n    return aggregated_text\\n\\nresult = relavent_info(query)\\n\\n\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI\\n\\n# Prompt template\\ntemplate = \"\"\"Answer the question based only on the following context, which can include text and tables:\\n{context}\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Option 1: LLM\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0.5)\\n\\n# RAG pipeline\\nchain2 = (\\n    {\\n        \"context\": RunnablePassthrough(),  # Wrap context in RunnablePassthrough\\n        \"question\": RunnablePassthrough(),  # Same for the question\\n    }\\n    | prompt\\n    | model\\n    | StrOutputParser()\\n)\\n\\n# Execute the chain\\noutput = chain2.invoke({\"context\": result, \"question\": query})\\nprint(output)'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''query = \"Amazone inductive task value\"\n",
    "\n",
    "def relavent_info(query):\n",
    "    result = vector_store.similarity_search(query)\n",
    "    print(result)\n",
    "    aggregated_text = \" \".join([doc.page_content for doc in result])\n",
    "    return aggregated_text\n",
    "\n",
    "result = relavent_info(query)\n",
    "\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Option 1: LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0.5)\n",
    "\n",
    "# RAG pipeline\n",
    "chain2 = (\n",
    "    {\n",
    "        \"context\": RunnablePassthrough(),  # Wrap context in RunnablePassthrough\n",
    "        \"question\": RunnablePassthrough(),  # Same for the question\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "output = chain2.invoke({\"context\": result, \"question\": query})\n",
    "print(output)'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
